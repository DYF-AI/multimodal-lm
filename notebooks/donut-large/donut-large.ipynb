{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "#### donut-base\n",
    "- swintransformer + bart-base-decoder\n",
    "#### donut-large\n",
    "- swintransformer + bart-larger-decoder"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "MP1 = \"J:/model/mllm-model/donut-pretrain/20240102/pl-checkpoint-159500-ned-0.8607632262639043\"\n",
    "MP2 = \"J:/model/facebook_bart-large\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import sys\n",
    "from transformers import (VisionEncoderDecoderModel,\n",
    "                          VisionEncoderDecoderConfig,\n",
    "                          DonutProcessor,\n",
    "                          AutoProcessor,\n",
    "                          BartModel,\n",
    "                          MBartModel,\n",
    "                          BartConfig,\n",
    "                          MBartConfig,\n",
    "                          AutoTokenizer)\n",
    "sys.path.append(\"../../src/multimodallm\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from multimodallm.utils import cal_model_parameters"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "donut_base_config = VisionEncoderDecoderConfig.from_pretrained(MP1)\n",
    "donut_base_model = VisionEncoderDecoderModel.from_pretrained(MP1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "(204299384, 74180728, 130118656)"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "donut_base_params = cal_model_parameters(donut_base_model)\n",
    "donut_base_encoder_params = cal_model_parameters(donut_base_model.encoder)\n",
    "donut_base_decoder_params = cal_model_parameters(donut_base_model.decoder)\n",
    "donut_base_params, donut_base_encoder_params, donut_base_decoder_params"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type bart to instantiate a model of type mbart. This is not supported for all configurations of models and can yield errors.\n",
      "You are using a model of type bart to instantiate a model of type mbart. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of MBartModel were not initialized from the model checkpoint at J:/model/facebook_bart-large and are newly initialized: ['decoder.layer_norm.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'decoder.layer_norm.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "bart_large_config = MBartConfig.from_pretrained(MP2)\n",
    "bart_large_model = MBartModel.from_pretrained(MP2)\n",
    "bart_large_tokenizer = AutoTokenizer.from_pretrained(MP2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "(406295552, 203680768, 254086144)"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bart_large_params = cal_model_parameters(bart_large_model)\n",
    "bart_large_encoder_params = cal_model_parameters(bart_large_model.encoder)\n",
    "bart_large_decoder_params = cal_model_parameters(bart_large_model.decoder)\n",
    "bart_large_params, bart_large_encoder_params, bart_large_decoder_params"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "Seq2SeqModelOutput(last_hidden_state=tensor([[[ 1.8731e-01,  1.2451e+00, -2.6025e-02,  ...,  2.7185e-01,\n          -7.0134e-02,  2.0836e-01],\n         [ 1.8448e-01,  1.3049e+00, -3.7771e-02,  ...,  2.7551e-01,\n          -6.2602e-02,  2.0327e-01],\n         [ 1.8678e-01,  9.1669e-01, -3.1941e-02,  ...,  2.6436e-01,\n          -5.6333e-02,  2.2700e-01],\n         ...,\n         [ 1.8920e-01,  2.1786e-01, -1.0730e-01,  ...,  3.3643e-01,\n          -8.3321e-02,  2.8835e-01],\n         [ 1.4821e-01,  5.8140e-01, -1.1566e-01,  ...,  3.2180e-01,\n          -6.0410e-02,  2.1415e-01],\n         [ 2.2279e-01,  1.2014e-03, -1.9514e-01,  ...,  3.6329e-01,\n          -6.7464e-02,  2.9462e-01]]], grad_fn=<NativeLayerNormBackward0>), past_key_values=((tensor([[[[ 6.4695e+00,  7.4873e+00,  1.3615e+01,  ...,  5.5435e+00,\n            1.9106e+01, -4.8808e+00],\n          [ 2.4149e+00,  6.9186e+00,  9.8275e+00,  ...,  4.7713e+00,\n            1.3457e+01, -6.1739e+00],\n          [-1.1826e+00, -1.9506e+00, -6.1460e+00,  ..., -7.3526e+00,\n           -5.4056e+00, -2.1048e+00],\n          ...,\n          [-2.4700e+00, -2.1766e+00, -3.3607e+00,  ...,  1.0281e+00,\n           -4.8591e+00,  7.4612e-01],\n          [-8.4528e+00,  8.0418e+00, -6.1984e+00,  ..., -1.8630e+00,\n           -1.5949e+01, -1.1555e+00],\n          [ 1.0052e-01,  7.6979e+00, -3.9203e-01,  ..., -5.1684e+00,\n           -1.0452e+01,  1.4972e+00]],\n\n         [[-1.8926e+01, -9.0926e+00, -3.0332e+00,  ..., -6.9614e+00,\n           -1.6639e+00,  9.6375e+00],\n          [-1.2930e+01, -9.5443e+00, -1.4444e+00,  ..., -1.0664e+01,\n           -2.3757e+00,  2.1071e+00],\n          [-1.4183e+00, -9.9112e-01, -1.8900e+00,  ..., -1.8894e+00,\n           -1.2249e+00,  7.7056e-01],\n          ...,\n          [-1.9641e+00, -1.3253e+00, -4.6774e+00,  ..., -1.5773e+00,\n           -2.7532e+00,  2.7255e+00],\n          [-3.9929e+00,  2.6958e+00,  3.4769e+00,  ..., -5.6877e+00,\n           -5.2616e+00,  5.7448e+00],\n          [-2.0061e+00, -1.7539e+00,  6.7155e+00,  ..., -8.9973e-01,\n            1.7320e+00,  6.2503e-01]],\n\n         [[ 1.7224e+00,  1.9133e+00,  6.9235e+00,  ..., -8.0412e-01,\n           -7.3285e+00,  2.1482e+01],\n          [-5.5980e-01,  8.2938e-01,  3.1810e+00,  ..., -1.7209e+00,\n           -3.9973e+00,  2.1406e+01],\n          [-2.4967e+00,  1.4289e+00,  5.7156e-01,  ..., -4.1918e+00,\n           -4.9528e-01, -2.4106e+00],\n          ...,\n          [ 7.4195e+00, -5.5954e-01, -6.5356e+00,  ...,  2.1066e+00,\n            1.4040e+00, -1.3238e+00],\n          [ 2.6839e+00, -1.9097e+00,  1.5086e-01,  ..., -8.7310e-02,\n            4.3850e+00, -2.7539e+00],\n          [-3.0012e-01,  1.9015e+00,  4.2700e+00,  ..., -1.0982e+00,\n            3.7218e+00, -4.4704e+00]],\n\n         ...,\n\n         [[ 3.1317e+00,  1.3569e+01,  9.2047e+00,  ...,  7.6377e+00,\n            1.1304e+01, -8.9845e-02],\n          [ 2.0481e+00,  3.1611e+00, -1.7588e+00,  ...,  2.9857e+00,\n           -3.7618e-01,  4.8616e+00],\n          [ 4.4570e-01,  1.5261e+00,  1.6199e+00,  ...,  2.6814e+00,\n           -1.0928e+00, -2.6040e-01],\n          ...,\n          [-1.5920e+00, -4.5854e-01,  1.8527e+00,  ..., -3.0908e+00,\n           -1.7698e+00, -2.7653e+00],\n          [-6.2058e-01, -2.6095e-01, -3.9747e+00,  ..., -8.2371e-01,\n           -7.0222e+00, -1.0190e+00],\n          [ 1.9591e+00, -2.5570e-01, -3.9121e+00,  ...,  3.9350e+00,\n           -1.9466e+00, -1.6268e+00]],\n\n         [[-4.5231e+00,  2.7113e+01,  7.8599e+00,  ...,  1.9900e+01,\n            6.3090e+00, -2.6327e+00],\n          [-3.8463e+00,  1.7484e+01,  9.7567e-01,  ...,  1.5095e+01,\n            3.8863e+00, -5.0363e+00],\n          [-5.8596e-01, -2.7273e+00, -2.9873e+00,  ..., -5.4741e+00,\n           -3.0088e+00,  3.2563e-01],\n          ...,\n          [-1.4619e-01, -5.3588e+00, -2.6427e+00,  ..., -4.0673e+00,\n           -7.5734e+00, -3.8227e+00],\n          [ 2.6140e-01, -6.0752e+00, -3.2480e+00,  ..., -5.1191e+00,\n           -1.8400e+00,  1.1565e+00],\n          [-3.7126e-01, -1.1398e+01,  1.4408e+00,  ..., -8.6329e+00,\n           -4.8973e+00, -8.4507e-01]],\n\n         [[ 1.9701e+01, -5.9820e+00, -5.3014e+00,  ..., -2.1766e+00,\n           -5.3666e+00, -6.3251e+00],\n          [ 1.0489e+01, -5.8817e+00,  1.3021e+00,  ...,  1.3990e+00,\n           -3.5355e+00,  2.7579e+00],\n          [ 4.1070e+00, -2.0857e+00,  4.1764e+00,  ..., -8.8009e-01,\n            1.9688e-02,  1.1120e+00],\n          ...,\n          [-5.1031e-01,  2.4413e+00, -4.6767e+00,  ...,  5.5303e+00,\n            3.1062e+00, -2.6993e+00],\n          [-2.3146e+00, -3.0625e+00,  2.3123e+00,  ...,  5.3949e+00,\n           -1.7728e+00, -5.5501e+00],\n          [ 1.2484e+00,  1.9718e+00,  3.6750e+00,  ...,  2.0681e+00,\n            6.8633e-01, -3.6645e+00]]]], grad_fn=<CloneBackward0>), tensor([[[[-2.0676e+00,  3.5347e+00, -5.5894e-01,  ...,  8.7839e-01,\n            2.8268e-01, -3.6196e+00],\n          [-1.0837e+00,  2.7145e+00, -3.5296e-01,  ...,  7.4610e-01,\n            1.0360e-01, -1.5511e+00],\n          [ 1.0411e+00,  3.4364e-02,  1.5574e-01,  ...,  5.0082e-02,\n           -7.9892e-01,  1.6357e+00],\n          ...,\n          [-2.8840e-01,  3.8240e-01, -1.0775e-01,  ..., -1.4083e-01,\n            6.2450e-02, -2.6191e-01],\n          [ 2.7022e-01,  4.4963e-02, -1.2828e+00,  ...,  3.1676e-01,\n            3.8315e-01,  7.3344e-01],\n          [ 9.5194e-02,  7.4703e-01,  1.3175e+00,  ...,  4.4149e-01,\n           -2.1492e-01,  1.7851e-01]],\n\n         [[ 1.0482e-01, -9.7236e-02, -2.6821e-01,  ...,  6.1396e-02,\n            4.2824e-01, -1.5610e-02],\n          [ 5.0007e-02, -1.8730e-01, -2.0776e-01,  ..., -7.1883e-02,\n            3.3433e-01,  2.9434e-02],\n          [ 2.6722e-01, -3.4561e-01, -1.5514e-01,  ...,  2.5946e-01,\n           -3.0742e-01,  2.6571e-01],\n          ...,\n          [-7.4500e-01,  1.2714e+00,  7.0212e-01,  ..., -9.6036e-01,\n            7.0176e-01,  4.4750e-01],\n          [ 4.9943e-01, -1.0908e+00,  3.2940e-01,  ...,  1.0025e+00,\n           -3.3001e-01, -3.8594e-01],\n          [-3.8083e-01, -9.0228e-02, -8.8661e-01,  ..., -8.9080e-01,\n            1.2255e+00, -1.6427e-02]],\n\n         [[ 3.5975e-01,  4.8522e-02, -2.9049e-01,  ..., -4.7925e-01,\n            5.8498e-03,  2.6304e-01],\n          [ 3.3681e-01,  1.2425e-01, -2.9451e-02,  ..., -5.5095e-01,\n            1.4014e-01,  1.8382e-01],\n          [-2.1164e-01,  9.3761e-01,  3.7216e-01,  ...,  1.6114e+00,\n           -9.7807e-02, -2.8842e-01],\n          ...,\n          [-6.9050e-01, -2.1378e+00,  3.5324e-01,  ...,  3.0291e-01,\n           -2.0778e-01, -1.3738e+00],\n          [ 4.1200e-01,  2.2222e-01,  1.9548e-01,  ..., -1.0521e+00,\n           -5.0270e-03, -6.0227e-01],\n          [-5.7643e-01,  7.5368e-01,  3.1274e-01,  ...,  1.4538e+00,\n           -6.4480e-01,  4.3253e-01]],\n\n         ...,\n\n         [[-4.1356e-01, -7.7794e-02, -8.4086e-03,  ...,  6.3809e-03,\n            4.8753e-01, -1.8539e-02],\n          [-2.9368e-01,  7.5926e-02,  1.7584e-01,  ...,  1.3917e+00,\n            1.1989e-01,  1.2528e-02],\n          [ 3.3358e-01,  1.9951e+00, -5.9734e-01,  ..., -7.0301e-01,\n            2.9794e-01, -5.2240e-02],\n          ...,\n          [ 5.7826e-01, -9.2256e-01,  2.0315e-01,  ...,  2.8507e-01,\n            7.7493e-01,  2.0560e+00],\n          [-6.2403e-01, -3.1082e-01,  4.7164e-01,  ...,  1.1108e+00,\n            8.6382e-01, -8.4800e-04],\n          [-3.1424e-01, -9.1915e-02, -7.8962e-02,  ...,  6.0274e-01,\n           -1.4673e+00, -1.6043e-01]],\n\n         [[ 4.9648e-01, -1.4636e-01,  7.8323e-02,  ..., -7.9530e-01,\n            5.5683e-01,  2.3133e-01],\n          [ 4.8344e-01, -8.9031e-02,  3.8784e-02,  ..., -7.6099e-01,\n            2.6400e-01,  2.2227e-01],\n          [-2.5260e+00, -1.5921e-01, -2.5484e-01,  ...,  1.1364e+00,\n            1.0975e+00, -9.0196e-01],\n          ...,\n          [-6.4203e-01,  2.6637e+00,  9.0750e-02,  ..., -4.6076e-01,\n           -1.3730e+00, -8.4384e-01],\n          [ 2.1731e-01, -2.1832e-01,  6.1337e-01,  ...,  9.3524e-01,\n            2.0462e-01,  1.2643e+00],\n          [ 7.4527e-01,  3.7706e-01,  8.1791e-01,  ..., -7.1185e-02,\n            1.9329e-01, -6.4401e-02]],\n\n         [[ 2.6361e-01, -8.4698e-02,  3.2980e-01,  ..., -1.8359e-01,\n           -1.6650e-01, -5.3372e-02],\n          [ 5.8034e-01,  3.2230e-02,  4.1405e-01,  ..., -9.8882e-01,\n           -1.2865e-01,  8.4413e-02],\n          [ 1.5983e+00,  6.4046e-01,  1.7238e-01,  ..., -9.5744e-01,\n           -1.9913e-01, -7.2592e-01],\n          ...,\n          [-2.3447e-01, -1.2257e+00,  2.0440e-01,  ..., -1.5121e-01,\n           -3.7452e-01,  1.0938e+00],\n          [-5.5913e-01,  2.9494e-01, -2.8228e-01,  ..., -9.7512e-01,\n            5.7135e-01, -1.1044e+00],\n          [-3.0820e-01,  8.0780e-01, -4.0926e-01,  ...,  1.4918e+00,\n           -7.1499e-01, -3.2140e-01]]]], grad_fn=<CloneBackward0>), tensor([[[[-1.5987,  1.9210,  1.1221,  ..., -1.2038,  6.5529,  2.4849],\n          [-1.9853,  0.9069,  0.6843,  ..., -0.9328,  6.1021,  3.6664],\n          [-2.0831, -0.6787,  0.7901,  ..., -1.1911,  3.4511,  3.2667],\n          ...,\n          [-1.7013,  0.5285,  0.7125,  ..., -1.8107,  5.5679,  3.2670],\n          [-1.4600,  1.1510,  0.4015,  ..., -1.5056,  5.4471,  4.3037],\n          [-2.0664,  1.1157,  0.9936,  ..., -1.7437,  5.9035,  3.3827]],\n\n         [[ 1.8118,  1.0465, -2.9725,  ..., -6.5449, -1.4937, -2.8681],\n          [ 1.9141,  0.3155, -0.5575,  ..., -6.1128, -1.6372, -3.6903],\n          [ 2.1854,  0.2598, -1.1176,  ..., -5.0322, -1.1506, -1.0557],\n          ...,\n          [ 2.2011,  0.3179, -0.0726,  ..., -6.0279, -0.7821, -2.7362],\n          [ 1.6269,  0.1986, -0.4600,  ..., -6.8215, -1.2460, -3.5405],\n          [ 1.8743,  0.5136, -1.0355,  ..., -5.6472, -1.0965, -3.5289]],\n\n         [[ 0.3449, -1.3707, -2.4376,  ..., -8.3099,  0.4657,  0.3382],\n          [ 0.1635,  0.2046, -3.0417,  ..., -6.7642, -0.5453, -0.0228],\n          [-0.7161,  2.5279, -2.9858,  ..., -7.9665,  0.7724, -1.0134],\n          ...,\n          [-0.7541,  1.5977, -3.2784,  ..., -7.1381, -0.2199, -0.2110],\n          [-0.9718,  1.1978, -3.2834,  ..., -6.4994, -0.9503, -0.5380],\n          [-0.3534,  0.0470, -3.4157,  ..., -7.1120, -0.2961,  0.1831]],\n\n         ...,\n\n         [[-0.7541,  0.1036,  2.1737,  ..., -1.9884,  3.0080,  1.2055],\n          [-1.0991,  1.0496,  1.1095,  ..., -3.1637,  2.7023,  2.4474],\n          [-1.8370,  1.2552,  0.3606,  ..., -2.4834,  2.1297,  3.7165],\n          ...,\n          [-1.3476,  2.1975,  0.9666,  ..., -3.4089,  1.4410,  2.6535],\n          [-0.9656,  1.3890,  0.9736,  ..., -3.6587,  1.6187,  2.8097],\n          [-0.7750,  1.0109,  1.1224,  ..., -3.4728,  2.2715,  2.3200]],\n\n         [[-1.9939, -1.7489,  2.4910,  ..., -0.4594, -4.6008, -3.3414],\n          [-1.8456, -1.0309,  2.8487,  ..., -0.7931, -3.5012, -3.1584],\n          [-1.0957, -1.2057,  2.2119,  ..., -1.3299, -2.4663, -4.1049],\n          ...,\n          [-1.6814, -1.3558,  2.5482,  ..., -0.7310, -2.4797, -2.7106],\n          [-2.1384, -1.6709,  2.9800,  ..., -0.9641, -3.4170, -3.1200],\n          [-1.8422, -1.3453,  3.2566,  ..., -0.8856, -2.8969, -2.9411]],\n\n         [[-0.0825, -2.1918, -2.7645,  ...,  0.2578, -2.8326,  0.2894],\n          [ 1.2564, -1.9725, -1.2240,  ...,  0.9880, -3.6969, -1.6783],\n          [ 1.3142,  0.3228,  0.4971,  ...,  1.5455, -5.5269, -1.3523],\n          ...,\n          [ 1.4054, -1.5916, -1.1437,  ...,  1.0760, -3.5514, -1.0787],\n          [ 1.3300, -1.4695, -1.2706,  ...,  1.2167, -4.8707, -1.6236],\n          [ 0.9331, -1.8794, -1.6748,  ...,  0.7341, -3.4323, -1.2525]]]],\n       grad_fn=<CloneBackward0>), tensor([[[[-0.7410,  0.1358,  3.4614,  ...,  2.1544, -3.5272,  0.4262],\n          [ 0.5963, -0.2064,  2.4043,  ...,  1.6354, -2.4230, -0.5108],\n          [ 0.8877, -0.1834,  3.6216,  ...,  1.1915, -2.7526, -0.3905],\n          ...,\n          [ 0.6081, -0.3579,  3.3007,  ...,  1.6989, -2.4573, -0.3457],\n          [ 0.5909, -0.3573,  2.5487,  ...,  1.5054, -2.7208, -0.6034],\n          [ 0.0568, -0.3285,  3.0164,  ...,  1.7877, -2.5965, -0.1152]],\n\n         [[ 0.0866,  0.5511,  0.6787,  ..., -3.8715, -1.4135, -0.8245],\n          [-0.9837,  1.2319,  0.8983,  ..., -4.3240, -1.2721, -1.0894],\n          [-1.5230,  1.0130,  1.9086,  ..., -3.0280, -2.4715, -0.5076],\n          ...,\n          [-0.7158,  0.7903,  1.3133,  ..., -4.1541, -1.4543, -1.3331],\n          [-0.9263,  1.0981,  1.3703,  ..., -4.3810, -1.5355, -1.2446],\n          [-0.3635,  0.8524,  1.0489,  ..., -4.2860, -0.8553, -1.1355]],\n\n         [[ 1.7085, -2.4578,  0.7038,  ...,  0.5247, -1.1393, -0.8851],\n          [ 1.9904, -2.5476,  1.8303,  ...,  0.5500, -2.2767, -0.8006],\n          [ 2.4718, -2.8941,  3.2354,  ..., -0.0119, -1.8972, -1.6012],\n          ...,\n          [ 2.4099, -3.0120,  2.5118,  ...,  0.9621, -2.4398, -0.4795],\n          [ 1.6982, -2.4358,  1.6367,  ...,  0.7526, -2.0645, -0.6947],\n          [ 2.4099, -2.4965,  1.6619,  ...,  0.4091, -1.6838, -0.6203]],\n\n         ...,\n\n         [[ 5.6325,  2.9854, -0.7138,  ...,  1.1897,  2.8917,  1.7558],\n          [ 6.1118,  3.3062,  0.4711,  ...,  2.2981,  2.5378,  2.4442],\n          [ 5.1809,  3.1553, -0.4139,  ...,  2.9272,  2.8507,  2.9710],\n          ...,\n          [ 6.3895,  3.4013,  0.3572,  ...,  2.4591,  2.5415,  2.9938],\n          [ 6.1588,  3.3716,  0.8035,  ...,  2.4702,  2.1640,  2.6895],\n          [ 6.2822,  3.3626,  0.1677,  ...,  2.1286,  2.9124,  2.4129]],\n\n         [[-1.1623, -0.5752, -0.3270,  ...,  2.5900, -2.8244, -0.5718],\n          [-0.1869, -0.2674,  0.6085,  ...,  2.8893, -2.0260,  0.1814],\n          [-0.3158, -0.0585,  1.8020,  ...,  1.2342, -0.8521,  1.1437],\n          ...,\n          [-0.8476, -0.2993,  0.7630,  ...,  1.7919, -1.6498,  0.5290],\n          [-0.3858, -0.5246,  0.1796,  ...,  2.5140, -2.2086, -0.0122],\n          [-0.6541, -0.8818,  0.6346,  ...,  2.3653, -2.0129,  0.1670]],\n\n         [[ 1.4416, -0.6539,  0.7557,  ...,  1.0867,  0.7678,  0.5164],\n          [ 1.9481, -0.6814,  0.7540,  ...,  0.4854,  0.5160,  0.5691],\n          [ 2.7982,  0.1206,  1.0936,  ...,  0.1917,  1.1423,  0.5370],\n          ...,\n          [ 2.1708, -0.6431,  0.5545,  ...,  0.5699,  0.6835,  0.9316],\n          [ 2.2668, -0.5018,  0.8455,  ...,  0.7281,  0.3414,  0.5364],\n          [ 2.0001, -0.5923,  0.5969,  ...,  0.7785,  0.4295,  0.8888]]]],\n       grad_fn=<CloneBackward0>)), (tensor([[[[ 3.0765, -0.7742,  0.9707,  ...,  0.4895, -0.6691,  0.3822],\n          [ 3.0107, -0.7195,  0.9049,  ...,  0.3405, -0.6198,  0.2695],\n          [ 2.7543, -0.9918,  1.2199,  ...,  0.0940, -1.6285,  0.7129],\n          ...,\n          [ 1.4233, -0.1708,  1.6610,  ...,  1.3517, -2.1436, -0.5426],\n          [ 1.5942, -0.9725,  1.5047,  ...,  0.9246, -1.8160, -0.6292],\n          [ 0.0417, -0.0627,  1.0495,  ...,  2.0445, -1.9677,  0.2296]],\n\n         [[ 2.2748, -0.4112,  2.6858,  ...,  0.0976, -0.3581, -1.2291],\n          [ 2.2572, -0.2511,  2.6758,  ...,  0.1617, -0.3055, -1.1399],\n          [ 2.2350, -1.1077,  2.7443,  ..., -0.6197, -0.7108, -1.3931],\n          ...,\n          [ 1.1245, -2.4468,  0.7558,  ..., -2.0046, -1.1517, -2.4255],\n          [ 1.4361, -2.8033,  1.2664,  ..., -0.6240, -1.2047, -2.1676],\n          [ 0.1815, -3.2912, -0.1822,  ..., -2.0528, -1.2598, -1.1128]],\n\n         [[ 0.4752, -1.7475, -0.2150,  ..., -0.7902, -0.6502, -0.4261],\n          [ 0.5182, -1.6996, -0.1344,  ..., -0.7897, -0.6649, -0.2723],\n          [ 0.9846, -2.3674, -0.0428,  ..., -1.1892, -0.5992, -0.3348],\n          ...,\n          [ 1.1067, -2.0162,  1.3990,  ..., -1.8966,  0.3571, -1.1761],\n          [ 1.0073, -2.1606,  0.9455,  ..., -1.7758, -0.4231, -0.9335],\n          [ 1.1053, -2.2364,  1.0259,  ..., -1.1924,  0.2530, -1.0796]],\n\n         ...,\n\n         [[-1.0925,  1.4432, -0.1876,  ...,  2.9767,  1.4436,  1.6403],\n          [-1.3621,  1.4168, -0.0317,  ...,  2.8498,  1.3926,  1.6778],\n          [-1.7668,  1.5733,  0.0445,  ...,  2.6878,  2.1827,  1.3448],\n          ...,\n          [-1.1425,  2.2596, -1.5580,  ...,  1.8738,  1.5454,  0.0617],\n          [-2.1052,  2.3967, -0.8651,  ...,  1.9586,  2.4492,  1.2936],\n          [ 0.3835,  3.5693, -0.5700,  ...,  1.9738,  2.2641, -0.4707]],\n\n         [[-1.0789, -0.7754, -2.2536,  ...,  2.7950,  0.9580, -3.8067],\n          [-1.0032, -0.9417, -2.1406,  ...,  2.7494,  0.9459, -3.8452],\n          [-1.5478, -1.6342, -1.5379,  ...,  3.0732,  0.6971, -3.4163],\n          ...,\n          [-3.0594,  0.7472, -1.9107,  ...,  2.9530, -0.6005, -2.3643],\n          [-2.1041, -0.7898, -2.0554,  ...,  2.8560, -0.6603, -2.6298],\n          [-2.7012,  0.0887, -1.3241,  ...,  2.2439,  0.1255, -1.4825]],\n\n         [[-3.1398, -1.7791, -0.5779,  ...,  1.0255,  2.2717, -0.1663],\n          [-3.2097, -1.7632, -0.3883,  ...,  0.9636,  2.5626, -0.1281],\n          [-3.3565, -1.2058, -0.5586,  ...,  1.2925,  2.0568, -1.1502],\n          ...,\n          [-2.5252,  1.9653, -1.3197,  ...,  2.6947, -0.2647, -2.0964],\n          [-2.1114,  0.7995, -0.2139,  ...,  2.0154,  0.1727, -1.2797],\n          [-0.4516,  2.3780, -0.8632,  ...,  2.5576, -0.4713, -1.8608]]]],\n       grad_fn=<CloneBackward0>), tensor([[[[-6.9317e-01, -3.5569e-01,  4.6511e-01,  ...,  1.0723e-01,\n           -4.8685e-01,  3.3968e-01],\n          [-6.9429e-01, -3.0025e-01,  4.9629e-01,  ...,  1.0663e-01,\n           -4.2023e-01,  3.3237e-01],\n          [-5.5329e-01, -4.3622e-01,  5.1275e-01,  ...,  6.8260e-02,\n           -6.0709e-01,  2.1612e-01],\n          ...,\n          [ 3.6746e-01, -4.8274e-01,  5.4239e-01,  ..., -2.3029e-02,\n           -6.9050e-01,  9.3816e-01],\n          [-6.3862e-02, -2.5694e-01,  2.4671e-01,  ..., -3.8476e-01,\n           -4.0382e-01,  7.0004e-01],\n          [ 1.3084e-01, -9.2175e-01,  2.3679e-02,  ...,  5.8538e-02,\n           -8.8236e-01,  1.1828e+00]],\n\n         [[-6.4688e-01,  3.5974e-01,  3.5164e-01,  ...,  1.1376e-01,\n           -5.1454e-02,  5.9971e-01],\n          [-6.7272e-01,  4.4349e-01,  3.7698e-01,  ...,  1.8362e-01,\n           -3.9472e-02,  5.9826e-01],\n          [-8.4294e-01,  5.3754e-01,  4.2828e-01,  ...,  1.8474e-01,\n            2.8204e-01,  7.6763e-01],\n          ...,\n          [-8.3956e-01,  4.0869e-01, -5.4251e-02,  ...,  3.4475e-01,\n            4.5773e-01,  7.8621e-01],\n          [-1.1441e+00,  2.3512e-01, -2.3896e-02,  ...,  2.1764e-01,\n            9.6696e-01,  5.2906e-01],\n          [-1.5920e+00, -1.2270e-01,  1.1963e-01,  ...,  6.1478e-01,\n            9.7518e-01,  1.2570e+00]],\n\n         [[ 1.8081e-01,  2.2772e+00, -5.3137e-01,  ...,  1.6022e+00,\n            1.0880e+00,  1.9909e-01],\n          [ 2.4623e-01,  2.1646e+00, -3.6863e-01,  ...,  1.7466e+00,\n            1.1230e+00,  1.7251e-01],\n          [ 7.7703e-01,  2.8973e+00, -2.9822e-01,  ...,  1.6396e+00,\n            1.4708e+00,  2.6758e-01],\n          ...,\n          [ 6.7731e-01,  3.5600e+00, -1.1495e+00,  ...,  1.8799e+00,\n            9.8942e-01,  2.5694e-01],\n          [ 1.6338e-01,  3.1186e+00, -1.0446e+00,  ...,  1.4892e+00,\n            1.1726e+00,  7.2066e-01],\n          [ 7.8726e-01,  4.3313e+00, -1.0008e+00,  ...,  9.2541e-01,\n            8.5003e-01,  1.3932e+00]],\n\n         ...,\n\n         [[-8.6355e-01, -2.5387e-01,  1.3918e-01,  ..., -1.3241e+00,\n           -5.6720e-02, -6.1372e-01],\n          [-8.4889e-01, -2.3125e-01,  9.8145e-02,  ..., -1.2932e+00,\n           -7.0114e-02, -5.7686e-01],\n          [-8.3550e-01, -2.1546e-01,  5.9884e-01,  ..., -1.4383e+00,\n            1.5469e-01, -5.4568e-01],\n          ...,\n          [-9.8013e-02,  8.5591e-02,  1.3422e+00,  ..., -1.2969e+00,\n            1.4206e-01,  2.0034e-01],\n          [-2.5969e-01, -6.6749e-02,  9.3595e-01,  ..., -1.2677e+00,\n            1.2884e-01,  2.1987e-03],\n          [-2.4123e-01, -2.0598e-01,  9.8905e-01,  ..., -7.6590e-01,\n           -2.6197e-01,  2.1045e-01]],\n\n         [[ 5.3983e-01,  3.3313e-01, -1.0180e+00,  ..., -8.4619e-02,\n           -1.2965e+00, -5.2013e-01],\n          [ 5.0452e-01,  3.9734e-01, -9.3675e-01,  ..., -1.1664e-01,\n           -1.3164e+00, -4.8027e-01],\n          [ 7.0062e-01,  3.7742e-01, -1.1693e+00,  ..., -8.5106e-02,\n           -1.4649e+00, -2.7860e-01],\n          ...,\n          [ 1.0577e+00,  4.1199e-01, -1.4347e+00,  ..., -3.3829e-01,\n           -2.0973e+00, -1.8252e-02],\n          [ 1.1523e+00,  8.2110e-01, -1.4010e+00,  ..., -2.7389e-02,\n           -2.2014e+00,  1.3714e-03],\n          [ 1.4422e+00,  6.8224e-01, -1.2536e+00,  ...,  3.4441e-01,\n           -1.9488e+00, -3.2937e-01]],\n\n         [[ 3.7265e-01,  1.4090e+00,  4.1285e-02,  ...,  5.5270e-02,\n            2.6449e-01, -5.7326e-01],\n          [ 3.5805e-01,  1.3728e+00, -3.1692e-02,  ...,  5.8772e-02,\n            2.5288e-01, -5.4531e-01],\n          [ 3.8167e-01,  1.6076e+00,  1.3545e-01,  ...,  1.4339e-01,\n            2.6904e-01, -7.9994e-01],\n          ...,\n          [ 3.7699e-01,  2.1829e+00, -1.4142e-01,  ...,  6.4302e-01,\n            1.0221e-02, -7.8519e-01],\n          [ 5.1768e-01,  2.4092e+00, -3.7206e-01,  ...,  3.8131e-01,\n           -1.1692e-02, -1.1907e+00],\n          [ 7.2003e-01,  2.2450e+00,  8.0862e-02,  ...,  6.7431e-01,\n            8.4997e-02, -8.5021e-01]]]], grad_fn=<CloneBackward0>), tensor([[[[-5.5023e-01, -3.3673e+00, -4.1662e+00,  ..., -1.4951e-03,\n           -3.0347e+00, -2.3286e-01],\n          [ 2.4464e-01, -2.3671e+00, -3.2523e+00,  ..., -2.7968e-01,\n           -3.1202e+00,  1.9546e+00],\n          [ 1.5371e-01, -1.7747e+00, -3.4443e+00,  ...,  1.9784e+00,\n           -2.9081e+00,  3.4456e-01],\n          ...,\n          [-2.0478e-02, -1.9417e+00, -3.2867e+00,  ...,  8.2417e-01,\n           -3.3672e+00,  1.6320e+00],\n          [ 4.8896e-01, -1.9987e+00, -3.3990e+00,  ..., -2.6812e-01,\n           -2.6882e+00,  1.7578e+00],\n          [-7.5059e-02, -2.3654e+00, -3.8903e+00,  ..., -9.6561e-02,\n           -2.8683e+00,  1.3363e+00]],\n\n         [[-1.8497e-02,  2.4478e+00,  2.3717e+00,  ..., -3.4491e+00,\n           -1.6406e+00, -3.7976e-01],\n          [-1.7167e+00,  6.3017e-01,  2.0451e+00,  ..., -4.1144e+00,\n           -3.6920e+00, -2.9143e-01],\n          [-2.9364e+00, -1.1247e+00,  1.2988e+00,  ..., -3.7949e+00,\n           -4.9055e+00, -1.2529e+00],\n          ...,\n          [-1.7902e+00, -2.5283e-01,  2.0009e+00,  ..., -3.9782e+00,\n           -4.3781e+00, -7.1062e-01],\n          [-1.7622e+00,  3.8045e-01,  1.6789e+00,  ..., -4.1718e+00,\n           -4.0291e+00, -3.2952e-01],\n          [-9.7050e-01,  8.5335e-01,  1.9993e+00,  ..., -3.9074e+00,\n           -3.3839e+00, -3.8832e-01]],\n\n         [[-2.9429e+00, -2.8321e+00, -3.6469e+00,  ..., -1.2503e+00,\n           -3.1230e+00,  1.8667e+00],\n          [-3.4203e+00, -2.8097e+00, -3.6832e+00,  ...,  3.0458e-02,\n           -2.5220e+00,  1.7911e+00],\n          [-3.7996e+00, -3.3348e+00, -2.5110e+00,  ...,  1.8092e+00,\n           -3.8281e+00,  1.3124e+00],\n          ...,\n          [-3.6493e+00, -3.2804e+00, -3.3771e+00,  ...,  4.8031e-01,\n           -2.4084e+00,  1.9404e+00],\n          [-3.5683e+00, -2.8486e+00, -3.0629e+00,  ...,  2.2469e-01,\n           -2.5038e+00,  1.6011e+00],\n          [-3.5422e+00, -3.1613e+00, -3.2218e+00,  ..., -2.3918e-01,\n           -2.6765e+00,  2.1237e+00]],\n\n         ...,\n\n         [[ 9.4776e-01, -4.6083e+00, -6.8543e-01,  ..., -2.7990e+00,\n            7.6812e-02, -1.8059e+00],\n          [ 2.5862e+00, -5.0660e+00, -1.0491e+00,  ..., -2.6473e+00,\n           -2.4629e-01, -2.6533e+00],\n          [ 2.7674e+00, -5.0179e+00, -1.4892e+00,  ..., -2.2743e+00,\n            6.5992e-01, -2.3524e+00],\n          ...,\n          [ 3.2441e+00, -4.8645e+00, -7.7708e-01,  ..., -2.5836e+00,\n           -1.6692e-01, -3.2568e+00],\n          [ 2.5082e+00, -4.8630e+00, -1.5476e+00,  ..., -3.3986e+00,\n           -2.0367e-01, -2.6861e+00],\n          [ 2.1265e+00, -4.5861e+00, -1.0546e+00,  ..., -3.0611e+00,\n           -2.9013e-01, -2.6985e+00]],\n\n         [[ 7.7697e+00, -1.9326e+00, -3.5258e+00,  ...,  1.2570e-01,\n            4.6145e+00, -2.6557e+00],\n          [ 7.5609e+00, -1.6046e+00, -3.7211e+00,  ...,  2.2684e+00,\n            3.8969e+00, -1.4406e+00],\n          [ 6.3825e+00, -1.2354e+00, -4.1350e+00,  ...,  6.6482e+00,\n            4.0779e+00,  1.2955e-01],\n          ...,\n          [ 6.8178e+00, -7.4355e-01, -4.1934e+00,  ...,  3.5154e+00,\n            4.6995e+00, -1.2963e+00],\n          [ 7.6564e+00, -1.1065e+00, -4.0790e+00,  ...,  2.2923e+00,\n            3.9185e+00, -1.6663e+00],\n          [ 7.4821e+00, -1.5017e+00, -4.0770e+00,  ...,  1.8456e+00,\n            4.1273e+00, -1.8674e+00]],\n\n         [[ 3.7274e-01,  7.3207e+00,  4.4380e+00,  ..., -2.9788e+00,\n           -6.4154e+00, -2.1994e+00],\n          [ 5.9490e-02,  8.0225e+00,  3.5817e+00,  ...,  3.4102e-01,\n           -6.5118e+00, -1.1802e-01],\n          [-7.6759e-01,  7.3846e+00,  2.8518e+00,  ...,  2.0967e+00,\n           -7.9318e+00,  3.4866e-01],\n          ...,\n          [-6.9703e-01,  7.9991e+00,  3.3334e+00,  ...,  1.0818e+00,\n           -7.1152e+00, -6.6205e-01],\n          [-2.1040e-01,  8.7038e+00,  3.6148e+00,  ...,  1.6630e+00,\n           -6.2874e+00,  5.4480e-01],\n          [-4.8850e-01,  8.1081e+00,  3.7590e+00,  ..., -6.2587e-02,\n           -6.7442e+00, -9.8817e-01]]]], grad_fn=<CloneBackward0>), tensor([[[[-1.3355e+00,  2.9020e+00,  3.3967e+00,  ...,  2.2920e-01,\n            1.9006e+00,  8.7576e-01],\n          [-2.1123e+00,  1.2617e+00,  1.4406e+00,  ...,  7.3417e-01,\n            2.0736e+00,  8.2692e-01],\n          [-2.0710e+00,  8.7179e-01,  1.9426e-01,  ...,  1.6229e+00,\n            3.5377e+00,  9.0498e-01],\n          ...,\n          [-2.6271e+00,  1.2718e+00,  6.9415e-01,  ...,  6.2104e-01,\n            2.0407e+00,  8.9745e-01],\n          [-2.2500e+00,  1.4036e+00,  1.2769e+00,  ...,  4.4562e-01,\n            1.8772e+00,  1.0084e+00],\n          [-2.3506e+00,  1.9501e+00,  1.8279e+00,  ...,  3.1563e-01,\n            1.7895e+00,  1.2576e+00]],\n\n         [[ 6.7960e-02, -2.2042e+00, -8.9967e-01,  ..., -3.2795e+00,\n            2.3380e+00,  5.0242e+00],\n          [-8.1546e-01, -5.5100e-01, -2.7076e-01,  ..., -3.4063e+00,\n            2.1416e+00,  5.0889e+00],\n          [-1.8791e+00, -7.5567e-01,  4.0199e-01,  ..., -4.0049e+00,\n            9.7307e-01,  3.9903e+00],\n          ...,\n          [-1.6180e+00, -2.9472e-01,  1.5070e-02,  ..., -3.5263e+00,\n            1.8354e+00,  4.5641e+00],\n          [-9.4933e-01, -8.5637e-01,  2.7448e-04,  ..., -3.6457e+00,\n            2.6225e+00,  4.6694e+00],\n          [-8.0072e-01, -9.8129e-01,  1.8494e-01,  ..., -3.7688e+00,\n            2.1338e+00,  5.3403e+00]],\n\n         [[ 3.4459e-01, -2.6664e+00,  8.7055e-02,  ...,  9.7797e-01,\n            2.6774e+00, -1.3979e-01],\n          [ 6.3313e-01, -1.7216e+00,  5.5690e-01,  ...,  1.6891e+00,\n            3.0341e+00, -6.4054e-01],\n          [ 1.6582e+00, -5.1289e-01,  2.7367e-01,  ...,  3.1621e+00,\n            2.2087e+00, -1.7468e+00],\n          ...,\n          [ 7.7096e-01, -1.6097e+00,  5.3409e-01,  ...,  1.9818e+00,\n            2.9741e+00, -8.5077e-01],\n          [ 1.4369e+00, -1.4960e+00,  6.0264e-01,  ...,  2.0493e+00,\n            3.5233e+00, -7.8052e-01],\n          [ 5.7750e-01, -1.9607e+00,  5.9872e-01,  ...,  1.6945e+00,\n            3.2540e+00, -8.4174e-01]],\n\n         ...,\n\n         [[-1.7412e+00,  2.9533e+00,  6.2268e-01,  ...,  6.0040e-01,\n           -6.0471e-01,  7.8242e-01],\n          [-2.3395e+00,  2.1672e+00,  1.4799e+00,  ...,  1.7230e+00,\n           -1.2782e+00,  2.4751e-01],\n          [-8.5250e-01,  2.8862e+00,  1.8729e+00,  ...,  1.8220e+00,\n           -1.6479e+00, -8.5761e-01],\n          ...,\n          [-1.9846e+00,  2.8354e+00,  1.4474e+00,  ...,  2.1977e+00,\n           -1.6780e+00,  6.7619e-01],\n          [-2.3562e+00,  2.5745e+00,  1.9059e+00,  ...,  1.4977e+00,\n           -1.4751e+00,  5.4081e-02],\n          [-2.3014e+00,  2.5827e+00,  1.3959e+00,  ...,  1.7376e+00,\n           -1.2820e+00,  4.7392e-01]],\n\n         [[-1.4283e+00, -1.7757e+00,  3.4362e-02,  ..., -4.2786e-01,\n            5.7348e-01,  1.5253e+00],\n          [-9.9029e-01, -2.6672e+00,  6.0451e-01,  ..., -4.6420e-01,\n            1.2081e+00,  1.3583e+00],\n          [ 1.1400e-01, -3.7705e+00,  6.7873e-01,  ..., -3.3488e-02,\n            1.6941e+00,  1.3616e+00],\n          ...,\n          [-7.7448e-01, -3.3848e+00,  5.8604e-01,  ..., -3.4759e-02,\n            1.4586e+00,  1.2895e+00],\n          [-7.8267e-01, -3.0568e+00,  4.6654e-01,  ..., -4.6526e-01,\n            1.4713e+00,  1.4846e+00],\n          [-1.1712e+00, -2.6521e+00,  2.5836e-01,  ..., -2.9239e-01,\n            1.3450e+00,  1.5417e+00]],\n\n         [[-4.0542e-01, -2.1310e+00, -3.7952e-01,  ..., -2.4153e+00,\n           -2.0040e+00,  3.2194e-01],\n          [ 5.7992e-02, -2.3772e+00, -9.8137e-03,  ..., -2.9485e+00,\n           -1.5840e+00,  6.6254e-01],\n          [-4.9110e-01, -2.9540e+00,  8.8644e-02,  ..., -2.7451e+00,\n           -4.7827e-01,  1.1117e+00],\n          ...,\n          [ 1.4947e-01, -2.4222e+00, -1.4760e-01,  ..., -3.0370e+00,\n           -7.5810e-01,  7.3846e-01],\n          [ 6.5603e-03, -2.4432e+00, -1.0274e-01,  ..., -3.0717e+00,\n           -1.0406e+00,  7.9059e-01],\n          [ 1.7048e-01, -2.1738e+00, -2.1729e-01,  ..., -2.7018e+00,\n           -1.4203e+00,  4.3634e-01]]]], grad_fn=<CloneBackward0>)), (tensor([[[[-1.5024,  1.2062, -2.6066,  ...,  0.2469,  0.6912,  1.4295],\n          [-1.4716,  1.2139, -2.4693,  ...,  0.1885,  0.6482,  1.3876],\n          [-1.8204,  1.2974, -2.4343,  ...,  0.6710,  0.2300,  1.5438],\n          ...,\n          [-3.2491,  0.8033, -2.6319,  ...,  2.0863, -1.2186,  1.5549],\n          [-2.3427,  1.1319, -2.3012,  ...,  1.4439, -0.4169,  1.0035],\n          [-2.3443,  0.7662, -2.7538,  ...,  2.4628, -1.2521,  1.2942]],\n\n         [[-2.6043, -5.6011, -0.7874,  ..., -0.9703,  2.3956, -6.9742],\n          [-2.6244, -5.7417, -0.7926,  ..., -0.8866,  2.3518, -7.0314],\n          [-1.7723, -5.1113, -1.0798,  ..., -1.0182,  1.8218, -6.3378],\n          ...,\n          [-0.7014, -3.1405, -0.9396,  ..., -1.7678, -0.0448, -3.8744],\n          [-1.1465, -4.5022, -0.5217,  ..., -2.2420,  0.7402, -5.8222],\n          [-0.6823, -2.5107, -0.6226,  ..., -2.3843, -1.0368, -2.9693]],\n\n         [[-1.0306, -0.2371,  2.4269,  ...,  0.1299,  0.7759, -0.5655],\n          [-0.9774, -0.1684,  2.4472,  ...,  0.1900,  0.7238, -0.5834],\n          [-1.0773,  0.2532,  2.4556,  ..., -0.2138,  0.5879, -1.1310],\n          ...,\n          [-1.0788,  0.9279,  1.4788,  ..., -1.0446,  0.4675, -1.6168],\n          [-0.9907,  0.3545,  2.2960,  ..., -0.2699,  0.2872, -1.1845],\n          [-1.4429,  1.0620,  1.1840,  ..., -1.0262,  0.6374, -1.6647]],\n\n         ...,\n\n         [[-2.4850, -0.3824,  5.7194,  ..., -0.5660,  0.6618, -0.1434],\n          [-2.5210, -0.4213,  5.7349,  ..., -0.5055,  0.7059, -0.2220],\n          [-1.4880, -0.8451,  5.2672,  ..., -0.4920,  0.6850, -0.0362],\n          ...,\n          [ 0.5046, -1.7983,  3.6058,  ..., -0.4376,  0.5770, -0.0866],\n          [-0.7885, -1.1916,  4.4046,  ..., -0.5195,  0.6177, -0.1839],\n          [ 1.6452, -1.9752,  1.9674,  ..., -0.6417,  0.1607,  0.0350]],\n\n         [[-0.6839,  0.1893, -4.2241,  ..., -3.0236,  0.6769, -0.8227],\n          [-0.7900,  0.3323, -4.2206,  ..., -3.0666,  0.7543, -0.7868],\n          [-0.9616, -0.0878, -3.9855,  ..., -2.4998,  1.1532, -0.8565],\n          ...,\n          [-0.9105, -1.9749, -2.2997,  ..., -0.6227,  1.6999, -0.9080],\n          [-1.7056, -1.1185, -3.2506,  ..., -1.3400,  1.5461, -0.0717],\n          [-0.7993, -3.1507, -1.1634,  ...,  1.0702,  2.3558, -0.2052]],\n\n         [[ 2.1146,  2.3551,  3.0007,  ...,  3.3618,  1.6193,  0.0910],\n          [ 2.0975,  2.3539,  2.9803,  ...,  3.4024,  1.6481,  0.0457],\n          [ 2.0058,  2.0896,  3.3024,  ...,  3.1649,  1.6355,  0.1764],\n          ...,\n          [ 1.1056,  1.7415,  2.8314,  ...,  2.3594,  1.2626,  0.6830],\n          [ 2.1978,  2.0496,  3.4478,  ...,  2.8258,  2.4308,  1.1182],\n          [ 0.8562,  1.3437,  2.3507,  ...,  1.7101,  0.9235,  1.2481]]]],\n       grad_fn=<CloneBackward0>), tensor([[[[-1.1619,  1.5578, -0.8576,  ..., -1.2452,  0.0864, -1.0581],\n          [-1.1620,  1.4664, -0.8886,  ..., -1.2194,  0.1217, -1.0367],\n          [-1.2133,  1.8070, -1.1795,  ..., -1.2521,  0.0734, -1.1589],\n          ...,\n          [-1.6501,  2.4937, -1.5282,  ..., -1.8007, -0.0244, -0.8784],\n          [-1.7788,  1.9480, -1.3277,  ..., -1.6886,  0.1658, -1.2750],\n          [-1.6296,  2.8224, -1.0924,  ..., -2.1675,  0.4309, -0.6631]],\n\n         [[ 0.6552,  0.3733, -0.2052,  ...,  0.5501, -0.5021,  0.0287],\n          [ 0.6321,  0.3773, -0.1984,  ...,  0.5721, -0.5175,  0.0455],\n          [ 0.7156,  0.3411, -0.1738,  ...,  0.5498, -0.4102,  0.1707],\n          ...,\n          [ 0.8614,  0.4857, -0.1117,  ...,  0.0198, -0.1935,  0.1901],\n          [ 0.8319,  0.4203, -0.1639,  ...,  0.3547, -0.3020,  0.2184],\n          [ 0.8706,  0.2778, -0.2719,  ...,  0.0314,  0.1028,  0.2366]],\n\n         [[-0.5004, -0.1967, -0.1469,  ...,  2.1472, -0.2150, -1.1067],\n          [-0.5309, -0.1853, -0.1550,  ...,  2.0992, -0.2170, -1.0941],\n          [-0.7010, -0.4390,  0.0289,  ...,  2.3479, -0.0981, -1.2773],\n          ...,\n          [-0.6189, -0.7872,  0.0090,  ...,  2.4916,  0.7269, -1.2806],\n          [-0.4470, -0.6984, -0.0668,  ...,  2.3620, -0.1453, -1.2794],\n          [-1.0233, -0.7077,  0.9195,  ...,  2.5664,  0.4539, -1.0462]],\n\n         ...,\n\n         [[-0.0533, -0.5693, -0.7286,  ...,  0.2133, -0.2614,  0.1223],\n          [-0.0080, -0.5646, -0.7069,  ...,  0.2172, -0.2575,  0.1227],\n          [ 0.0467, -0.8006, -0.7626,  ...,  0.4019, -0.4010,  0.1320],\n          ...,\n          [-0.1892, -1.3340, -0.6770,  ...,  0.8185, -0.4747,  0.0518],\n          [ 0.0264, -0.9715, -0.7132,  ...,  0.6681, -0.5611,  0.1084],\n          [-0.0244, -1.6957, -0.6905,  ...,  1.1344, -0.5563,  0.3529]],\n\n         [[ 0.1404, -0.4271,  0.1775,  ...,  0.0560,  0.2434,  0.4613],\n          [ 0.1552, -0.4107,  0.1782,  ...,  0.0845,  0.2485,  0.4589],\n          [ 0.2931, -0.4518,  0.0124,  ...,  0.0363,  0.2754,  0.4674],\n          ...,\n          [ 0.5473, -0.5934,  0.0480,  ..., -0.2860,  0.1632,  0.5519],\n          [ 0.5081, -0.5307,  0.0394,  ..., -0.1895,  0.2503,  0.3504],\n          [ 0.7792, -0.4342,  0.0277,  ..., -0.4934,  0.4074,  0.5136]],\n\n         [[-0.1222,  0.2171,  0.3104,  ...,  0.5142, -0.1103,  0.1074],\n          [-0.1184,  0.2059,  0.2718,  ...,  0.4639, -0.0953,  0.1157],\n          [-0.2079,  0.1843, -0.0264,  ...,  0.7049, -0.2510,  0.1519],\n          ...,\n          [-0.6902, -0.2197, -0.4858,  ...,  1.0403, -0.2846,  0.0878],\n          [-0.5944, -0.0103, -0.0611,  ...,  0.7357, -0.0944,  0.1998],\n          [-0.6162, -0.3538, -0.6899,  ...,  0.7775, -0.4247,  0.2494]]]],\n       grad_fn=<CloneBackward0>), tensor([[[[-3.8853,  1.7486, -4.2908,  ...,  1.0175, -4.1072, -3.1031],\n          [-3.5139,  2.5036, -2.7809,  ...,  0.6788, -1.6546, -4.2903],\n          [-3.5002,  2.6960, -2.1947,  ...,  1.0760, -1.1761, -3.9298],\n          ...,\n          [-3.5704,  2.7275, -2.6907,  ...,  1.0358, -1.6852, -4.0126],\n          [-4.1658,  2.5641, -2.4256,  ...,  0.5492, -1.2230, -4.4120],\n          [-4.1655,  2.4385, -3.0778,  ...,  1.0965, -2.4205, -3.7944]],\n\n         [[ 1.6029,  1.7702,  2.3156,  ..., -0.7935, -0.3417, -4.5117],\n          [ 2.5351,  2.2200,  1.7865,  ..., -1.7685, -0.5418, -4.7439],\n          [ 3.7097,  2.3901,  1.2163,  ..., -2.0270, -0.4190, -2.8427],\n          ...,\n          [ 2.8265,  1.8457,  1.2166,  ..., -2.0351, -0.3394, -3.8872],\n          [ 2.3591,  2.2427,  1.7971,  ..., -2.1129, -0.4333, -4.9024],\n          [ 2.1453,  1.7671,  1.7403,  ..., -1.5304, -0.4077, -4.3539]],\n\n         [[ 2.6752, -1.9524, -0.0407,  ..., -4.2446,  3.1982, -2.8237],\n          [ 2.9307, -1.4519,  0.5953,  ..., -3.2073,  3.2895, -3.0939],\n          [ 1.6134, -0.7567,  0.2878,  ..., -2.7751,  2.0473, -3.2896],\n          ...,\n          [ 2.1902, -1.6623,  0.5061,  ..., -2.8830,  3.4499, -3.1487],\n          [ 2.6974, -1.5460, -0.0127,  ..., -2.4894,  2.7145, -3.0281],\n          [ 2.3904, -1.6321, -0.0524,  ..., -3.0098,  3.3987, -2.8309]],\n\n         ...,\n\n         [[ 2.0170,  0.5563, -1.5493,  ...,  0.1637, -2.7266, -2.6792],\n          [ 0.5638, -0.6678, -1.1161,  ...,  0.7412, -1.1702, -2.4015],\n          [ 1.5756, -1.3645, -2.4004,  ...,  1.7288, -1.8859, -2.6171],\n          ...,\n          [ 0.9255, -0.4275, -1.4762,  ...,  0.9130, -0.8487, -2.4008],\n          [ 0.8850, -1.4389, -0.5199,  ...,  1.0914, -1.2128, -2.6175],\n          [ 1.1603, -0.6666, -1.4716,  ...,  0.9579, -1.5919, -2.3668]],\n\n         [[ 3.5083,  4.0191, -1.1839,  ..., -1.5321,  3.7607,  1.2842],\n          [ 4.3388,  4.8182, -1.8079,  ..., -0.1086,  4.9125,  0.7105],\n          [ 3.9923,  5.2814, -2.6329,  ...,  0.9516,  5.1400,  0.6965],\n          ...,\n          [ 3.9850,  5.4580, -2.5757,  ...,  0.0719,  5.2322,  0.6438],\n          [ 4.7682,  5.3971, -2.8133,  ...,  0.4471,  5.3245,  0.7884],\n          [ 3.7358,  5.0425, -2.0733,  ..., -0.6947,  4.6311,  0.4413]],\n\n         [[-4.7513, -0.8668, -2.5759,  ..., -4.5724,  1.7520, -1.1524],\n          [-5.4368, -1.5231, -3.5235,  ..., -4.9242,  1.4188, -2.0390],\n          [-6.0989, -1.8109, -3.2295,  ..., -3.2623,  2.8134, -1.8046],\n          ...,\n          [-5.8843, -1.2305, -4.2918,  ..., -4.5973,  2.5342, -2.1996],\n          [-5.4142, -0.9158, -3.7517,  ..., -4.7307,  1.8510, -1.9721],\n          [-5.6148, -1.3917, -3.4362,  ..., -4.2346,  1.7272, -1.9531]]]],\n       grad_fn=<CloneBackward0>), tensor([[[[-0.5918,  0.4872, -2.1433,  ..., -0.6622, -2.9515,  0.3044],\n          [-1.0151,  1.9671, -2.3988,  ..., -1.0547, -3.8481,  0.4157],\n          [-1.0250,  2.0808, -1.7486,  ..., -1.8436, -4.7996,  0.0967],\n          ...,\n          [-1.0957,  1.6733, -2.2425,  ..., -1.2051, -4.0187,  0.4257],\n          [-0.8775,  1.7192, -2.5576,  ..., -0.7969, -3.7684,  0.8107],\n          [-0.9075,  1.2009, -2.6735,  ..., -1.1378, -3.9029,  0.9284]],\n\n         [[-2.2506,  3.7596, -0.9105,  ..., -0.0619,  1.7650, -0.8932],\n          [-1.0196,  3.7144, -1.6651,  ..., -1.3490,  0.9819, -0.7027],\n          [-0.6948,  2.6917, -2.1190,  ..., -1.4017,  0.5039,  0.6280],\n          ...,\n          [-0.6431,  2.9992, -2.0987,  ..., -1.8134,  0.4817, -0.4038],\n          [-0.7045,  4.0188, -1.6695,  ..., -2.0449,  0.5768, -0.9623],\n          [-1.6080,  3.8201, -1.5903,  ..., -1.1978,  0.8409, -0.6027]],\n\n         [[ 0.2363, -2.3725,  0.1362,  ..., -1.3217,  1.6000, -1.3282],\n          [ 0.3497, -2.0095, -0.1454,  ..., -0.5439,  1.8932, -1.4611],\n          [ 0.2819, -1.3854, -0.8819,  ..., -0.3302,  1.5593, -1.3020],\n          ...,\n          [ 0.3730, -1.5700, -0.5920,  ..., -0.4317,  1.8705, -1.0785],\n          [ 0.6291, -1.4161, -0.4891,  ..., -0.5532,  1.9005, -1.1299],\n          [ 0.4257, -2.1200, -0.4209,  ..., -0.5125,  1.7109, -1.0269]],\n\n         ...,\n\n         [[-0.9263,  0.6707,  3.0349,  ..., -1.0282, -0.9591, -1.5539],\n          [-2.3452,  0.4768,  3.2257,  ..., -1.6805, -1.0657, -1.3127],\n          [-3.8579,  0.5747,  2.8604,  ..., -2.6043, -0.4526, -0.7336],\n          ...,\n          [-2.9922,  0.3745,  2.9936,  ..., -2.1026, -0.7807, -1.2560],\n          [-2.5590,  0.4286,  3.1119,  ..., -1.8337, -1.0870, -0.9148],\n          [-1.9986,  0.4400,  3.2843,  ..., -1.8088, -1.2628, -1.2506]],\n\n         [[-1.4526, -1.6580,  1.8030,  ...,  0.9509,  0.9809,  1.6616],\n          [-1.0115, -0.8049,  1.1144,  ...,  0.8199,  0.9824,  1.7233],\n          [-0.6587, -0.5498,  0.3296,  ...,  1.2896,  0.7989,  1.1207],\n          ...,\n          [-0.7422, -0.9155,  0.8302,  ...,  0.3404,  0.6818,  1.6834],\n          [-0.8491, -0.4485,  1.0487,  ...,  0.7452,  1.1067,  1.4627],\n          [-0.8886, -0.6898,  1.5257,  ...,  0.7265,  1.0786,  1.8515]],\n\n         [[ 3.5079,  1.0431, -0.6384,  ..., -4.9126, -2.2296,  2.0275],\n          [ 2.5731,  0.9576, -0.1255,  ..., -3.5345, -2.5207,  2.9137],\n          [ 1.6767,  1.2589,  0.6683,  ..., -2.1446, -2.0488,  3.0122],\n          ...,\n          [ 2.1812,  1.2416, -0.1694,  ..., -3.0121, -2.7550,  2.6434],\n          [ 2.4645,  0.6414,  0.1121,  ..., -3.4762, -2.7586,  2.6620],\n          [ 3.0937,  0.8901, -0.3677,  ..., -3.7612, -2.6050,  2.7156]]]],\n       grad_fn=<CloneBackward0>)), (tensor([[[[ 2.3246e-01,  1.6856e+00, -1.3471e+01,  ..., -1.9830e+00,\n           -9.4796e-01,  2.9772e+00],\n          [ 2.2204e-01,  1.7026e+00, -1.3583e+01,  ..., -2.0559e+00,\n           -9.3604e-01,  3.0336e+00],\n          [-9.4175e-02,  1.2823e+00, -1.2394e+01,  ..., -1.7674e+00,\n           -1.0054e+00,  2.8414e+00],\n          ...,\n          [-5.0182e-01,  3.8875e-01, -8.5424e+00,  ..., -9.6525e-01,\n           -9.5179e-01,  1.8853e+00],\n          [-1.7813e-01,  1.1283e+00, -1.1226e+01,  ..., -1.4530e+00,\n           -8.0649e-01,  2.4301e+00],\n          [-4.8929e-01,  7.2801e-03, -6.3267e+00,  ..., -1.9852e-02,\n           -9.6652e-01,  1.1270e+00]],\n\n         [[ 4.2958e-01,  3.5606e-01, -2.9473e+00,  ..., -8.2029e-01,\n            1.4004e+00,  9.5761e-01],\n          [ 4.4412e-01,  3.6906e-01, -2.9469e+00,  ..., -8.1842e-01,\n            1.3547e+00,  9.8438e-01],\n          [ 4.1148e-01,  1.4472e-01, -2.8276e+00,  ..., -1.1777e+00,\n            1.0502e+00,  1.0487e+00],\n          ...,\n          [ 3.6844e-01, -8.7015e-03, -2.0165e+00,  ..., -1.7548e+00,\n           -4.2029e-01,  8.1656e-01],\n          [ 7.7085e-02,  5.1039e-02, -2.1595e+00,  ..., -1.2872e+00,\n            5.3808e-01,  1.1724e+00],\n          [-7.9874e-02,  4.5098e-01, -1.8947e+00,  ..., -1.4806e+00,\n            8.5616e-02,  4.7183e-01]],\n\n         [[-1.0399e+00,  1.9297e+00,  2.1680e-01,  ...,  6.3650e-01,\n           -5.8003e-01,  5.3265e+00],\n          [-1.1164e+00,  1.9452e+00,  2.0086e-01,  ...,  6.3397e-01,\n           -5.8202e-01,  5.4100e+00],\n          [-1.0108e+00,  1.5375e+00,  3.1502e-01,  ...,  9.5201e-01,\n           -8.9353e-01,  4.8392e+00],\n          ...,\n          [-1.0949e+00,  6.8092e-01,  7.8299e-01,  ...,  1.6662e+00,\n           -1.2006e+00,  3.1958e+00],\n          [-7.7044e-01,  1.1772e+00,  6.8595e-01,  ...,  1.3540e+00,\n           -1.3675e+00,  4.3873e+00],\n          [-1.0171e+00, -1.4630e-01,  5.7511e-01,  ...,  1.9746e+00,\n           -2.1132e+00,  2.0475e+00]],\n\n         ...,\n\n         [[ 6.5470e+00, -1.0915e+01,  5.5399e+00,  ...,  2.7977e+00,\n            6.7395e+00, -1.6114e+00],\n          [ 6.5801e+00, -1.0932e+01,  5.3989e+00,  ...,  2.6664e+00,\n            6.7769e+00, -1.5414e+00],\n          [ 5.9853e+00, -1.0136e+01,  4.7914e+00,  ...,  3.5197e+00,\n            5.9749e+00, -8.5682e-01],\n          ...,\n          [ 4.6161e+00, -7.6276e+00,  2.7234e+00,  ...,  5.0409e+00,\n            4.2678e+00, -1.0138e+00],\n          [ 5.1201e+00, -9.3996e+00,  3.1416e+00,  ...,  3.5576e+00,\n            5.3730e+00, -1.6277e+00],\n          [ 2.9618e+00, -6.5109e+00,  2.3053e+00,  ...,  4.8087e+00,\n            2.7057e+00, -5.5374e-01]],\n\n         [[-1.1596e+00, -6.8232e-01, -1.9872e-01,  ..., -2.7384e-02,\n            1.0767e-01,  1.7608e-01],\n          [-1.1617e+00, -6.6225e-01, -1.7440e-01,  ..., -6.3755e-02,\n            6.0530e-02,  2.1563e-01],\n          [-7.3798e-01, -4.8690e-01, -4.6394e-01,  ..., -1.4580e-02,\n            2.9831e-01,  9.6854e-03],\n          ...,\n          [ 2.2858e-01,  4.3875e-01, -5.0275e-01,  ...,  1.7763e-01,\n            7.2986e-01, -3.6769e-01],\n          [-5.6220e-01, -1.5998e-01, -3.8418e-01,  ...,  8.2090e-02,\n            3.8999e-01, -1.2020e-01],\n          [ 6.9552e-01,  3.6221e-01, -3.6277e-01,  ...,  2.7023e-01,\n            1.1655e+00, -7.1960e-01]],\n\n         [[-8.4875e+00, -9.1488e+00,  2.7447e+00,  ...,  2.9758e+00,\n           -1.3991e+00, -1.0145e+00],\n          [-8.5341e+00, -9.2196e+00,  2.7394e+00,  ...,  3.0715e+00,\n           -1.4670e+00, -1.0259e+00],\n          [-7.8862e+00, -8.6075e+00,  2.5763e+00,  ...,  2.7681e+00,\n           -1.0874e+00, -9.2428e-01],\n          ...,\n          [-5.5032e+00, -6.7012e+00,  2.1629e+00,  ...,  1.4852e+00,\n           -3.1363e-01, -4.2709e-01],\n          [-7.2519e+00, -7.6806e+00,  2.2342e+00,  ...,  2.1206e+00,\n           -9.3018e-01, -7.0429e-01],\n          [-4.5534e+00, -5.0382e+00,  1.7409e+00,  ...,  7.0067e-01,\n            1.4258e-01, -1.9265e-02]]]], grad_fn=<CloneBackward0>), tensor([[[[ 7.7642e-01,  1.5431e+00, -4.1487e-01,  ..., -6.2554e-01,\n           -6.5974e-01, -1.2239e+00],\n          [ 7.9698e-01,  1.4811e+00, -4.0653e-01,  ..., -6.0373e-01,\n           -6.7403e-01, -1.1985e+00],\n          [ 6.9175e-01,  1.8874e+00, -5.6098e-01,  ..., -7.2986e-01,\n           -9.2400e-01, -1.4096e+00],\n          ...,\n          [ 5.4809e-01,  2.7116e+00, -6.5995e-01,  ..., -1.0665e+00,\n           -1.1299e+00, -1.7677e+00],\n          [ 4.7572e-01,  2.4108e+00, -6.1250e-01,  ..., -8.8295e-01,\n           -7.9322e-01, -1.4403e+00],\n          [ 4.7330e-01,  3.0590e+00, -9.7315e-01,  ..., -9.5846e-01,\n           -8.5972e-01, -1.9953e+00]],\n\n         [[ 7.4473e-01,  9.8588e-02, -1.3501e-01,  ...,  4.7328e-01,\n            3.0302e-01,  5.4016e-02],\n          [ 7.5246e-01,  1.1366e-01, -1.3102e-01,  ...,  4.6939e-01,\n            3.1132e-01,  6.5342e-02],\n          [ 9.5156e-01,  8.4087e-02, -1.4097e-01,  ...,  5.7237e-01,\n            4.0762e-01,  5.3600e-02],\n          ...,\n          [ 1.4616e+00,  1.9786e-01, -1.5492e-01,  ...,  9.4454e-01,\n            4.4818e-01,  7.6823e-02],\n          [ 1.1093e+00,  9.0059e-02, -1.2264e-01,  ...,  7.6960e-01,\n            5.0566e-01, -2.1514e-02],\n          [ 1.6020e+00,  1.0999e-01, -8.8905e-02,  ...,  9.3388e-01,\n            1.9664e-01, -6.8731e-02]],\n\n         [[ 2.3843e-01,  1.7201e-01,  2.5836e-01,  ..., -5.5499e-01,\n           -7.3503e-01,  1.7375e+00],\n          [ 2.3665e-01,  1.4455e-01,  2.3784e-01,  ..., -5.5591e-01,\n           -7.0628e-01,  1.7158e+00],\n          [ 1.9095e-01,  1.5501e-01,  4.0992e-01,  ..., -5.3254e-01,\n           -9.2078e-01,  1.8586e+00],\n          ...,\n          [-1.5275e-02, -1.8524e-01,  8.8358e-01,  ..., -6.0095e-01,\n           -1.4984e+00,  1.9621e+00],\n          [ 1.2470e-01, -6.9348e-02,  4.7719e-01,  ..., -6.7518e-01,\n           -1.2370e+00,  2.0191e+00],\n          [-3.4065e-01,  3.7413e-01,  8.5602e-01,  ..., -7.2233e-01,\n           -1.7763e+00,  1.8955e+00]],\n\n         ...,\n\n         [[-4.5396e-01, -3.9106e-01, -3.4095e-01,  ..., -7.2675e-01,\n            9.2461e-01, -3.5662e-01],\n          [-4.2842e-01, -4.0557e-01, -3.4426e-01,  ..., -6.9288e-01,\n            9.2663e-01, -3.5270e-01],\n          [-5.0004e-01, -4.9741e-01, -2.9746e-01,  ..., -9.2499e-01,\n            9.6328e-01, -4.0890e-01],\n          ...,\n          [-4.7557e-01, -7.4274e-01, -2.6464e-01,  ..., -1.1520e+00,\n            1.0874e+00, -3.5890e-01],\n          [-3.1224e-01, -8.5168e-01, -2.1329e-01,  ..., -7.8459e-01,\n            8.5180e-01, -3.7067e-01],\n          [-5.4766e-01, -6.7831e-01, -2.7285e-01,  ..., -1.4683e+00,\n            8.9292e-01, -2.9340e-01]],\n\n         [[-4.8305e-02, -2.4364e-01, -7.4383e-01,  ...,  6.9467e-01,\n            8.3584e-02, -6.4228e-01],\n          [-6.8575e-02, -2.0571e-01, -7.1672e-01,  ...,  6.6201e-01,\n            5.9566e-02, -6.4717e-01],\n          [-1.7956e-01, -2.2522e-01, -9.0026e-01,  ...,  6.0486e-01,\n            1.3618e-01, -5.7997e-01],\n          ...,\n          [ 4.5019e-02, -3.3046e-01, -9.2709e-01,  ...,  5.0676e-01,\n            1.4234e-03, -7.6704e-01],\n          [-1.0098e-01, -2.5726e-01, -4.0268e-01,  ...,  5.5316e-01,\n           -1.9137e-02, -6.0311e-01],\n          [ 4.8462e-02, -6.7457e-01, -9.0050e-01,  ...,  3.8287e-01,\n            1.2935e-01, -7.1597e-01]],\n\n         [[ 2.2564e-01,  1.6286e+00, -8.8657e-01,  ...,  2.3097e-01,\n            1.6755e-02,  2.6579e-01],\n          [ 2.0170e-01,  1.6371e+00, -8.6489e-01,  ...,  2.3853e-01,\n            1.1729e-02,  2.9605e-01],\n          [ 1.5132e-01,  1.7683e+00, -9.8677e-01,  ...,  3.8099e-01,\n            1.5302e-01,  3.2066e-01],\n          ...,\n          [-1.8110e-01,  2.3105e+00, -9.4120e-01,  ...,  3.5335e-01,\n            1.7951e-01,  1.7697e-01],\n          [-4.9318e-02,  2.1350e+00, -9.2883e-01,  ...,  2.7760e-01,\n            9.8963e-02,  1.9036e-01],\n          [-2.4814e-01,  2.4018e+00, -1.0691e+00,  ...,  4.2225e-01,\n           -3.5778e-02,  4.1145e-02]]]], grad_fn=<CloneBackward0>), tensor([[[[-7.8258,  2.3707,  0.4339,  ..., -1.2983, -6.0839, -1.5234],\n          [-8.2897,  2.9022,  0.3055,  ..., -0.6319, -6.7102, -2.4885],\n          [-8.0602,  3.2092, -0.3483,  ..., -1.3905, -5.7470, -3.8495],\n          ...,\n          [-8.7093,  2.6911,  0.3388,  ..., -1.0223, -6.7999, -2.4037],\n          [-8.3217,  3.3896,  0.7472,  ..., -0.6295, -7.3135, -1.6989],\n          [-8.1046,  2.4415,  0.1359,  ..., -0.5984, -7.0199, -2.1242]],\n\n         [[-0.5839, -1.1120, -1.4009,  ..., -5.7011, -0.0724, -0.5527],\n          [ 0.3707, -0.7083, -0.7048,  ..., -5.2537, -0.6482, -0.9172],\n          [-0.1341, -1.6911, -0.7625,  ..., -4.9534,  0.8015, -0.6618],\n          ...,\n          [ 0.2567, -1.1647, -0.5678,  ..., -4.8426, -0.3502, -0.4205],\n          [ 0.6337, -1.1228, -0.8881,  ..., -5.2990, -0.8192, -1.0678],\n          [ 0.4355, -1.3934, -0.7032,  ..., -5.3427, -0.6640, -0.2988]],\n\n         [[ 3.0432, -1.8289,  0.0256,  ..., -0.1024,  1.4530,  5.3465],\n          [ 3.0778, -1.6917,  0.5667,  ...,  0.0087,  2.4426,  5.2748],\n          [ 2.7878, -1.7930,  0.9762,  ..., -1.4387,  2.7414,  4.8934],\n          ...,\n          [ 2.3476, -1.5032,  0.8294,  ..., -0.4946,  1.7845,  5.2975],\n          [ 2.9301, -2.2785,  0.1011,  ..., -0.1306,  2.2274,  5.2345],\n          [ 2.7150, -1.6334,  0.2997,  ..., -0.4580,  1.6530,  5.5954]],\n\n         ...,\n\n         [[-4.1679, -2.7888,  3.1914,  ..., -2.9323, -1.0571,  2.9899],\n          [-3.2270, -3.3048,  1.8042,  ..., -2.1623, -0.9599,  1.6013],\n          [-2.0798, -2.9883,  1.5728,  ..., -1.1186, -1.2858,  0.2147],\n          ...,\n          [-2.9840, -2.8571,  1.2397,  ..., -2.8675, -0.4489,  1.4774],\n          [-2.8372, -3.3279,  1.4619,  ..., -1.9519, -1.1395,  0.6612],\n          [-3.1762, -2.7730,  1.7747,  ..., -2.6737, -0.7384,  1.7900]],\n\n         [[-1.3422, -2.4810, -1.7709,  ...,  1.3208,  2.3070,  2.8977],\n          [-1.8139, -1.2429, -3.0477,  ...,  2.4500,  1.3140,  2.0746],\n          [-2.4182, -0.7766, -3.4959,  ...,  3.0554,  0.6303,  1.4523],\n          ...,\n          [-1.6291, -1.0666, -3.4678,  ...,  2.6815,  1.2698,  1.6499],\n          [-2.0779, -1.3854, -3.2501,  ...,  2.8249,  0.9747,  1.9051],\n          [-1.9702, -1.2390, -2.8693,  ...,  2.0318,  1.7651,  2.4274]],\n\n         [[-1.8055,  0.1757,  1.2263,  ..., -1.3708, -0.9015,  1.8470],\n          [-0.6103, -1.4481,  1.4355,  ..., -1.8428, -0.3639,  0.7620],\n          [-0.8594, -1.9014,  0.2682,  ..., -1.6942, -1.4536,  0.0678],\n          ...,\n          [-0.3399, -1.8180,  0.4211,  ..., -1.9110, -0.3769,  0.4229],\n          [-0.8301, -1.6182,  1.0892,  ..., -2.0956, -0.1715,  0.5754],\n          [-0.7004, -1.2122,  0.8014,  ..., -2.0912, -0.5500,  0.9973]]]],\n       grad_fn=<CloneBackward0>), tensor([[[[ 1.7359e+00, -1.6957e+00,  6.5567e-01,  ..., -2.7600e+00,\n            3.0317e+00,  1.6608e+00],\n          [ 1.4241e+00, -1.8709e+00, -8.8979e-02,  ..., -2.8962e+00,\n            3.1047e+00,  1.8138e+00],\n          [ 8.8499e-01, -2.6687e+00, -1.1122e+00,  ..., -2.9390e+00,\n            3.1599e+00,  1.4994e+00],\n          ...,\n          [ 1.3672e+00, -2.0075e+00, -8.1645e-01,  ..., -2.9084e+00,\n            3.1084e+00,  1.7951e+00],\n          [ 1.1087e+00, -1.9669e+00, -1.0074e-01,  ..., -2.8476e+00,\n            2.9587e+00,  1.9517e+00],\n          [ 1.3925e+00, -1.8676e+00, -7.7439e-02,  ..., -2.8724e+00,\n            2.8745e+00,  1.9603e+00]],\n\n         [[-2.2229e+00, -1.7933e+00,  2.9536e+00,  ...,  1.0149e+00,\n           -8.9390e-01, -2.0774e+00],\n          [-1.7399e+00, -2.1230e+00,  2.8694e+00,  ...,  1.8637e+00,\n           -2.7007e-01, -1.2072e+00],\n          [-8.7271e-01, -1.6932e+00,  2.4518e+00,  ...,  1.5035e+00,\n            2.8360e-01, -1.2510e+00],\n          ...,\n          [-1.1394e+00, -2.1285e+00,  2.5827e+00,  ...,  1.8818e+00,\n           -2.5541e-01, -9.3669e-01],\n          [-1.6902e+00, -1.7553e+00,  2.7531e+00,  ...,  1.9345e+00,\n           -1.0882e-02, -7.6162e-01],\n          [-1.7088e+00, -1.8213e+00,  2.7038e+00,  ...,  1.5684e+00,\n           -5.7947e-01, -1.1143e+00]],\n\n         [[ 4.2311e-01,  3.5064e+00, -1.3790e+00,  ...,  2.6159e+00,\n           -3.1378e+00, -2.5119e+00],\n          [-8.4987e-02,  1.8365e+00, -1.2816e+00,  ...,  3.9713e+00,\n           -4.0950e+00, -2.6432e+00],\n          [-1.0085e+00,  1.0890e+00, -1.3976e+00,  ...,  3.7147e+00,\n           -4.0414e+00, -2.6764e+00],\n          ...,\n          [-5.4386e-02,  9.7128e-01, -1.5017e+00,  ...,  4.2996e+00,\n           -4.6478e+00, -2.7366e+00],\n          [-9.3957e-02,  1.8686e+00, -1.0014e+00,  ...,  4.0769e+00,\n           -4.1883e+00, -2.3489e+00],\n          [ 3.1473e-01,  2.1077e+00, -1.3450e+00,  ...,  3.7346e+00,\n           -4.1111e+00, -2.5813e+00]],\n\n         ...,\n\n         [[-2.1745e+00, -4.1722e+00,  3.5654e+00,  ...,  1.3322e+00,\n            2.8844e+00, -2.5679e+00],\n          [-8.4954e-01, -4.5874e+00,  3.9561e+00,  ...,  1.2638e+00,\n            3.2646e+00, -2.3547e+00],\n          [-6.3470e-01, -4.5172e+00,  3.6972e+00,  ...,  1.1468e+00,\n            3.0219e+00, -1.9177e+00],\n          ...,\n          [-4.7756e-01, -4.7268e+00,  3.9444e+00,  ...,  5.6649e-01,\n            2.9734e+00, -2.4152e+00],\n          [-9.7527e-01, -5.0811e+00,  3.7570e+00,  ...,  1.0067e+00,\n            2.9132e+00, -2.5601e+00],\n          [-1.1942e+00, -4.7649e+00,  3.8271e+00,  ...,  1.1715e+00,\n            3.1111e+00, -2.5142e+00]],\n\n         [[-4.9240e-01,  1.4774e-03,  1.9493e+00,  ...,  2.1512e+00,\n           -1.6283e+00, -6.1715e-01],\n          [-1.6780e-01,  3.9698e-01,  2.8894e+00,  ...,  1.2139e+00,\n           -1.5118e+00, -1.0911e+00],\n          [ 4.3960e-02,  1.4404e+00,  2.5234e+00,  ...,  6.6296e-01,\n            3.1819e-01, -7.9044e-01],\n          ...,\n          [-1.5402e-02,  9.9175e-01,  2.6159e+00,  ...,  1.3698e+00,\n           -7.9991e-01, -1.5456e+00],\n          [-2.3285e-01,  5.9330e-01,  2.3469e+00,  ...,  1.2109e+00,\n           -1.2427e+00, -1.1429e+00],\n          [-3.4263e-01,  4.9830e-01,  2.3108e+00,  ...,  1.6665e+00,\n           -1.4187e+00, -1.2371e+00]],\n\n         [[ 1.0655e-01, -1.3712e-01,  1.4395e+00,  ...,  1.1976e-01,\n           -8.1448e-01, -1.1592e-01],\n          [-6.9123e-01, -1.5133e-01,  1.6273e+00,  ...,  5.5008e-01,\n           -9.2970e-01,  1.5476e-01],\n          [-1.0194e+00,  8.5780e-01,  1.4502e+00,  ...,  8.5881e-01,\n           -1.2947e+00,  1.0204e+00],\n          ...,\n          [-7.2992e-01,  5.0580e-02,  1.5605e+00,  ...,  8.5121e-01,\n           -9.3791e-01,  3.7347e-01],\n          [-7.7574e-01,  4.9826e-02,  1.6875e+00,  ...,  2.2869e-01,\n           -5.8241e-01,  7.0763e-02],\n          [-5.5139e-01,  8.8663e-02,  1.5872e+00,  ...,  7.0512e-01,\n           -7.9391e-01,  3.5022e-01]]]], grad_fn=<CloneBackward0>)), (tensor([[[[-1.3742,  1.2549, -0.7989,  ...,  0.8818, -0.9905, -0.8524],\n          [-1.3880,  1.2456, -0.7856,  ...,  0.8894, -0.9466, -0.8692],\n          [-1.2965,  1.4981, -0.8117,  ...,  0.9453, -1.0763, -0.6849],\n          ...,\n          [-0.7062,  1.9409, -0.8313,  ...,  1.0461, -1.3746, -0.2233],\n          [-0.8369,  1.8067, -0.8523,  ...,  1.0862, -1.2623, -0.3386],\n          [-0.6994,  2.4483, -0.6915,  ...,  1.1489, -1.5418, -0.0958]],\n\n         [[-3.6338, -4.0447, -2.4984,  ..., -3.8625,  3.3676, -5.3312],\n          [-3.6237, -4.0766, -2.4606,  ..., -3.8434,  3.3442, -5.2982],\n          [-3.5643, -4.2018, -2.4977,  ..., -3.8077,  3.3600, -5.4839],\n          ...,\n          [-3.5307, -4.0356, -2.7265,  ..., -3.8829,  3.3856, -5.1500],\n          [-3.5925, -4.2230, -2.3906,  ..., -3.6732,  3.2717, -5.2680],\n          [-3.6625, -4.1933, -2.7261,  ..., -4.0765,  3.1494, -4.9619]],\n\n         [[-2.6029,  0.7524, -0.3244,  ...,  2.2772, -1.8658,  0.2885],\n          [-2.5485,  0.7197, -0.3438,  ...,  2.3096, -1.8398,  0.2815],\n          [-2.4633,  0.8929, -0.1401,  ...,  2.1764, -1.7876, -0.1121],\n          ...,\n          [-2.3756,  1.3835,  0.3556,  ...,  1.9372, -1.4808, -0.9907],\n          [-2.4882,  1.1559,  0.1087,  ...,  1.7995, -1.7340, -0.4536],\n          [-2.5151,  1.7928,  0.3137,  ...,  1.7064, -1.3508, -1.3520]],\n\n         ...,\n\n         [[-3.6743,  2.0759, -0.2115,  ...,  1.4732,  2.1481, -0.7850],\n          [-3.6915,  1.9924, -0.2074,  ...,  1.4843,  2.1661, -0.7905],\n          [-3.5794,  1.9604, -0.2382,  ...,  1.1988,  2.0130, -0.8609],\n          ...,\n          [-3.4430,  1.8158, -0.2641,  ...,  0.3510,  1.1294, -0.9276],\n          [-3.3261,  1.5166, -0.0599,  ...,  0.4218,  1.4231, -0.9441],\n          [-2.9773,  1.6630, -0.2691,  ..., -0.3491,  0.5592, -0.9485]],\n\n         [[ 1.9645,  2.0217, -2.5041,  ...,  0.6215,  0.6573, -0.3968],\n          [ 1.9632,  1.9791, -2.5356,  ...,  0.6337,  0.6664, -0.3663],\n          [ 1.9595,  2.1998, -2.4934,  ...,  0.5430,  0.8643, -0.4714],\n          ...,\n          [ 2.0889,  2.9005, -2.4571,  ...,  0.4712,  1.2714, -0.5748],\n          [ 1.9758,  2.3705, -2.6834,  ...,  0.7347,  1.1123, -0.3251],\n          [ 2.1481,  3.4126, -2.3627,  ...,  0.3028,  1.3607, -0.2890]],\n\n         [[-4.3106, -1.8625,  1.2600,  ...,  1.3626,  1.4322,  0.3054],\n          [-4.3040, -1.8655,  1.2057,  ...,  1.3097,  1.4353,  0.3181],\n          [-4.3843, -2.0483,  1.1474,  ...,  1.4548,  1.3569,  0.2593],\n          ...,\n          [-4.4012, -2.4741,  1.3121,  ...,  1.9671,  1.3357,  0.5167],\n          [-4.3863, -2.1963,  1.1711,  ...,  1.8914,  1.2149,  0.3660],\n          [-4.3449, -2.5678,  1.5360,  ...,  2.2373,  1.3479,  0.6865]]]],\n       grad_fn=<CloneBackward0>), tensor([[[[ 3.3632e-01, -9.3785e-02,  3.6347e-01,  ..., -6.7428e-02,\n           -1.0095e-01,  5.4644e-02],\n          [ 3.4663e-01, -9.7334e-02,  3.5680e-01,  ..., -7.9530e-02,\n           -1.0477e-01,  6.2107e-02],\n          [ 2.8091e-01, -5.5957e-02,  3.9444e-01,  ..., -4.2526e-03,\n           -1.2071e-01, -5.5468e-03],\n          ...,\n          [ 2.6819e-01, -5.0791e-03,  4.9408e-01,  ...,  2.4479e-01,\n            2.6467e-02, -2.7812e-01],\n          [ 1.8636e-01, -1.1275e-01,  4.0119e-01,  ...,  1.3215e-01,\n           -3.0693e-02, -1.1153e-01],\n          [ 2.1811e-01, -6.2558e-02,  4.9611e-01,  ...,  3.3142e-01,\n            2.2184e-01, -4.9773e-01]],\n\n         [[ 1.0297e-01,  2.4832e-01, -7.9913e-02,  ...,  1.9325e-01,\n           -6.0031e-01,  2.4626e-01],\n          [ 9.9239e-02,  2.4149e-01, -8.3684e-02,  ...,  1.9094e-01,\n           -5.8181e-01,  2.4811e-01],\n          [ 3.6793e-02,  2.9069e-01, -6.3084e-02,  ...,  2.1478e-01,\n           -7.3689e-01,  1.8237e-01],\n          ...,\n          [-6.8307e-02,  4.1131e-01, -1.2983e-01,  ...,  3.0008e-01,\n           -1.0516e+00, -9.8771e-02],\n          [-3.2087e-02,  3.5294e-01, -2.4911e-02,  ...,  2.7723e-01,\n           -8.0199e-01,  2.3713e-02],\n          [-1.0411e-01,  5.6074e-01,  2.0742e-02,  ...,  3.6453e-01,\n           -1.1889e+00, -1.9714e-01]],\n\n         [[-5.3447e-02, -6.7023e-02, -2.3237e-01,  ...,  2.3230e-01,\n            6.1129e-01,  3.5387e-01],\n          [-7.3875e-02, -7.5200e-02, -2.2790e-01,  ...,  2.4052e-01,\n            6.0897e-01,  3.4193e-01],\n          [-1.7562e-01, -9.3777e-02, -2.3833e-01,  ...,  2.5372e-01,\n            7.0678e-01,  4.0673e-01],\n          ...,\n          [-4.3477e-01, -3.2694e-01, -2.8797e-01,  ...,  2.9995e-01,\n            9.4915e-01,  6.9343e-01],\n          [-2.9554e-01, -3.4870e-01, -2.6240e-01,  ...,  2.5368e-01,\n            8.1579e-01,  5.0330e-01],\n          [-4.4390e-01, -3.4789e-01, -2.4027e-01,  ...,  3.2219e-01,\n            1.0357e+00,  7.5575e-01]],\n\n         ...,\n\n         [[ 7.1391e-01, -4.3118e-01, -1.6111e-02,  ..., -2.1052e-01,\n           -4.1334e-01,  2.7927e-02],\n          [ 6.9720e-01, -4.2924e-01,  1.0081e-03,  ..., -2.1457e-01,\n           -4.1794e-01,  2.4352e-02],\n          [ 7.0771e-01, -5.0452e-01, -8.9482e-02,  ..., -2.1694e-01,\n           -3.8258e-01,  5.2886e-02],\n          ...,\n          [ 9.0851e-01, -6.6439e-01, -3.3154e-01,  ..., -2.1879e-01,\n           -2.7954e-01,  1.3380e-01],\n          [ 7.3915e-01, -5.3023e-01, -1.6444e-01,  ..., -8.7754e-02,\n           -3.2715e-01,  3.1714e-02],\n          [ 9.5584e-01, -8.7261e-01, -3.6817e-01,  ..., -1.4734e-01,\n           -1.8744e-01,  1.2032e-01]],\n\n         [[-5.2934e-01,  2.4449e-01, -9.8153e-02,  ..., -8.9692e-02,\n           -6.7774e-01,  2.0480e-01],\n          [-5.3773e-01,  2.3770e-01, -1.1688e-01,  ..., -9.2454e-02,\n           -6.7481e-01,  2.3265e-01],\n          [-5.5273e-01,  1.7778e-01, -1.3387e-01,  ..., -1.0646e-01,\n           -7.8256e-01,  1.2438e-01],\n          ...,\n          [-5.6391e-01, -8.3203e-02, -1.1847e-01,  ..., -1.5687e-01,\n           -1.1072e+00, -8.5796e-02],\n          [-5.5510e-01,  1.6368e-01, -1.4042e-01,  ..., -1.1967e-01,\n           -9.6564e-01, -1.0153e-02],\n          [-6.8876e-01, -9.1997e-02, -3.7707e-01,  ...,  1.4334e-02,\n           -1.3559e+00, -2.0738e-01]],\n\n         [[-4.1691e-01,  7.2744e-01,  1.6074e-01,  ..., -1.2011e-01,\n            2.3768e-01,  3.6258e-01],\n          [-4.2412e-01,  7.1669e-01,  1.5456e-01,  ..., -1.2629e-01,\n            2.4244e-01,  3.6134e-01],\n          [-4.5415e-01,  9.4690e-01,  2.3796e-01,  ..., -1.9419e-01,\n            3.6224e-01,  2.7149e-01],\n          ...,\n          [-5.7418e-01,  1.6291e+00,  1.3599e-01,  ..., -2.7528e-01,\n            3.0329e-01,  1.2429e-01],\n          [-5.7987e-01,  1.2550e+00,  1.2365e-01,  ..., -4.0105e-02,\n            5.4697e-01,  2.0545e-01],\n          [-8.3384e-01,  2.0140e+00,  2.7532e-01,  ..., -1.4507e-01,\n            5.6231e-01, -1.4960e-01]]]], grad_fn=<CloneBackward0>), tensor([[[[ 1.2998, -3.5460,  2.1501,  ..., -0.2596,  2.0951, -0.1752],\n          [ 0.9948, -2.8822,  2.4721,  ...,  0.1494,  1.6320,  1.1099],\n          [ 0.5805, -1.8629,  2.9381,  ...,  1.1407,  2.3034,  1.5688],\n          ...,\n          [ 0.6817, -2.4021,  2.6234,  ...,  0.8283,  1.4430,  1.4795],\n          [ 0.9289, -3.0306,  2.5604,  ...,  0.5180,  1.6432,  1.6465],\n          [ 0.6152, -3.2694,  2.2671,  ...,  0.3306,  1.7811,  1.0820]],\n\n         [[ 1.4734,  1.6931, -1.3125,  ..., -4.1353,  1.0450,  0.4558],\n          [ 2.1370,  0.6827,  0.9488,  ..., -4.9358,  0.6088, -1.0368],\n          [ 1.7777,  1.0961,  1.3126,  ..., -5.9983, -0.5840, -1.4887],\n          ...,\n          [ 2.0604,  1.1374,  1.3834,  ..., -4.4964, -0.1259, -1.1126],\n          [ 2.3306,  1.2585,  1.4335,  ..., -5.6412,  1.0343, -0.8190],\n          [ 2.0447,  1.4689,  0.4418,  ..., -4.4974,  0.4840, -0.2796]],\n\n         [[-0.3983, -1.7382,  1.1745,  ..., -0.3041, -3.1792, -0.7068],\n          [-0.3033, -2.2201,  2.1197,  ..., -1.3248, -2.5970,  0.3699],\n          [-1.0973, -1.1669,  2.2769,  ..., -1.7793, -2.5727, -0.4196],\n          ...,\n          [-0.5876, -2.0158,  2.5107,  ..., -1.0616, -2.7179,  0.0432],\n          [-1.2461, -2.4526,  1.9084,  ..., -2.0730, -2.3790,  0.0975],\n          [-0.2931, -2.0744,  2.6925,  ..., -0.8344, -2.9065,  0.5367]],\n\n         ...,\n\n         [[-0.0714, -1.4389, -1.6777,  ...,  0.8197, -7.2649, -6.0630],\n          [-0.2307, -2.1234, -1.1023,  ...,  0.5876, -7.6107, -5.7992],\n          [-0.9865, -1.9583,  0.2741,  ...,  0.7480, -7.0766, -3.1575],\n          ...,\n          [-0.7476, -2.4250, -0.9716,  ...,  0.1101, -7.8294, -4.2870],\n          [ 0.1972, -1.5159, -0.9464,  ..., -0.2032, -7.7808, -5.2916],\n          [-0.4460, -2.2078, -1.1739,  ...,  0.1201, -7.5472, -5.2946]],\n\n         [[-2.0069, -4.0261, -1.2847,  ...,  2.5909,  0.6977,  7.6103],\n          [ 0.3701, -4.5648, -0.3884,  ...,  2.8429, -0.2914,  4.7925],\n          [ 0.9688, -2.5768,  1.2399,  ...,  2.3063, -0.5886,  2.4026],\n          ...,\n          [ 0.6066, -4.6611,  0.1109,  ...,  2.3476, -0.6120,  3.8370],\n          [ 0.6986, -5.0266, -0.6262,  ...,  2.4445, -0.5689,  4.4200],\n          [-0.0705, -4.5434, -0.4381,  ...,  2.4504, -0.0889,  5.4151]],\n\n         [[ 0.6971,  0.1714,  1.5342,  ..., -0.5870,  2.4730, -0.9321],\n          [ 0.1398,  0.0632,  0.5558,  ..., -0.4217,  4.4191, -1.4705],\n          [-0.4719,  1.3922, -0.2134,  ..., -2.4112,  4.4500, -2.2561],\n          ...,\n          [-0.2866,  0.8350,  0.5234,  ..., -0.6363,  4.4504, -2.0050],\n          [-0.1098,  0.2155,  1.2503,  ..., -0.9819,  4.8683, -1.3304],\n          [-0.1508,  0.7851,  0.9948,  ..., -0.6368,  4.0986, -1.3797]]]],\n       grad_fn=<CloneBackward0>), tensor([[[[ 8.7664e-01, -3.6280e-01,  2.8426e+00,  ...,  2.9418e+00,\n            1.8870e+00, -1.8350e+00],\n          [ 2.5010e-01, -1.0098e+00,  2.6723e+00,  ...,  2.3057e+00,\n            1.8140e+00, -2.1513e+00],\n          [-3.7926e-02, -2.5588e+00,  1.9998e+00,  ...,  1.1461e+00,\n            2.3812e+00, -2.7628e+00],\n          ...,\n          [ 1.4696e-01, -1.3841e+00,  2.4185e+00,  ...,  1.7976e+00,\n            2.4948e+00, -1.8110e+00],\n          [ 1.3251e-01, -7.4730e-01,  2.8968e+00,  ...,  2.4157e+00,\n            1.7537e+00, -2.2005e+00],\n          [ 3.4911e-01, -7.0817e-01,  2.5889e+00,  ...,  2.3383e+00,\n            2.0594e+00, -2.0178e+00]],\n\n         [[ 1.4131e+00, -6.0489e-01, -7.5039e-01,  ..., -2.9308e-03,\n           -6.2926e-01, -3.0166e+00],\n          [ 8.5147e-01, -3.6329e-01, -7.8196e-01,  ...,  6.9338e-02,\n           -2.3134e-01, -3.0807e+00],\n          [ 6.2215e-01, -1.0115e+00,  2.7320e-01,  ..., -3.9403e-02,\n           -5.2334e-01, -2.2344e+00],\n          ...,\n          [ 6.8983e-01, -6.9872e-01, -5.1882e-01,  ...,  4.0065e-01,\n           -5.6684e-01, -2.8302e+00],\n          [ 4.6027e-01, -6.3474e-01, -6.6064e-01,  ..., -1.7389e-01,\n           -2.4811e-01, -2.8400e+00],\n          [ 8.6692e-01, -6.3810e-01, -9.2174e-01,  ...,  3.2368e-01,\n           -4.6516e-01, -2.9328e+00]],\n\n         [[ 2.4401e+00,  2.1864e-02,  1.2509e+00,  ..., -2.3780e+00,\n           -2.4739e+00,  1.2113e+00],\n          [ 2.9118e+00,  6.8710e-01,  2.2841e+00,  ..., -1.5579e+00,\n           -2.1140e+00,  1.4815e+00],\n          [ 2.8746e+00,  1.1082e+00,  2.9458e+00,  ..., -9.4786e-01,\n           -2.0428e+00,  1.2620e+00],\n          ...,\n          [ 2.6028e+00,  1.3701e+00,  2.3730e+00,  ..., -1.1867e+00,\n           -2.0874e+00,  1.3324e+00],\n          [ 2.6764e+00,  7.7133e-01,  2.4326e+00,  ..., -1.8518e+00,\n           -2.2924e+00,  1.4583e+00],\n          [ 2.7507e+00,  9.1835e-01,  2.1420e+00,  ..., -1.7076e+00,\n           -2.3305e+00,  1.4554e+00]],\n\n         ...,\n\n         [[-1.5123e+00,  2.5458e+00, -3.2771e-02,  ...,  1.3808e+00,\n           -7.4974e-01,  4.7597e+00],\n          [-1.9509e+00,  1.9043e+00,  8.7924e-01,  ...,  1.2253e+00,\n           -7.5401e-01,  4.7318e+00],\n          [-2.8110e-01,  8.3333e-01,  6.9945e-01,  ...,  6.2300e-01,\n           -7.4783e-01,  4.7152e+00],\n          ...,\n          [-1.7569e+00,  1.7663e+00,  7.8337e-01,  ...,  1.4593e+00,\n           -8.6737e-01,  5.2077e+00],\n          [-1.5103e+00,  1.7682e+00,  7.2720e-01,  ...,  1.2932e+00,\n           -1.1088e+00,  4.8809e+00],\n          [-1.7204e+00,  2.1179e+00,  2.9491e-01,  ...,  1.2608e+00,\n           -9.6118e-01,  5.0618e+00]],\n\n         [[ 2.3241e+00,  1.4360e+00, -4.8444e-01,  ..., -2.8207e-02,\n           -1.0632e+00,  1.6406e+00],\n          [ 1.3738e+00,  7.9716e-01,  1.2675e-01,  ..., -2.2737e-01,\n           -2.6188e+00,  9.2048e-01],\n          [ 5.5303e-02,  1.0231e+00,  2.8160e-01,  ...,  3.1326e-01,\n           -3.3604e+00,  1.0935e+00],\n          ...,\n          [ 1.0874e+00,  8.5863e-01,  7.3902e-02,  ..., -1.1458e-01,\n           -3.3277e+00,  1.1278e+00],\n          [ 1.2250e+00,  1.1146e+00, -6.1842e-01,  ..., -4.5252e-01,\n           -2.8440e+00,  1.4783e+00],\n          [ 1.3634e+00,  1.1964e+00, -3.5096e-01,  ...,  1.5294e-02,\n           -2.5162e+00,  1.4685e+00]],\n\n         [[ 7.3695e-01,  7.7871e-02, -2.5656e-01,  ..., -1.4986e+00,\n           -2.1487e+00, -5.0746e-01],\n          [ 2.8273e-01, -2.9227e-01, -4.5856e-01,  ..., -4.6133e-01,\n           -2.3950e+00, -1.8433e+00],\n          [-8.1244e-02, -3.8062e-02, -1.0360e+00,  ...,  2.7311e-01,\n           -2.4293e+00, -1.8681e+00],\n          ...,\n          [ 1.7663e-01, -2.1782e-01, -9.2080e-01,  ..., -1.0775e-01,\n           -2.3617e+00, -2.1770e+00],\n          [ 4.2421e-01, -6.1136e-01, -5.7851e-01,  ..., -4.1621e-01,\n           -2.3845e+00, -1.7592e+00],\n          [ 3.7541e-01, -1.6197e-01, -6.4778e-01,  ..., -7.6231e-01,\n           -2.3227e+00, -1.4532e+00]]]], grad_fn=<CloneBackward0>)), (tensor([[[[-2.7032,  1.0111, -3.1079,  ...,  0.9316, -0.8561, -4.5406],\n          [-2.6804,  1.0035, -3.0563,  ...,  0.9303, -0.8436, -4.5076],\n          [-2.8597,  0.9884, -3.1785,  ...,  0.9353, -0.8141, -4.5429],\n          ...,\n          [-3.0192,  0.8954, -3.4015,  ...,  0.8001, -0.4016, -4.4372],\n          [-3.0288,  1.0223, -3.5522,  ...,  0.8739, -0.5728, -4.3955],\n          [-3.2458,  0.7056, -3.3954,  ...,  1.0242, -0.4409, -3.7792]],\n\n         [[ 1.1661, -1.0586,  0.7980,  ...,  1.4187,  4.2586,  5.0685],\n          [ 1.1804, -1.0759,  0.8027,  ...,  1.4274,  4.2211,  5.0416],\n          [ 1.0693, -1.1467,  0.8702,  ...,  1.5118,  4.3546,  5.1039],\n          ...,\n          [ 0.9935, -1.3768,  0.7696,  ...,  1.4825,  4.5597,  5.1113],\n          [ 0.8580, -1.3146,  0.8106,  ...,  1.5155,  4.3696,  5.2118],\n          [ 0.6132, -1.6158,  0.7140,  ...,  1.4187,  4.2980,  5.0722]],\n\n         [[-2.2248,  2.1928, -0.1725,  ...,  2.9734, -4.3782, -2.9467],\n          [-2.2134,  2.1962, -0.1814,  ...,  2.9867, -4.4137, -2.9183],\n          [-2.3148,  2.1568, -0.2840,  ...,  2.9911, -4.2633, -3.0626],\n          ...,\n          [-2.0308,  1.8182, -0.3801,  ...,  2.6711, -3.6529, -3.1535],\n          [-2.1244,  2.1220, -0.1696,  ...,  2.6542, -4.0658, -3.1563],\n          [-1.7348,  1.7257, -0.2473,  ...,  2.7372, -3.1272, -3.0252]],\n\n         ...,\n\n         [[-1.2508, -0.4859,  0.2860,  ..., -0.4562, -0.0159,  1.1329],\n          [-1.2231, -0.5045,  0.2852,  ..., -0.4649, -0.0149,  1.1158],\n          [-1.3847, -0.4584,  0.3514,  ..., -0.4562, -0.0718,  1.1211],\n          ...,\n          [-1.6695, -0.4655,  0.2943,  ..., -0.4264, -0.6765,  0.7610],\n          [-1.4228, -0.5270,  0.3140,  ..., -0.6507, -0.3756,  0.9479],\n          [-1.6381, -0.2160,  0.3855,  ..., -0.4288, -0.9504,  0.4321]],\n\n         [[ 2.3903,  2.9549,  1.1458,  ..., -1.8374,  3.2143,  2.5108],\n          [ 2.3847,  2.9424,  1.1700,  ..., -1.8315,  3.2281,  2.4905],\n          [ 2.4818,  2.9421,  1.1223,  ..., -1.8418,  3.2942,  2.5227],\n          ...,\n          [ 2.4513,  3.1126,  1.4032,  ..., -1.9088,  3.3025,  2.5221],\n          [ 2.3118,  2.8792,  1.2126,  ..., -1.9201,  3.2977,  2.4697],\n          [ 2.6480,  2.9401,  1.4310,  ..., -2.0605,  3.2067,  2.4627]],\n\n         [[ 4.5410,  0.1061, -0.5708,  ...,  2.8045,  5.2089, -1.7759],\n          [ 4.5139,  0.1013, -0.5529,  ...,  2.8075,  5.1950, -1.7812],\n          [ 4.4731,  0.0363, -0.6644,  ...,  2.7593,  5.4468, -1.7792],\n          ...,\n          [ 4.2924, -0.3254, -0.7017,  ...,  2.8321,  5.8073, -1.8778],\n          [ 4.2564, -0.3335, -0.7271,  ...,  2.9976,  5.5785, -1.8435],\n          [ 4.2657, -0.2647, -0.4604,  ...,  2.7918,  6.0018, -1.7367]]]],\n       grad_fn=<CloneBackward0>), tensor([[[[-0.0530,  0.2452,  0.0773,  ...,  0.0646, -0.2646, -0.1518],\n          [-0.0538,  0.2379,  0.0735,  ...,  0.0651, -0.2768, -0.1424],\n          [-0.0022,  0.2237,  0.0373,  ...,  0.0656, -0.2747, -0.1385],\n          ...,\n          [ 0.1078,  0.1955, -0.0399,  ..., -0.0895, -0.3172, -0.1937],\n          [ 0.0247,  0.2325,  0.1040,  ..., -0.0439, -0.2958, -0.1721],\n          [ 0.0998,  0.1154, -0.0272,  ..., -0.0519, -0.4452, -0.1778]],\n\n         [[ 0.0742,  0.2991, -0.2101,  ..., -0.1152, -0.1547,  0.1393],\n          [ 0.0616,  0.2995, -0.2154,  ..., -0.1159, -0.1392,  0.1431],\n          [ 0.1237,  0.1865, -0.2106,  ..., -0.0590, -0.1598,  0.1337],\n          ...,\n          [ 0.4299, -0.0077, -0.2010,  ...,  0.0274, -0.1844,  0.2755],\n          [ 0.2320, -0.0669, -0.3046,  ..., -0.0767, -0.2467,  0.2126],\n          [ 0.4461, -0.0275, -0.3400,  ..., -0.0579, -0.1963,  0.2818]],\n\n         [[-0.2658, -0.3896,  0.0692,  ...,  0.5168,  0.5067,  0.3044],\n          [-0.2658, -0.3848,  0.0724,  ...,  0.5100,  0.5129,  0.3126],\n          [-0.2591, -0.3828,  0.0756,  ...,  0.5200,  0.5060,  0.2554],\n          ...,\n          [-0.3411, -0.3931,  0.0704,  ...,  0.5558,  0.4721,  0.1240],\n          [-0.3145, -0.3628,  0.0938,  ...,  0.4900,  0.4730,  0.1828],\n          [-0.3814, -0.5567,  0.0405,  ...,  0.5714,  0.5381, -0.0274]],\n\n         ...,\n\n         [[-0.7209, -0.4425,  0.3137,  ..., -0.1766,  0.1562, -0.4321],\n          [-0.7072, -0.4418,  0.3079,  ..., -0.1776,  0.1681, -0.4531],\n          [-0.8569, -0.3648,  0.2492,  ..., -0.1637,  0.1936, -0.4198],\n          ...,\n          [-1.1612, -0.1912,  0.2036,  ..., -0.2244,  0.1319, -0.3751],\n          [-1.0314, -0.1859,  0.1792,  ..., -0.1870,  0.1733, -0.3941],\n          [-1.2463, -0.2220,  0.3046,  ..., -0.1219,  0.2805, -0.2974]],\n\n         [[-0.0125, -0.5016, -0.0676,  ...,  0.4388,  0.1731, -0.2341],\n          [-0.0197, -0.5045, -0.0601,  ...,  0.4523,  0.1575, -0.2312],\n          [-0.0403, -0.5393, -0.0540,  ...,  0.3822,  0.2129, -0.3144],\n          ...,\n          [-0.1321, -0.5032,  0.0775,  ...,  0.1745,  0.3232, -0.3845],\n          [-0.0132, -0.4631,  0.0797,  ...,  0.2991,  0.2644, -0.3317],\n          [-0.2009, -0.5006,  0.1084,  ...,  0.1145,  0.3034, -0.5341]],\n\n         [[-0.1116, -0.1277, -0.0507,  ...,  0.5838, -0.0796,  0.1095],\n          [-0.1102, -0.1324, -0.0592,  ...,  0.5839, -0.0742,  0.1010],\n          [-0.1444, -0.1239, -0.0383,  ...,  0.5695, -0.1015,  0.1142],\n          ...,\n          [-0.2508, -0.1030, -0.0915,  ...,  0.6928, -0.0290,  0.1876],\n          [-0.1343, -0.1986, -0.0888,  ...,  0.5686, -0.1400,  0.1804],\n          [-0.3289, -0.0523, -0.1665,  ...,  0.7186, -0.1515,  0.2016]]]],\n       grad_fn=<CloneBackward0>), tensor([[[[ 4.8949e+00, -1.4908e+00, -1.0161e+00,  ...,  4.5870e+00,\n           -5.3322e+00,  8.7466e-01],\n          [ 4.8977e+00, -9.6745e-01,  6.5293e-03,  ...,  4.3100e+00,\n           -4.9473e+00,  1.7613e-01],\n          [ 3.3001e+00, -2.1125e+00,  5.9057e-01,  ...,  2.7144e+00,\n           -4.5478e+00,  1.4279e+00],\n          ...,\n          [ 4.1546e+00, -1.2331e+00,  3.0377e-02,  ...,  3.7727e+00,\n           -4.6078e+00,  4.9166e-01],\n          [ 4.2648e+00, -1.5412e+00,  1.0387e+00,  ...,  3.4300e+00,\n           -4.7405e+00, -2.6842e-02],\n          [ 4.8713e+00, -1.1636e+00, -4.2630e-01,  ...,  4.1344e+00,\n           -4.8804e+00,  4.7558e-01]],\n\n         [[-6.4594e-01, -2.0893e+00, -4.8883e-01,  ...,  5.3769e+00,\n           -1.1868e+00, -1.0779e+00],\n          [ 3.5968e-01, -2.5960e+00, -1.1654e+00,  ...,  5.4436e+00,\n           -2.6835e+00, -1.8117e+00],\n          [ 1.2519e+00, -2.7938e+00, -1.8558e+00,  ...,  5.2224e+00,\n           -2.4726e+00, -1.6074e+00],\n          ...,\n          [ 8.6420e-01, -2.3464e+00, -1.3352e+00,  ...,  5.3333e+00,\n           -2.6566e+00, -1.9981e+00],\n          [ 1.1268e+00, -2.7235e+00, -1.3650e+00,  ...,  6.0522e+00,\n           -2.7391e+00, -1.5625e+00],\n          [ 8.7566e-01, -2.6766e+00, -9.0521e-01,  ...,  5.4824e+00,\n           -2.3000e+00, -1.6359e+00]],\n\n         [[ 1.2639e+00, -7.3847e+00,  9.2321e-01,  ..., -2.0954e+00,\n           -6.6964e-01, -2.2558e+00],\n          [ 1.0417e+00, -6.8918e+00,  3.3940e-01,  ..., -1.8982e+00,\n           -8.3557e-01, -1.1730e+00],\n          [-1.9507e-01, -4.9590e+00, -2.8207e-01,  ..., -1.7318e+00,\n           -2.0116e+00, -5.5078e-01],\n          ...,\n          [ 6.1214e-01, -6.5399e+00,  1.7450e-01,  ..., -1.7800e+00,\n           -1.4142e+00, -6.9837e-01],\n          [ 5.3693e-01, -7.3203e+00,  4.9271e-01,  ..., -2.1570e+00,\n           -1.5540e+00, -5.1975e-01],\n          [ 1.3147e+00, -7.5839e+00,  4.8483e-01,  ..., -1.9010e+00,\n           -6.3504e-01, -1.0956e+00]],\n\n         ...,\n\n         [[-1.3495e+00, -5.8580e+00, -3.2724e+00,  ...,  3.7949e-01,\n           -1.2756e+00, -4.1862e+00],\n          [ 1.7376e-01, -5.0377e+00, -3.3122e+00,  ...,  2.4712e+00,\n           -2.1851e+00, -4.4397e+00],\n          [ 3.7281e-01, -3.2597e+00, -2.8434e+00,  ...,  3.7341e+00,\n           -1.8235e+00, -4.8817e+00],\n          ...,\n          [-9.2656e-02, -3.7727e+00, -3.4178e+00,  ...,  2.9578e+00,\n           -2.4073e+00, -4.1087e+00],\n          [ 5.3453e-01, -4.9751e+00, -3.2910e+00,  ...,  2.7982e+00,\n           -2.2666e+00, -4.4613e+00],\n          [ 9.9509e-02, -4.7026e+00, -3.6213e+00,  ...,  2.3876e+00,\n           -2.1275e+00, -4.1170e+00]],\n\n         [[ 1.9521e+00,  3.5438e+00,  3.1050e-01,  ...,  1.9795e+00,\n            3.2302e+00,  1.1188e+00],\n          [ 2.2753e+00,  3.7220e+00, -1.2155e+00,  ...,  1.4824e+00,\n            3.2789e+00,  1.1821e-01],\n          [ 2.5749e+00,  3.5964e+00, -2.9256e+00,  ...,  2.2497e+00,\n            3.2074e+00, -1.1741e+00],\n          ...,\n          [ 2.7082e+00,  3.9445e+00, -2.5330e+00,  ...,  1.9468e+00,\n            2.9576e+00, -3.3675e-02],\n          [ 2.5184e+00,  3.9188e+00, -1.8124e+00,  ...,  1.2014e+00,\n            3.1839e+00, -2.1332e-01],\n          [ 2.3081e+00,  3.2805e+00, -1.3175e+00,  ...,  1.9942e+00,\n            2.7102e+00, -1.3096e-01]],\n\n         [[ 2.0931e-01, -1.6496e-01,  5.6332e+00,  ...,  3.0390e+00,\n            1.8525e+00, -2.6079e+00],\n          [-1.3159e+00, -2.8803e-01,  4.5299e+00,  ...,  3.5805e+00,\n            3.7697e-01, -2.7133e+00],\n          [-1.6180e+00,  2.1558e-01,  2.6546e+00,  ...,  3.2029e+00,\n           -1.5721e+00, -3.4952e+00],\n          ...,\n          [-1.3032e+00,  2.0132e-01,  4.5018e+00,  ...,  3.8074e+00,\n           -6.3835e-01, -2.7529e+00],\n          [-2.0965e+00, -1.0598e+00,  4.1645e+00,  ...,  4.3324e+00,\n           -3.6564e-01, -2.9953e+00],\n          [-1.1054e+00, -2.6828e-01,  5.0874e+00,  ...,  3.9010e+00,\n            5.4112e-01, -2.9396e+00]]]], grad_fn=<CloneBackward0>), tensor([[[[-2.1621, -2.2894, -1.1227,  ..., -4.7707, -3.6905, -1.6106],\n          [-2.7334, -2.0638, -1.4761,  ..., -4.9135, -2.6221, -0.7167],\n          [-2.4579, -1.1426, -2.9045,  ..., -4.5534, -2.7824, -0.5983],\n          ...,\n          [-2.9372, -1.4694, -2.1089,  ..., -4.3644, -2.9212, -0.4735],\n          [-3.1354, -1.4676, -1.2863,  ..., -4.3945, -3.1856, -0.6872],\n          [-2.5800, -1.7759, -1.4777,  ..., -4.5198, -3.0330, -1.2009]],\n\n         [[-0.0619,  0.1355,  2.6481,  ...,  0.0144, -0.5125, -0.3258],\n          [ 0.0413,  0.7382,  2.4185,  ...,  0.0757, -0.0336, -0.5923],\n          [ 0.1672,  1.5135,  2.0025,  ..., -0.1285, -0.1259, -0.2103],\n          ...,\n          [-0.4396,  1.0228,  1.8838,  ..., -0.0625, -0.3565, -0.4525],\n          [-0.0267,  0.9608,  2.0674,  ..., -0.1660, -0.1113, -0.8206],\n          [-0.1610,  0.9353,  2.1688,  ..., -0.3959, -0.4595, -0.5605]],\n\n         [[-2.3210, -2.2080,  2.3192,  ...,  1.9063, -0.7569, -3.6832],\n          [-1.0535, -1.8995,  1.5377,  ...,  1.6958,  0.0127, -2.6454],\n          [-0.3951, -2.2274,  0.9436,  ...,  3.0013, -0.1480, -1.6743],\n          ...,\n          [-0.7810, -1.8938,  1.6412,  ...,  1.6805,  0.3585, -2.4069],\n          [-0.9120, -2.3237,  2.0290,  ...,  1.8983, -0.1305, -3.0701],\n          [-0.7188, -1.8091,  1.5295,  ...,  1.9654,  0.0205, -2.8510]],\n\n         ...,\n\n         [[-4.0691,  0.1691,  0.6984,  ...,  0.1199, -2.0242, -1.1165],\n          [-4.0925, -0.0514,  1.5920,  ...,  0.4739, -1.4728, -1.1770],\n          [-4.7514, -0.4873,  1.5734,  ...,  1.1982, -1.0938, -1.9643],\n          ...,\n          [-4.4814,  0.0730,  1.6081,  ...,  0.7146, -1.0734, -1.1271],\n          [-3.8988,  0.1843,  1.7467,  ...,  0.0176, -1.3596, -0.9922],\n          [-4.1295,  0.1144,  1.1046,  ...,  0.4008, -1.2343, -1.3221]],\n\n         [[ 0.8078,  1.0325,  2.4736,  ..., -0.6789, -0.6341, -1.9068],\n          [ 1.6883,  1.9608,  2.2267,  ..., -1.0277, -1.4152, -2.0600],\n          [ 2.4399,  1.9075,  1.7741,  ..., -0.4974, -2.3872, -1.0728],\n          ...,\n          [ 1.2770,  1.7339,  1.5418,  ..., -0.9562, -1.7401, -2.0601],\n          [ 1.7665,  1.7329,  2.0513,  ..., -0.8634, -1.6954, -1.5770],\n          [ 1.7280,  1.6569,  1.7003,  ..., -1.1770, -1.3611, -2.3549]],\n\n         [[ 0.0092, -2.8303, -4.4067,  ..., -3.8786, -1.4433, -1.4109],\n          [-0.3159, -2.4125, -4.2469,  ..., -4.3862, -1.4144, -0.6463],\n          [-0.8768, -2.8223, -3.8761,  ..., -4.2076, -0.9842,  0.3969],\n          ...,\n          [-0.5284, -2.7027, -4.0488,  ..., -4.4201, -1.5905, -0.0124],\n          [ 0.0064, -2.3475, -3.9320,  ..., -4.8853, -1.4189, -0.1971],\n          [-0.1585, -2.8324, -4.2166,  ..., -4.1361, -1.7371, -0.1814]]]],\n       grad_fn=<CloneBackward0>)), (tensor([[[[ 1.6262, -0.7795, -0.1673,  ..., -2.1025,  0.8496, -0.4692],\n          [ 1.6267, -0.8000, -0.1605,  ..., -2.1116,  0.8561, -0.4564],\n          [ 1.7202, -0.8645, -0.2906,  ..., -2.1633,  0.9317, -0.3889],\n          ...,\n          [ 2.0531, -1.0040, -0.5924,  ..., -1.9734,  0.7947, -0.3877],\n          [ 1.7953, -0.9029, -0.5549,  ..., -2.0090,  0.7494, -0.6249],\n          [ 2.0393, -1.1552, -0.8217,  ..., -1.8759,  0.7125, -0.0195]],\n\n         [[-0.5007, -0.2873,  3.0531,  ..., -0.1275, -3.3048,  1.0494],\n          [-0.5219, -0.2727,  3.0685,  ..., -0.1042, -3.3019,  1.0587],\n          [-0.5319, -0.4074,  2.9147,  ..., -0.2202, -3.4370,  1.0060],\n          ...,\n          [-0.5910, -0.7762,  2.5702,  ..., -0.3224, -3.6766,  0.9482],\n          [-0.5118, -0.5420,  2.7710,  ..., -0.3226, -3.4447,  1.1067],\n          [-0.3885, -0.8494,  2.5231,  ..., -0.3372, -3.6250,  0.9189]],\n\n         [[ 1.3647, -2.9321, -1.1080,  ...,  0.9310, -0.8247,  4.4790],\n          [ 1.3735, -2.9244, -1.1142,  ...,  0.9410, -0.8151,  4.4648],\n          [ 1.2740, -3.0029, -1.2292,  ...,  0.8646, -0.7924,  4.4314],\n          ...,\n          [ 1.0835, -3.2531, -1.3984,  ...,  0.4846, -0.6942,  4.3633],\n          [ 1.0418, -2.9842, -1.3572,  ...,  0.6931, -0.8977,  4.4103],\n          [ 1.0612, -3.5382, -1.4136,  ...,  0.2379, -0.6756,  4.3851]],\n\n         ...,\n\n         [[ 0.9066, -3.3652,  0.7768,  ...,  0.6106,  2.7747, -3.4052],\n          [ 0.8682, -3.3386,  0.7647,  ...,  0.6035,  2.7688, -3.3922],\n          [ 0.8372, -3.4981,  0.6440,  ...,  0.7653,  2.6992, -3.4645],\n          ...,\n          [ 0.9700, -3.9239,  0.5666,  ...,  0.6458,  2.8464, -3.5626],\n          [ 0.9610, -3.6541,  0.4359,  ...,  0.5995,  2.7776, -3.5467],\n          [ 1.2603, -3.9608,  0.5216,  ...,  0.4156,  3.0482, -3.3752]],\n\n         [[ 1.7817, 10.8342, -5.1456,  ..., -0.6309, -0.8555,  0.5440],\n          [ 1.7788, 10.7844, -5.1309,  ..., -0.6348, -0.8873,  0.5155],\n          [ 1.7595, 11.0241, -5.0831,  ..., -0.5653, -0.6672,  0.5551],\n          ...,\n          [ 1.7827, 11.2793, -5.1336,  ..., -0.2962,  0.1940,  0.4114],\n          [ 1.7237, 11.0380, -5.2023,  ..., -0.2839, -0.2243,  0.4199],\n          [ 1.5441, 11.4506, -5.1746,  ..., -0.1276,  0.5342,  0.5923]],\n\n         [[ 2.7480, -0.2031,  1.7696,  ..., -0.5498,  1.1980,  3.1821],\n          [ 2.7279, -0.1868,  1.7721,  ..., -0.5652,  1.1725,  3.1951],\n          [ 2.9043, -0.2753,  1.7684,  ..., -0.4831,  1.2206,  3.1868],\n          ...,\n          [ 3.0308, -0.3546,  1.6591,  ..., -0.4929,  1.4181,  2.9104],\n          [ 2.9791, -0.2728,  1.5600,  ..., -0.4334,  1.3123,  3.0645],\n          [ 3.1230, -0.1562,  1.7054,  ..., -0.4685,  1.3841,  2.9493]]]],\n       grad_fn=<CloneBackward0>), tensor([[[[-0.6094,  1.0389, -0.3203,  ...,  0.5570,  0.6763,  0.1313],\n          [-0.6167,  1.0337, -0.3169,  ...,  0.5702,  0.6691,  0.1323],\n          [-0.6188,  1.0393, -0.2599,  ...,  0.5358,  0.6620,  0.1333],\n          ...,\n          [-0.5906,  1.1041, -0.2656,  ...,  0.3917,  0.6413,  0.2142],\n          [-0.5623,  1.1006, -0.3341,  ...,  0.4863,  0.6642,  0.2081],\n          [-0.5746,  1.1440, -0.4881,  ...,  0.2880,  0.5092,  0.1839]],\n\n         [[-0.9384, -0.1230,  0.6204,  ..., -0.1905,  0.9670, -0.9831],\n          [-0.9449, -0.1106,  0.6306,  ..., -0.1939,  0.9658, -0.9905],\n          [-0.9670, -0.0871,  0.5936,  ..., -0.1424,  0.9346, -0.9215],\n          ...,\n          [-0.9417,  0.0190,  0.5179,  ..., -0.1155,  0.7948, -0.7332],\n          [-0.9957, -0.0588,  0.5069,  ..., -0.2087,  0.7765, -0.8911],\n          [-0.8960, -0.0191,  0.5512,  ..., -0.1253,  0.7544, -0.5477]],\n\n         [[-0.2035,  0.3690,  0.6224,  ...,  1.3871,  0.0662, -0.8743],\n          [-0.1994,  0.3813,  0.6245,  ...,  1.3828,  0.0553, -0.8767],\n          [-0.1864,  0.3779,  0.5219,  ...,  1.3592,  0.0798, -0.9010],\n          ...,\n          [-0.2932,  0.3688,  0.2912,  ...,  1.1852,  0.0818, -0.9418],\n          [-0.1838,  0.4530,  0.3806,  ...,  1.2338,  0.1197, -0.9381],\n          [-0.3516,  0.3202,  0.2828,  ...,  0.9071,  0.1354, -0.9341]],\n\n         ...,\n\n         [[-0.4606, -0.3632, -0.9947,  ...,  1.1363, -0.1691,  0.3728],\n          [-0.4694, -0.3580, -1.0072,  ...,  1.1433, -0.1681,  0.3917],\n          [-0.4077, -0.3924, -0.9078,  ...,  1.0858, -0.1988,  0.3045],\n          ...,\n          [-0.2808, -0.4561, -0.7809,  ...,  1.0151, -0.1860,  0.0584],\n          [-0.3171, -0.3846, -0.8721,  ...,  1.0923, -0.1577,  0.1968],\n          [-0.2175, -0.5629, -0.6467,  ...,  0.8638, -0.2003, -0.0318]],\n\n         [[-0.8502, -0.4074, -0.9350,  ...,  0.4802, -0.7373,  1.3094],\n          [-0.8594, -0.4304, -0.9361,  ...,  0.5025, -0.7308,  1.3107],\n          [-0.8390, -0.3084, -0.8371,  ...,  0.3665, -0.7329,  1.2317],\n          ...,\n          [-0.7810, -0.0197, -0.7301,  ...,  0.1412, -0.7049,  1.1086],\n          [-0.8478, -0.2443, -0.8135,  ...,  0.2844, -0.6744,  1.1064],\n          [-0.6618,  0.0832, -0.5334,  ...,  0.0421, -0.7771,  1.0834]],\n\n         [[-0.0425,  0.3632, -0.2799,  ..., -0.4477,  0.2205,  1.4763],\n          [-0.0395,  0.3778, -0.2648,  ..., -0.4652,  0.2195,  1.4647],\n          [-0.0956,  0.3647, -0.3800,  ..., -0.3102,  0.2174,  1.5220],\n          ...,\n          [-0.2623,  0.3095, -0.5486,  ..., -0.0265,  0.3391,  1.6423],\n          [-0.2562,  0.3457, -0.4198,  ..., -0.2189,  0.2078,  1.5361],\n          [-0.2563,  0.2828, -0.7482,  ...,  0.2347,  0.2271,  1.4976]]]],\n       grad_fn=<CloneBackward0>), tensor([[[[ 2.2568,  1.7649, -0.1700,  ...,  5.6287, -3.2022, -2.6283],\n          [ 2.2836,  0.2431,  0.7412,  ...,  4.3082, -0.5890, -2.5564],\n          [ 3.3121, -0.9568,  1.6770,  ...,  3.1863,  0.6193, -1.5993],\n          ...,\n          [ 2.7422,  0.4849,  1.2704,  ...,  3.8849,  0.0229, -3.0016],\n          [ 2.3862,  0.1778,  0.5672,  ...,  4.2359,  0.0149, -2.3844],\n          [ 2.8177,  0.9711,  0.5459,  ...,  4.5636, -0.3091, -2.5485]],\n\n         [[ 0.1211,  3.9922,  1.8494,  ..., -3.2790,  0.1447, -0.3224],\n          [ 1.3332,  3.8920,  0.4801,  ..., -1.4868, -0.6804,  1.1989],\n          [ 2.6468,  5.1758,  1.0936,  ..., -1.4692, -0.0427,  0.3284],\n          ...,\n          [ 1.4423,  4.6558,  0.1990,  ..., -1.4836, -1.4750,  1.2186],\n          [ 1.7478,  3.7087,  0.0164,  ..., -1.2775, -0.7030,  1.3834],\n          [ 1.6067,  4.2819,  0.6008,  ..., -1.9284, -0.8601,  0.8468]],\n\n         [[-6.2989,  2.1830,  1.4934,  ..., -3.2060,  2.3944, -2.1215],\n          [-7.3997,  3.4679,  1.6498,  ..., -3.5845,  3.5567, -0.7209],\n          [-8.5000,  3.8107,  1.2679,  ..., -4.8451,  4.2374, -0.9201],\n          ...,\n          [-7.7640,  3.8672,  1.3582,  ..., -4.2342,  3.5070, -1.0068],\n          [-7.9258,  3.2289,  1.6632,  ..., -3.2754,  3.9777, -0.3734],\n          [-7.5649,  3.1563,  1.8464,  ..., -3.4867,  2.7258, -0.7532]],\n\n         ...,\n\n         [[-1.3427, -1.9979,  6.0783,  ...,  2.6361,  0.8453,  1.2846],\n          [-3.0809, -4.8706,  4.4874,  ...,  1.5455,  2.0322,  0.3518],\n          [-4.5224, -5.1393,  3.8243,  ...,  0.6157,  2.2997,  0.7487],\n          ...,\n          [-3.2533, -4.7503,  3.7297,  ...,  1.2432,  1.5321, -0.0444],\n          [-3.2607, -4.9843,  3.6462,  ...,  1.1271,  1.9448,  0.5218],\n          [-2.6708, -3.9778,  4.6460,  ...,  1.3821,  1.7752,  0.6625]],\n\n         [[ 4.2793, -2.7176,  0.7326,  ...,  2.0562,  4.2675, -1.1528],\n          [ 3.6078, -1.5980,  0.6221,  ...,  2.1221,  3.6205, -2.3564],\n          [ 3.3014, -0.9890,  2.4925,  ...,  2.6460,  1.7827, -2.0814],\n          ...,\n          [ 3.6294, -1.6264,  1.2870,  ...,  1.9544,  3.0894, -1.9401],\n          [ 3.6991, -1.1541,  0.4417,  ...,  2.3342,  3.0324, -2.5574],\n          [ 4.1419, -2.0045,  0.6637,  ...,  1.7658,  3.7422, -2.3169]],\n\n         [[ 3.1268,  4.3902, -0.0611,  ..., -2.5275,  1.7745, -1.9005],\n          [ 3.5223,  5.1006, -0.1872,  ..., -3.3400, -0.2603, -2.3631],\n          [ 1.6872,  3.0961, -1.5286,  ..., -2.3985,  1.2431, -3.5008],\n          ...,\n          [ 2.2353,  4.3371, -1.1535,  ..., -3.3026,  0.4978, -2.6989],\n          [ 4.2204,  5.1762, -0.7583,  ..., -3.7378, -0.6560, -2.2170],\n          [ 3.2264,  4.8076, -0.5655,  ..., -3.6529,  0.6546, -1.8891]]]],\n       grad_fn=<CloneBackward0>), tensor([[[[-1.9937,  2.7163,  0.1229,  ..., -1.1874,  0.6870,  1.4451],\n          [-1.0601,  2.4997, -0.2856,  ..., -0.2515,  0.2971,  0.3254],\n          [-0.8542,  3.7021, -0.1949,  ..., -0.7575,  0.0155, -0.4638],\n          ...,\n          [-0.8510,  2.5671, -0.5601,  ..., -0.0247,  0.2672, -0.3340],\n          [-0.7953,  2.5222, -0.7074,  ..., -0.2567,  0.1434,  0.0249],\n          [-1.4507,  2.4338, -0.6438,  ..., -0.3024,  0.2977,  0.1950]],\n\n         [[ 0.0716, -0.0329, -0.7206,  ..., -0.5177,  1.9338,  2.3539],\n          [ 0.4155,  0.7984, -0.9954,  ..., -1.5945,  1.2267,  2.4954],\n          [ 0.5842,  2.1652, -1.3069,  ..., -2.1322,  2.0163,  2.7912],\n          ...,\n          [ 0.3412,  1.3457, -1.2431,  ..., -1.5069,  1.3881,  2.9402],\n          [ 0.1351,  0.9255, -0.9814,  ..., -1.4410,  0.9174,  2.4843],\n          [ 0.3145,  0.8524, -0.8886,  ..., -1.4075,  1.3549,  2.8526]],\n\n         [[-1.2927, -2.3554, -1.0950,  ...,  4.8326, -0.4992,  2.9510],\n          [-1.6444, -3.2877, -1.0892,  ...,  4.1441, -0.3207,  2.7256],\n          [-1.2646, -3.4888, -0.6629,  ...,  3.6380, -0.8962,  3.0903],\n          ...,\n          [-1.5189, -3.8747, -0.9119,  ...,  4.0828, -0.5473,  1.7445],\n          [-1.4676, -3.3019, -1.5134,  ...,  4.3959, -0.5149,  2.2523],\n          [-1.6863, -3.5053, -1.2299,  ...,  4.7428, -0.5411,  2.6500]],\n\n         ...,\n\n         [[-3.7894,  1.7765,  0.1721,  ...,  0.0243, -1.8718,  1.1383],\n          [-3.4143,  2.4168, -0.2216,  ..., -0.5892, -1.0664,  1.1349],\n          [-4.1369,  1.6644,  0.1129,  ..., -1.4905, -0.7090,  1.4165],\n          ...,\n          [-3.3394,  2.6481, -0.2132,  ..., -0.6385, -0.9022,  1.3406],\n          [-3.9323,  2.7748,  0.1088,  ..., -0.7622, -1.1117,  1.1846],\n          [-3.6402,  2.6963, -0.0844,  ..., -0.6470, -1.3512,  0.9074]],\n\n         [[ 0.3461, -3.2668, -1.0108,  ...,  2.7427, -0.3222, -4.2846],\n          [ 0.2074, -3.6304,  1.2063,  ...,  2.2765, -0.2017, -4.8657],\n          [-0.7086, -4.1444,  2.7841,  ...,  1.8989,  0.3305, -5.7282],\n          ...,\n          [ 0.3060, -3.3173,  1.6702,  ...,  2.5643, -0.4158, -5.2118],\n          [ 0.2514, -3.6316,  1.5134,  ...,  2.4066,  0.0218, -5.0720],\n          [ 0.7489, -3.6237,  0.9238,  ...,  2.6950, -0.3469, -5.1528]],\n\n         [[-1.5504,  0.8991,  1.1144,  ..., -2.4865,  0.5617,  0.5276],\n          [-1.9574,  1.5250,  0.3725,  ..., -2.8220,  0.4760,  0.1030],\n          [-1.0775,  1.0614,  0.0828,  ..., -2.6850,  0.9240,  0.0221],\n          ...,\n          [-1.6011,  0.9635, -0.0090,  ..., -3.4963,  0.7746, -0.1111],\n          [-1.9288,  1.4176, -0.0990,  ..., -3.1992,  1.1354, -0.1589],\n          [-2.0005,  1.2899,  0.5897,  ..., -3.3033,  0.7729, -0.0636]]]],\n       grad_fn=<CloneBackward0>)), (tensor([[[[-1.1032e-01, -2.5844e+00,  6.1856e+00,  ...,  1.1040e+00,\n           -7.4614e+00, -2.8983e+00],\n          [-1.3164e-01, -2.5763e+00,  6.1672e+00,  ...,  1.1152e+00,\n           -7.4650e+00, -2.8699e+00],\n          [-3.6818e-02, -2.6165e+00,  6.3950e+00,  ...,  1.0813e+00,\n           -7.6387e+00, -3.1972e+00],\n          ...,\n          [ 2.3823e-01, -2.6320e+00,  6.7348e+00,  ...,  1.0716e+00,\n           -8.1243e+00, -3.7948e+00],\n          [ 7.1217e-02, -2.6881e+00,  6.5742e+00,  ...,  1.0272e+00,\n           -7.7435e+00, -3.5018e+00],\n          [ 4.8475e-01, -2.6999e+00,  6.9117e+00,  ...,  1.1744e+00,\n           -8.1734e+00, -4.3537e+00]],\n\n         [[ 1.3386e-02,  2.4149e+00,  1.7465e+00,  ..., -1.6150e+00,\n           -6.3454e+00,  6.1975e+00],\n          [ 1.5130e-02,  2.3821e+00,  1.7411e+00,  ..., -1.6328e+00,\n           -6.3102e+00,  6.1910e+00],\n          [ 1.4000e-01,  2.4298e+00,  1.7801e+00,  ..., -1.4685e+00,\n           -6.5975e+00,  6.3579e+00],\n          ...,\n          [ 3.2072e-01,  2.4545e+00,  1.9564e+00,  ..., -1.4029e+00,\n           -7.1337e+00,  6.7247e+00],\n          [ 2.1981e-01,  2.4179e+00,  1.8500e+00,  ..., -1.4173e+00,\n           -6.8103e+00,  6.4023e+00],\n          [ 2.9969e-01,  2.5046e+00,  1.9528e+00,  ..., -1.4209e+00,\n           -7.4560e+00,  6.8422e+00]],\n\n         [[ 5.0751e+00, -6.9629e-03, -1.0472e+00,  ..., -5.0030e+00,\n           -1.4016e+00, -1.4880e+00],\n          [ 5.0618e+00,  7.5625e-03, -1.0684e+00,  ..., -4.9717e+00,\n           -1.4007e+00, -1.4878e+00],\n          [ 5.1969e+00, -1.6534e-01, -9.5861e-01,  ..., -5.1340e+00,\n           -1.5289e+00, -1.4941e+00],\n          ...,\n          [ 5.5951e+00, -5.2248e-01, -6.7663e-01,  ..., -5.5721e+00,\n           -1.6586e+00, -1.4378e+00],\n          [ 5.4397e+00, -3.9803e-01, -8.6380e-01,  ..., -5.3416e+00,\n           -1.5243e+00, -1.4040e+00],\n          [ 5.7486e+00, -6.5029e-01, -6.3496e-01,  ..., -5.6089e+00,\n           -1.7869e+00, -1.3500e+00]],\n\n         ...,\n\n         [[ 4.2387e+00, -8.6498e-01,  7.8435e+00,  ...,  8.4225e+00,\n           -1.4356e+00,  3.4220e+00],\n          [ 4.2458e+00, -8.6063e-01,  7.7751e+00,  ...,  8.3961e+00,\n           -1.4464e+00,  3.4322e+00],\n          [ 4.1199e+00, -8.7882e-01,  8.1637e+00,  ...,  8.7649e+00,\n           -1.3820e+00,  3.3176e+00],\n          ...,\n          [ 4.0093e+00, -9.9163e-01,  8.7861e+00,  ...,  9.4457e+00,\n           -1.3078e+00,  3.0629e+00],\n          [ 3.9970e+00, -8.5402e-01,  8.3058e+00,  ...,  9.0543e+00,\n           -1.5219e+00,  3.1607e+00],\n          [ 3.7568e+00, -1.0580e+00,  9.0467e+00,  ...,  9.7459e+00,\n           -1.2240e+00,  3.0218e+00]],\n\n         [[ 4.9268e-03, -2.3949e+00, -3.1111e+00,  ..., -2.3423e+00,\n            1.8123e+00, -4.1966e+00],\n          [-4.8452e-02, -2.4178e+00, -3.1015e+00,  ..., -2.4142e+00,\n            1.7724e+00, -4.1606e+00],\n          [ 2.4278e-01, -2.1899e+00, -3.0553e+00,  ..., -2.1039e+00,\n            2.0502e+00, -4.3597e+00],\n          ...,\n          [ 7.6067e-01, -1.8846e+00, -3.0714e+00,  ..., -1.5195e+00,\n            2.8032e+00, -4.9622e+00],\n          [ 3.6328e-01, -2.1867e+00, -2.9649e+00,  ..., -1.9779e+00,\n            2.1967e+00, -4.6750e+00],\n          [ 1.0052e+00, -1.7252e+00, -3.1170e+00,  ..., -1.1948e+00,\n            3.1419e+00, -5.1842e+00]],\n\n         [[ 1.5770e+00,  4.4222e+00, -8.4176e+00,  ..., -4.2960e+00,\n           -4.6649e+00,  5.6459e+00],\n          [ 1.5579e+00,  4.4006e+00, -8.3629e+00,  ..., -4.2343e+00,\n           -4.6681e+00,  5.6060e+00],\n          [ 1.7755e+00,  4.5631e+00, -8.6994e+00,  ..., -4.5611e+00,\n           -4.6463e+00,  5.8965e+00],\n          ...,\n          [ 2.1948e+00,  5.0512e+00, -9.5793e+00,  ..., -5.4110e+00,\n           -4.4740e+00,  6.4959e+00],\n          [ 1.9928e+00,  4.6934e+00, -8.9944e+00,  ..., -4.8413e+00,\n           -4.5263e+00,  6.1712e+00],\n          [ 2.4727e+00,  5.3000e+00, -1.0154e+01,  ..., -5.7566e+00,\n           -4.2985e+00,  6.5910e+00]]]], grad_fn=<CloneBackward0>), tensor([[[[-0.7748,  0.4773,  0.4465,  ..., -0.0140, -0.3575, -0.7097],\n          [-0.7732,  0.4872,  0.4305,  ..., -0.0213, -0.3563, -0.7141],\n          [-0.6947,  0.4669,  0.4948,  ...,  0.0068, -0.3472, -0.6974],\n          ...,\n          [-0.6139,  0.3287,  0.7533,  ...,  0.0495, -0.3285, -0.6942],\n          [-0.6752,  0.4653,  0.5758,  ..., -0.0106, -0.3663, -0.7138],\n          [-0.5024,  0.2328,  0.8098,  ...,  0.1044, -0.3536, -0.6851]],\n\n         [[ 1.0667,  0.3615,  0.2811,  ...,  0.5845, -0.6406,  1.0694],\n          [ 1.0621,  0.3555,  0.2841,  ...,  0.5827, -0.6388,  1.0797],\n          [ 1.0901,  0.3540,  0.2893,  ...,  0.5640, -0.5922,  0.9889],\n          ...,\n          [ 1.0045,  0.3830,  0.3559,  ...,  0.6301, -0.5335,  0.7772],\n          [ 1.0326,  0.3377,  0.3578,  ...,  0.5688, -0.6174,  0.9217],\n          [ 0.9533,  0.3577,  0.3841,  ...,  0.5649, -0.4537,  0.7615]],\n\n         [[-0.2467, -0.6616,  1.3686,  ..., -0.4997, -0.8401,  0.9890],\n          [-0.2451, -0.6549,  1.3737,  ..., -0.5011, -0.8347,  0.9834],\n          [-0.2672, -0.6589,  1.3311,  ..., -0.5062, -0.8189,  1.0112],\n          ...,\n          [-0.2358, -0.7493,  1.2577,  ..., -0.5775, -0.8282,  1.0846],\n          [-0.2533, -0.6993,  1.3519,  ..., -0.5612, -0.8015,  1.0998],\n          [-0.2577, -0.8423,  1.2009,  ..., -0.5526, -0.7307,  1.1056]],\n\n         ...,\n\n         [[ 2.1615, -0.3653,  0.9508,  ..., -1.4949,  0.4228,  0.3515],\n          [ 2.1771, -0.3726,  0.9561,  ..., -1.5053,  0.4062,  0.3512],\n          [ 2.0659, -0.3132,  0.9212,  ..., -1.4209,  0.4484,  0.3904],\n          ...,\n          [ 1.8600, -0.1976,  0.8317,  ..., -1.3077,  0.5769,  0.4969],\n          [ 2.0757, -0.2868,  0.9261,  ..., -1.3443,  0.4790,  0.4341],\n          [ 1.6452, -0.1145,  0.8526,  ..., -1.2342,  0.6531,  0.5119]],\n\n         [[-0.3431, -0.9068,  1.1338,  ..., -0.0543,  0.7418,  0.7636],\n          [-0.3468, -0.9067,  1.1640,  ..., -0.0413,  0.7416,  0.7581],\n          [-0.3022, -0.9155,  1.0765,  ..., -0.1239,  0.7048,  0.7326],\n          ...,\n          [-0.3919, -0.8830,  1.0111,  ..., -0.2379,  0.6773,  0.7366],\n          [-0.3998, -0.8521,  1.1237,  ..., -0.1835,  0.7111,  0.7711],\n          [-0.3536, -0.8993,  0.9030,  ..., -0.3756,  0.6445,  0.7122]],\n\n         [[-0.0042,  0.5787, -0.1985,  ...,  0.5015, -2.3521, -0.3479],\n          [-0.0048,  0.5803, -0.2011,  ...,  0.5128, -2.3613, -0.3471],\n          [-0.0518,  0.5901, -0.1719,  ...,  0.4730, -2.2833, -0.3693],\n          ...,\n          [-0.0538,  0.5655, -0.1460,  ...,  0.3788, -2.1706, -0.2934],\n          [-0.1100,  0.5899, -0.2313,  ...,  0.4619, -2.2831, -0.3102],\n          [-0.0309,  0.5203, -0.1152,  ...,  0.2812, -2.0700, -0.2494]]]],\n       grad_fn=<CloneBackward0>), tensor([[[[-4.0460, -1.7895,  3.8023,  ...,  0.2729, -3.3642, -1.0469],\n          [-4.0954, -0.8404,  3.9899,  ...,  0.5687, -0.5973, -0.5471],\n          [-2.8680, -0.3986,  3.3820,  ..., -0.0268,  1.7336, -0.4770],\n          ...,\n          [-3.7962, -0.4273,  3.6760,  ..., -0.0194, -0.5205, -0.3988],\n          [-3.3861, -0.2511,  3.6415,  ...,  0.8357, -0.7244, -0.6036],\n          [-4.2954, -0.9536,  3.9030,  ...,  0.6381, -0.9969, -0.4296]],\n\n         [[-1.8723, -0.4210,  3.4585,  ..., -0.3711, -1.2821,  2.6470],\n          [-1.5128, -1.5630,  2.6164,  ..., -0.3355, -0.5357,  1.1246],\n          [-1.3631, -2.7494,  2.5132,  ..., -0.0486, -1.1027,  0.9971],\n          ...,\n          [-1.5257, -1.3152,  2.6399,  ..., -0.8745, -0.9342,  0.9046],\n          [-1.1271, -1.5306,  2.7959,  ..., -0.3764, -0.5719,  0.9647],\n          [-1.4485, -1.1801,  2.4851,  ..., -0.5500, -0.8348,  1.1546]],\n\n         [[ 1.9878, -3.8754,  5.0807,  ..., -4.9453, -0.7050,  1.9795],\n          [ 2.2772, -4.4163,  3.9638,  ..., -5.7629,  1.5942,  0.3098],\n          [ 3.9195, -3.9616,  2.0758,  ..., -5.4888,  0.2443, -0.9682],\n          ...,\n          [ 2.1538, -4.7241,  3.5368,  ..., -5.7741,  1.2150, -0.2518],\n          [ 2.0796, -3.7544,  3.4369,  ..., -5.2870,  1.5206, -0.4956],\n          [ 2.3854, -4.6291,  3.9417,  ..., -5.3948,  1.2675,  0.5356]],\n\n         ...,\n\n         [[-2.0074, -1.2379, -5.1597,  ...,  2.3801, -2.7694, -2.5864],\n          [-2.1110, -3.2432, -3.7344,  ...,  3.5117, -3.9383, -3.7741],\n          [-2.8233, -2.7348, -2.8431,  ...,  2.6057, -4.4404, -4.1314],\n          ...,\n          [-2.4986, -3.4011, -3.5123,  ...,  2.7470, -4.1922, -3.5096],\n          [-2.7271, -3.6636, -3.3811,  ...,  3.6829, -4.1890, -4.6047],\n          [-2.0902, -2.7247, -3.9236,  ...,  2.9054, -3.8911, -3.5387]],\n\n         [[-4.5206,  1.5519,  0.2993,  ...,  0.1476,  3.2701, -3.0417],\n          [-3.8461,  1.2931, -0.7758,  ..., -1.0787,  4.2013, -3.2881],\n          [-2.3669, -0.0702, -3.5551,  ..., -2.7957,  3.4219, -2.6870],\n          ...,\n          [-3.3608,  0.9750, -1.4133,  ..., -1.9914,  4.0337, -3.1146],\n          [-3.4443,  1.6693, -0.7139,  ..., -1.0342,  5.0167, -3.1734],\n          [-3.7760,  1.1787, -1.1527,  ..., -0.8375,  3.8135, -3.0063]],\n\n         [[ 1.9602,  4.1263, -0.9384,  ...,  0.9325, -3.0106, -0.9867],\n          [ 3.2834,  4.4955,  0.6466,  ...,  0.9253, -3.5262, -1.6378],\n          [ 1.4202,  3.9365,  0.4234,  ...,  0.5660, -4.9022, -2.9232],\n          ...,\n          [ 2.6052,  4.1607,  0.3750,  ...,  0.3433, -4.9045, -1.7043],\n          [ 3.2656,  4.6967,  1.0659,  ...,  0.4182, -3.8463, -1.7368],\n          [ 2.6061,  4.7059,  0.2009,  ...,  0.4322, -3.8770, -1.2928]]]],\n       grad_fn=<CloneBackward0>), tensor([[[[ 0.6074, -1.4211, -0.8628,  ...,  0.9255,  3.3875, -0.8830],\n          [ 0.4729, -1.1932, -1.0476,  ...,  1.1685,  2.7282, -0.8038],\n          [ 0.8211, -1.0864, -0.1198,  ...,  1.3650,  2.1044, -0.0475],\n          ...,\n          [ 0.3265, -1.3577, -0.5533,  ...,  1.5695,  2.6730, -0.8007],\n          [ 0.4493, -1.0621, -0.7161,  ...,  0.8554,  2.7657, -0.4496],\n          [ 0.5092, -1.6106, -0.8477,  ...,  1.4280,  3.2280, -0.8787]],\n\n         [[-1.0949,  0.7625, -1.5575,  ...,  1.2401, -1.6637,  1.7092],\n          [ 0.2373,  0.2521, -1.0993,  ...,  1.0189, -0.7087,  1.6228],\n          [ 0.6824,  0.5465, -0.0671,  ...,  1.5526, -0.6640,  2.1845],\n          ...,\n          [ 0.0556,  0.4961, -0.2933,  ...,  0.9308, -0.5140,  1.8954],\n          [ 0.5418,  0.7310, -0.8383,  ...,  1.1255, -0.5718,  2.0820],\n          [-0.2570,  0.7709, -1.1410,  ...,  1.4970, -1.0197,  1.8036]],\n\n         [[ 0.1591, -0.4696, -1.8193,  ..., -1.2982,  0.0288,  0.1334],\n          [ 0.0586,  0.4379, -1.1817,  ..., -1.1080,  0.1461,  0.4755],\n          [-0.3301,  1.2116, -1.3849,  ..., -1.1573,  0.6960,  0.2503],\n          ...,\n          [-0.1688,  0.8713, -1.3085,  ..., -1.1424,  0.3228,  0.4516],\n          [ 0.0866,  0.6903, -1.0935,  ..., -1.0719,  0.0171,  0.4841],\n          [ 0.0196,  0.4947, -1.5179,  ..., -1.1801,  0.3220,  0.3829]],\n\n         ...,\n\n         [[ 0.1252,  1.6283,  1.1416,  ..., -0.1758,  0.1840,  0.7741],\n          [-0.6050,  0.3557,  1.4884,  ..., -0.3040,  0.1425,  1.1284],\n          [-0.5555, -0.3556,  1.6673,  ..., -0.0144,  0.4760,  1.2730],\n          ...,\n          [-0.6188,  0.3184,  1.6948,  ...,  0.0834, -0.1977,  1.1967],\n          [-0.7229,  0.3399,  1.0786,  ..., -0.4490,  0.2889,  1.1590],\n          [-0.5039,  0.6875,  1.4250,  ..., -0.2953,  0.1125,  1.0643]],\n\n         [[ 0.0872,  0.0335,  3.1086,  ..., -1.8062,  0.4519, -3.7117],\n          [ 0.5508, -0.3615,  2.8976,  ..., -2.1708,  0.2000, -4.0039],\n          [-0.3566, -0.4380,  2.4390,  ..., -2.7935, -0.2523, -3.0948],\n          ...,\n          [ 0.6987, -0.5994,  2.6069,  ..., -2.2322,  0.0298, -3.5807],\n          [ 0.7993, -0.6377,  2.5211,  ..., -2.0673, -0.2157, -3.9236],\n          [ 0.5006, -0.4488,  2.8501,  ..., -2.1722,  0.2326, -4.0574]],\n\n         [[ 0.2630, -0.1563, -1.3821,  ...,  1.6476, -0.4719, -0.2689],\n          [ 0.7849, -0.5901, -1.0643,  ...,  0.8469, -0.5162, -0.7377],\n          [ 0.7247, -1.7767, -1.4117,  ...,  0.7229, -0.4881, -0.7470],\n          ...,\n          [ 1.0190, -0.9850, -1.2596,  ...,  1.0084, -0.4581, -0.1761],\n          [ 1.1689, -0.9005, -0.9811,  ...,  0.6066, -0.4853, -0.8470],\n          [ 0.4921, -0.6564, -1.1861,  ...,  1.0476, -0.5846, -0.3379]]]],\n       grad_fn=<CloneBackward0>)), (tensor([[[[-1.9430, -0.2306,  1.5299,  ...,  3.3123, -0.8388,  3.0971],\n          [-1.9529, -0.2101,  1.5239,  ...,  3.3167, -0.8432,  3.1158],\n          [-1.9149, -0.3010,  1.5622,  ...,  3.3656, -0.7387,  3.1281],\n          ...,\n          [-1.8636, -0.5818,  1.6433,  ...,  3.4872, -0.5548,  3.0635],\n          [-1.9728, -0.3254,  1.5736,  ...,  3.3537, -0.7912,  3.1003],\n          [-1.9125, -0.6242,  1.6143,  ...,  3.5778, -0.3186,  3.0579]],\n\n         [[ 1.4598, -0.9451, -3.2480,  ...,  2.8112, -0.9267, -2.1060],\n          [ 1.4782, -0.9459, -3.2513,  ...,  2.8077, -0.9184, -2.1029],\n          [ 1.4003, -0.8762, -3.1899,  ...,  2.8454, -0.9161, -2.1264],\n          ...,\n          [ 1.1680, -0.7923, -3.0122,  ...,  2.8702, -0.9755, -2.1775],\n          [ 1.2980, -0.8557, -3.1910,  ...,  2.8195, -0.9579, -2.1421],\n          [ 1.0930, -0.7505, -2.8751,  ...,  2.9638, -0.9196, -2.1691]],\n\n         [[ 0.0470, -3.0406,  5.5863,  ...,  3.3698, -1.3541,  5.3553],\n          [ 0.1194, -3.0269,  5.5700,  ...,  3.3612, -1.3581,  5.3071],\n          [-0.2175, -2.9021,  5.5669,  ...,  3.4221, -1.2482,  5.6953],\n          ...,\n          [-1.2456, -2.8007,  5.5142,  ...,  3.5226, -0.8560,  6.3438],\n          [-0.5653, -2.7643,  5.6081,  ...,  3.4244, -1.0192,  5.8480],\n          [-2.0592, -2.7397,  5.5948,  ...,  3.3443, -0.8898,  6.8651]],\n\n         ...,\n\n         [[ 1.6763,  0.6463, -0.5744,  ..., -1.6659, -3.2765, -1.5411],\n          [ 1.6514,  0.6633, -0.5702,  ..., -1.6507, -3.2533, -1.5444],\n          [ 1.8284,  0.7150, -0.8328,  ..., -1.6858, -3.2544, -1.3773],\n          ...,\n          [ 1.9960,  0.8376, -1.0850,  ..., -1.8765, -3.3294, -0.9064],\n          [ 1.7869,  0.8257, -0.9288,  ..., -1.8358, -3.1748, -1.2970],\n          [ 1.9806,  0.8037, -1.1315,  ..., -2.0237, -3.2895, -0.5616]],\n\n         [[-1.4361, -2.4575,  0.4060,  ..., -1.5081, -3.0930,  5.1031],\n          [-1.4663, -2.4184,  0.3827,  ..., -1.5217, -3.0577,  5.1138],\n          [-1.4292, -2.3759,  0.5324,  ..., -1.5302, -3.3604,  5.0115],\n          ...,\n          [-1.3786, -2.2541,  0.9093,  ..., -1.5814, -4.0179,  4.7639],\n          [-1.5830, -2.4041,  0.7112,  ..., -1.5915, -3.6087,  4.8639],\n          [-1.3180, -2.2334,  1.0661,  ..., -1.6784, -4.4775,  4.5763]],\n\n         [[ 2.4996,  0.7332, -0.7930,  ...,  2.7214,  1.7565,  3.8183],\n          [ 2.4735,  0.7017, -0.7975,  ...,  2.7049,  1.7440,  3.8396],\n          [ 2.7556,  0.9333, -0.8857,  ...,  2.7645,  1.8174,  3.8843],\n          ...,\n          [ 3.2882,  1.2288, -1.0752,  ...,  2.8282,  2.0030,  3.7701],\n          [ 3.0753,  1.0715, -1.0593,  ...,  2.6849,  1.8851,  3.7273],\n          [ 3.5805,  1.3862, -1.1617,  ...,  2.9807,  2.1076,  3.7453]]]],\n       grad_fn=<CloneBackward0>), tensor([[[[-0.3551,  0.8679,  0.6565,  ..., -0.2323, -0.6343,  0.1125],\n          [-0.3533,  0.8752,  0.6599,  ..., -0.2399, -0.6387,  0.1124],\n          [-0.3224,  0.8268,  0.6602,  ..., -0.2460, -0.6389,  0.1341],\n          ...,\n          [-0.2419,  0.6607,  0.6515,  ..., -0.2099, -0.6292,  0.2365],\n          [-0.2702,  0.7762,  0.6438,  ..., -0.2612, -0.6743,  0.3153],\n          [-0.1551,  0.5897,  0.6793,  ..., -0.1928, -0.7320,  0.2897]],\n\n         [[ 0.1054, -0.5107,  0.2633,  ..., -0.1710, -0.2835, -0.2647],\n          [ 0.1039, -0.5050,  0.2631,  ..., -0.1825, -0.2995, -0.2608],\n          [ 0.1033, -0.5668,  0.2634,  ..., -0.1681, -0.2228, -0.2971],\n          ...,\n          [ 0.0780, -0.5925,  0.2229,  ..., -0.1228,  0.0564, -0.3811],\n          [ 0.0972, -0.5919,  0.2547,  ..., -0.1762, -0.0974, -0.3480],\n          [ 0.0841, -0.6101,  0.2306,  ..., -0.0491,  0.1763, -0.3754]],\n\n         [[ 0.2710, -0.7272,  0.6504,  ..., -0.2662, -0.2989, -0.3785],\n          [ 0.2621, -0.7300,  0.6586,  ..., -0.2631, -0.2879, -0.3807],\n          [ 0.2604, -0.7123,  0.6122,  ..., -0.2389, -0.3433, -0.3682],\n          ...,\n          [ 0.2315, -0.6064,  0.5158,  ..., -0.1558, -0.4148, -0.3922],\n          [ 0.2325, -0.6352,  0.5416,  ..., -0.1500, -0.3455, -0.3843],\n          [ 0.2218, -0.6170,  0.5836,  ..., -0.1719, -0.4499, -0.3968]],\n\n         ...,\n\n         [[-0.4339,  0.3476,  0.0216,  ..., -0.0486, -0.2256,  0.1718],\n          [-0.4406,  0.3467,  0.0175,  ..., -0.0433, -0.2260,  0.1659],\n          [-0.4223,  0.3712,  0.0273,  ..., -0.0385, -0.2293,  0.1533],\n          ...,\n          [-0.3559,  0.3694,  0.0879,  ..., -0.0351, -0.2135,  0.1778],\n          [-0.3785,  0.3837,  0.0700,  ..., -0.0210, -0.2252,  0.1681],\n          [-0.3201,  0.3792,  0.1199,  ..., -0.0467, -0.2473,  0.1663]],\n\n         [[ 0.6791,  0.0541, -0.6672,  ...,  1.1801, -0.1065, -0.4531],\n          [ 0.6817,  0.0525, -0.6743,  ...,  1.1874, -0.1089, -0.4648],\n          [ 0.6708,  0.0320, -0.6594,  ...,  1.1503, -0.0666, -0.3876],\n          ...,\n          [ 0.6714, -0.0278, -0.6379,  ...,  0.9097,  0.0121, -0.2590],\n          [ 0.6981,  0.0277, -0.6731,  ...,  1.0697, -0.0117, -0.3597],\n          [ 0.6262, -0.0132, -0.6444,  ...,  0.8417,  0.0560, -0.2348]],\n\n         [[-1.0778, -0.3988, -0.6478,  ..., -0.2752,  0.2747, -0.0400],\n          [-1.0704, -0.4024, -0.6454,  ..., -0.2833,  0.2720, -0.0400],\n          [-1.0799, -0.3764, -0.6630,  ..., -0.2666,  0.2681, -0.0731],\n          ...,\n          [-1.1053, -0.3461, -0.6802,  ..., -0.2845,  0.2644, -0.1517],\n          [-1.1755, -0.3697, -0.6992,  ..., -0.2406,  0.2506, -0.1203],\n          [-1.0859, -0.3493, -0.6803,  ..., -0.3566,  0.2510, -0.1903]]]],\n       grad_fn=<CloneBackward0>), tensor([[[[ 5.1532, -2.1829,  0.3452,  ...,  3.9273,  1.9784, -1.9275],\n          [ 4.0745, -2.3167, -0.1809,  ...,  3.1965,  0.7565, -3.2056],\n          [ 4.4305, -3.0702, -1.2218,  ...,  2.6102,  1.2932, -5.3788],\n          ...,\n          [ 3.3370, -3.0949, -1.3468,  ...,  2.4322,  0.9188, -3.8371],\n          [ 3.6034, -2.4166, -0.7314,  ...,  2.9795,  0.7088, -3.3782],\n          [ 4.0982, -2.8514, -0.4597,  ...,  3.2772,  0.8553, -3.0580]],\n\n         [[ 4.6107,  2.8866, -2.0253,  ..., -4.8576, -0.3283, -0.1288],\n          [ 4.3960,  3.1945, -1.8664,  ..., -4.7113,  0.1857, -1.1010],\n          [ 3.6979,  3.4524, -1.6759,  ..., -3.0006,  1.0037, -0.0231],\n          ...,\n          [ 4.3606,  3.6331, -1.6227,  ..., -4.8237,  0.4342, -0.8035],\n          [ 4.7194,  3.4809, -1.7332,  ..., -4.3227,  0.0684, -0.9220],\n          [ 4.9119,  3.6732, -1.7122,  ..., -4.3433, -0.2648, -0.8287]],\n\n         [[-0.5901,  2.1387, -0.7246,  ...,  2.3890, -3.3915, -1.3836],\n          [-2.1717,  0.0435, -1.3975,  ...,  0.5749, -4.5267, -1.1043],\n          [-4.8083, -1.0403, -1.1140,  ..., -0.5978, -5.1172,  0.7276],\n          ...,\n          [-2.7492, -0.6836, -1.7094,  ...,  0.1607, -4.3891,  0.2777],\n          [-2.5964, -0.1668, -1.2767,  ...,  0.3958, -5.1386, -0.4613],\n          [-2.1099,  0.6033, -1.5843,  ...,  1.4623, -4.2148, -0.9580]],\n\n         ...,\n\n         [[-1.4427, -0.9459,  3.6274,  ...,  2.4571,  4.1448, -2.2402],\n          [-0.3032, -0.0660,  1.8239,  ...,  2.6200,  6.5283, -3.4068],\n          [-0.9750, -0.7190,  1.0172,  ...,  1.5736,  7.5159, -3.1140],\n          ...,\n          [-0.3533,  0.1221,  1.3066,  ...,  2.6200,  6.9991, -3.5147],\n          [ 0.3535,  0.4823,  1.7116,  ...,  2.5484,  6.9153, -3.3433],\n          [-0.5139, -0.4638,  2.1830,  ...,  2.4707,  5.9243, -2.8498]],\n\n         [[ 5.6862,  2.9160,  4.4996,  ...,  0.3831, -3.0540, -0.4892],\n          [ 6.8528,  2.4254,  3.4881,  ..., -0.8602, -1.5028, -0.5749],\n          [ 6.3635,  2.4711,  3.1737,  ..., -0.2752, -0.8244, -0.0858],\n          ...,\n          [ 6.6427,  2.3167,  3.4869,  ..., -0.7799, -1.2772, -0.7416],\n          [ 6.5699,  2.7349,  3.6873,  ..., -1.0474, -1.0965, -0.6599],\n          [ 6.1237,  2.4428,  3.8527,  ..., -0.5946, -1.9544, -0.4773]],\n\n         [[ 4.8169,  2.0547,  0.1918,  ...,  2.5073,  1.0612,  2.7123],\n          [ 5.8714, -0.7486,  0.9832,  ...,  3.2242,  0.5865,  2.8954],\n          [ 5.2990, -2.2758,  2.3922,  ...,  2.7975, -0.4489,  1.0983],\n          ...,\n          [ 6.1412, -0.6506,  1.8113,  ...,  3.1450,  0.3353,  1.9948],\n          [ 5.8469, -0.6182,  1.1390,  ...,  3.3243,  1.0438,  2.8697],\n          [ 5.8293, -0.1021,  1.2298,  ...,  3.1454,  0.6361,  2.8459]]]],\n       grad_fn=<CloneBackward0>), tensor([[[[-0.8734,  0.3206,  3.9854,  ..., -1.6321, -0.2276, -0.8672],\n          [-0.6601,  0.5428,  4.3557,  ..., -1.5308, -0.3448, -1.0022],\n          [-0.0687,  0.8423,  4.4060,  ..., -1.4009, -0.8046, -1.5189],\n          ...,\n          [-0.1721,  0.5382,  4.4113,  ..., -1.5105, -0.4750, -1.0766],\n          [-0.4433,  0.7434,  4.2899,  ..., -1.5001, -0.0813, -0.9080],\n          [-0.6899,  0.4831,  4.5659,  ..., -1.3837, -0.3988, -1.0133]],\n\n         [[ 1.5425, -1.0913,  1.4123,  ..., -0.9470,  0.2165,  1.3431],\n          [ 0.8064, -1.6839,  0.3621,  ..., -0.0651, -0.5961,  1.1711],\n          [ 0.1705, -1.0023,  1.3269,  ..., -0.9507, -0.2512,  1.0419],\n          ...,\n          [ 0.3101, -1.4050,  0.6254,  ..., -0.3798, -0.4046,  0.8773],\n          [ 0.3549, -1.4719,  0.4682,  ..., -0.1495, -0.6831,  1.3608],\n          [ 0.7076, -1.5310,  0.7857,  ..., -0.5157, -0.4733,  1.3088]],\n\n         [[-0.5827, -0.1342,  0.5100,  ...,  0.8832, -0.5514, -0.8046],\n          [-0.4393,  0.3232,  0.1489,  ...,  0.0433, -0.3639, -0.9274],\n          [-0.3289,  0.4771, -0.7593,  ...,  0.2352, -0.9315, -0.7840],\n          ...,\n          [-0.0613,  0.4817, -0.1447,  ..., -0.0972, -0.3746, -0.7257],\n          [-0.6014,  0.3123,  0.0696,  ..., -0.2319, -0.3829, -1.3332],\n          [-0.1359,  0.3685,  0.1582,  ..., -0.0393, -0.5003, -1.0468]],\n\n         ...,\n\n         [[ 1.4116,  0.1894,  1.9661,  ...,  2.5850, -0.0351,  3.8436],\n          [ 1.3279, -0.9001,  0.7562,  ...,  2.5644,  0.6651,  4.0394],\n          [ 1.0571, -1.2088,  0.8707,  ...,  2.8429, -0.3183,  3.9091],\n          ...,\n          [ 1.0303, -1.1712,  0.9803,  ...,  2.2313,  0.4049,  3.9564],\n          [ 1.6374, -1.1178,  0.5821,  ...,  2.7684,  0.7741,  3.8840],\n          [ 1.3816, -0.7150,  1.1977,  ...,  2.2363,  0.4966,  3.7506]],\n\n         [[ 0.4113, -1.8088, -2.2218,  ...,  1.6340,  3.3296,  0.6201],\n          [ 1.7410, -2.8800, -1.9897,  ...,  1.4421,  3.2193, -0.2921],\n          [ 2.5953, -3.3115, -1.6352,  ...,  1.6735,  3.1223, -1.4755],\n          ...,\n          [ 1.8180, -3.3690, -2.0638,  ...,  1.3406,  3.0974, -1.1145],\n          [ 2.2931, -2.6769, -1.8309,  ...,  1.1148,  3.0799, -0.5054],\n          [ 1.6916, -2.8086, -1.9137,  ...,  1.7622,  3.4564, -0.2502]],\n\n         [[ 0.9873,  1.4855,  0.3644,  ...,  2.5881, -0.9331,  0.5901],\n          [ 0.5232,  0.1386,  0.6415,  ...,  2.1515, -0.5506,  0.1989],\n          [ 0.2873, -0.1730,  0.8732,  ...,  1.3115, -0.3214, -0.0410],\n          ...,\n          [ 0.8340,  0.2910,  0.8488,  ...,  2.2968, -0.5723, -0.3106],\n          [ 0.5506, -0.0071,  0.7116,  ...,  1.9541, -0.5471,  0.1888],\n          [ 0.7936,  0.4797,  0.7749,  ...,  2.4309, -0.7000,  0.3984]]]],\n       grad_fn=<CloneBackward0>)), (tensor([[[[-2.6773e-01,  2.3083e+00,  3.3672e+00,  ...,  2.3521e+00,\n            4.5662e+00, -5.6813e+00],\n          [-2.5156e-01,  2.2880e+00,  3.3289e+00,  ...,  2.3304e+00,\n            4.5619e+00, -5.6671e+00],\n          [-3.2257e-01,  2.5023e+00,  3.5162e+00,  ...,  2.4319e+00,\n            4.7216e+00, -5.7142e+00],\n          ...,\n          [-5.8616e-01,  2.8564e+00,  3.7977e+00,  ...,  2.6141e+00,\n            5.2607e+00, -5.9601e+00],\n          [-5.0269e-01,  2.7583e+00,  3.6076e+00,  ...,  2.4659e+00,\n            4.8471e+00, -5.7713e+00],\n          [-7.1437e-01,  3.1554e+00,  4.0076e+00,  ...,  2.9231e+00,\n            5.6991e+00, -6.1212e+00]],\n\n         [[-1.0209e+00, -1.7690e+00, -2.6581e-01,  ..., -2.7017e+00,\n           -2.3111e+00,  1.9685e+00],\n          [-9.9240e-01, -1.7687e+00, -2.7135e-01,  ..., -2.6602e+00,\n           -2.3509e+00,  1.9650e+00],\n          [-1.0154e+00, -1.8186e+00, -1.3247e-01,  ..., -3.0135e+00,\n           -2.1366e+00,  1.9430e+00],\n          ...,\n          [-1.2178e+00, -2.1155e+00,  4.0235e-02,  ..., -3.6918e+00,\n           -1.5681e+00,  1.7640e+00],\n          [-1.0983e+00, -1.9170e+00,  4.9393e-03,  ..., -3.2194e+00,\n           -1.9791e+00,  1.8303e+00],\n          [-1.4676e+00, -2.3613e+00,  3.3840e-02,  ..., -4.1914e+00,\n           -1.3907e+00,  1.5156e+00]],\n\n         [[-4.6183e-01, -1.6116e+00,  1.9499e+00,  ...,  1.4185e+00,\n            1.9131e+00, -5.8574e-02],\n          [-4.3426e-01, -1.6003e+00,  1.9437e+00,  ...,  1.4088e+00,\n            1.8616e+00, -5.0395e-02],\n          [-4.7930e-01, -1.6391e+00,  2.0060e+00,  ...,  1.5442e+00,\n            2.0970e+00,  2.1859e-02],\n          ...,\n          [-4.5646e-01, -1.7374e+00,  2.1278e+00,  ...,  1.7042e+00,\n            2.5294e+00,  5.1881e-02],\n          [-5.9583e-01, -1.6644e+00,  2.0101e+00,  ...,  1.6581e+00,\n            2.2517e+00, -7.5722e-04],\n          [-3.6878e-01, -1.8935e+00,  1.9467e+00,  ...,  1.7577e+00,\n            2.8175e+00,  1.3369e-01]],\n\n         ...,\n\n         [[ 3.5586e+00,  1.1349e+00, -6.9585e-02,  ..., -2.2739e+00,\n            1.5525e+00,  5.6110e+00],\n          [ 3.5510e+00,  1.1776e+00, -8.8604e-02,  ..., -2.2725e+00,\n            1.5365e+00,  5.6242e+00],\n          [ 3.5891e+00,  1.0716e+00,  1.0947e-01,  ..., -2.3521e+00,\n            1.6131e+00,  5.6573e+00],\n          ...,\n          [ 3.5129e+00,  8.5257e-01,  5.2009e-01,  ..., -2.4138e+00,\n            1.6648e+00,  5.6943e+00],\n          [ 3.5251e+00,  9.3673e-01,  2.8028e-01,  ..., -2.4649e+00,\n            1.4810e+00,  5.6300e+00],\n          [ 3.5435e+00,  6.8095e-01,  8.9520e-01,  ..., -2.4729e+00,\n            1.7542e+00,  5.6962e+00]],\n\n         [[-4.6406e-01, -8.7514e-01,  1.7042e+00,  ...,  1.5842e+00,\n           -3.6867e-01,  7.9333e+00],\n          [-4.3698e-01, -8.6641e-01,  1.7272e+00,  ...,  1.5926e+00,\n           -3.5550e-01,  7.9056e+00],\n          [-6.0202e-01, -8.3214e-01,  1.7127e+00,  ...,  1.5886e+00,\n           -3.8533e-01,  8.0356e+00],\n          ...,\n          [-7.7909e-01, -6.9153e-01,  1.6832e+00,  ...,  1.6336e+00,\n           -4.2687e-01,  8.2179e+00],\n          [-5.9804e-01, -8.5517e-01,  1.7553e+00,  ...,  1.5392e+00,\n           -4.6349e-01,  8.0628e+00],\n          [-8.9885e-01, -5.6274e-01,  1.6400e+00,  ...,  1.7252e+00,\n           -4.5639e-01,  8.2763e+00]],\n\n         [[-2.9387e+00, -4.4508e+00, -1.9543e-01,  ...,  1.9769e-01,\n            1.3568e+00, -2.8640e+00],\n          [-2.9320e+00, -4.4524e+00, -2.0849e-01,  ...,  2.0572e-01,\n            1.3496e+00, -2.8048e+00],\n          [-2.9685e+00, -4.4325e+00, -1.5563e-01,  ...,  1.7230e-01,\n            1.3658e+00, -3.2121e+00],\n          ...,\n          [-3.0820e+00, -4.5450e+00, -1.5209e-02,  ...,  2.3841e-01,\n            1.3942e+00, -3.8297e+00],\n          [-2.8974e+00, -4.5305e+00, -2.0908e-01,  ...,  2.3685e-01,\n            1.4857e+00, -3.3419e+00],\n          [-3.2392e+00, -4.4773e+00, -4.8437e-02,  ...,  1.6136e-01,\n            1.3742e+00, -4.1621e+00]]]], grad_fn=<CloneBackward0>), tensor([[[[ 1.0771, -0.8736,  0.3528,  ...,  0.3318, -0.7280,  0.3558],\n          [ 1.0778, -0.8777,  0.3491,  ...,  0.3288, -0.7414,  0.3634],\n          [ 1.1077, -0.8848,  0.3801,  ...,  0.3273, -0.6211,  0.3195],\n          ...,\n          [ 1.2049, -0.8300,  0.4896,  ...,  0.3466, -0.3815,  0.2123],\n          [ 1.1085, -0.8726,  0.4298,  ...,  0.3471, -0.5295,  0.2676],\n          [ 1.2392, -0.8445,  0.5753,  ...,  0.3749, -0.2766,  0.2049]],\n\n         [[ 1.3491,  0.7201, -0.9256,  ...,  0.1387,  0.0384,  0.9759],\n          [ 1.3625,  0.7187, -0.9281,  ...,  0.1355,  0.0463,  0.9815],\n          [ 1.3047,  0.7654, -0.8884,  ...,  0.1456,  0.0493,  0.9253],\n          ...,\n          [ 1.1855,  0.7410, -0.7392,  ...,  0.1590,  0.0264,  0.8175],\n          [ 1.2402,  0.7599, -0.8266,  ...,  0.0965,  0.0653,  0.8592],\n          [ 1.0909,  0.7536, -0.6486,  ...,  0.2029, -0.0523,  0.7544]],\n\n         [[ 0.5336, -0.6761,  1.1721,  ..., -0.5566, -1.5438,  0.6510],\n          [ 0.5453, -0.6769,  1.1667,  ..., -0.5514, -1.5502,  0.6490],\n          [ 0.4947, -0.6927,  1.1639,  ..., -0.5462, -1.5380,  0.6737],\n          ...,\n          [ 0.3517, -0.6857,  1.1286,  ..., -0.4760, -1.5357,  0.6630],\n          [ 0.3978, -0.7198,  1.1410,  ..., -0.4897, -1.5483,  0.6989],\n          [ 0.2820, -0.6275,  1.1304,  ..., -0.4483, -1.5406,  0.5889]],\n\n         ...,\n\n         [[-0.2476,  0.1191,  1.0375,  ..., -1.1057,  0.7368, -0.0140],\n          [-0.2516,  0.1156,  1.0396,  ..., -1.1017,  0.7441, -0.0214],\n          [-0.2639,  0.1642,  1.0456,  ..., -1.1027,  0.6697, -0.0760],\n          ...,\n          [-0.2183,  0.2849,  0.9006,  ..., -1.0816,  0.5049, -0.1628],\n          [-0.2120,  0.2418,  0.9789,  ..., -1.1557,  0.5876, -0.1300],\n          [-0.2212,  0.3255,  0.8357,  ..., -1.0818,  0.4107, -0.1410]],\n\n         [[ 0.1497, -0.1418,  0.4233,  ...,  1.6344,  0.7075, -0.5994],\n          [ 0.1574, -0.1458,  0.4234,  ...,  1.6438,  0.7042, -0.6093],\n          [ 0.1383, -0.1280,  0.4278,  ...,  1.6820,  0.7059, -0.5317],\n          ...,\n          [ 0.1205, -0.1015,  0.4043,  ...,  1.6642,  0.6724, -0.2893],\n          [ 0.1123, -0.1617,  0.4189,  ...,  1.6731,  0.7007, -0.4366],\n          [ 0.1754, -0.0755,  0.4315,  ...,  1.5872,  0.6734, -0.1942]],\n\n         [[ 0.4618,  0.4098,  0.2808,  ..., -0.9872,  0.2571, -0.1525],\n          [ 0.4731,  0.4089,  0.2879,  ..., -0.9794,  0.2647, -0.1418],\n          [ 0.5011,  0.3459,  0.2630,  ..., -0.9866,  0.2935, -0.1023],\n          ...,\n          [ 0.5079,  0.3332,  0.2045,  ..., -1.0338,  0.3297, -0.1636],\n          [ 0.5396,  0.3627,  0.2769,  ..., -0.9889,  0.3282, -0.1576],\n          [ 0.5397,  0.2976,  0.0582,  ..., -1.1039,  0.3202, -0.1868]]]],\n       grad_fn=<CloneBackward0>), tensor([[[[ 1.9428, -2.3023, -0.3477,  ..., -3.5528,  0.1414,  0.1262],\n          [ 2.3701, -2.5859, -0.7978,  ..., -2.8754,  0.8290, -0.6835],\n          [ 2.1846, -3.3503, -2.5516,  ..., -1.9658,  1.4256, -1.5790],\n          ...,\n          [ 2.2333, -2.5078, -1.3921,  ..., -2.5103,  1.2290, -1.4853],\n          [ 2.4140, -3.7588, -1.1407,  ..., -2.6376,  1.5838, -0.8565],\n          [ 2.2106, -2.7513, -0.5544,  ..., -3.3655,  0.6372, -0.3835]],\n\n         [[-4.3560, -3.0743, -3.6749,  ...,  7.0095,  2.1966, -0.9475],\n          [-4.0930, -2.6805, -4.5908,  ...,  7.1579,  2.0024, -1.3537],\n          [-3.2891, -3.2174, -5.3393,  ...,  5.9513,  2.0718, -0.9116],\n          ...,\n          [-4.2822, -3.0131, -4.7619,  ...,  6.4470,  2.1502, -1.0311],\n          [-3.7578, -3.0854, -4.4519,  ...,  7.3975,  1.9058, -1.0884],\n          [-3.9268, -2.9485, -4.6394,  ...,  6.9928,  2.4105, -1.2167]],\n\n         [[-2.8887, -1.4207,  0.3062,  ...,  0.9288, -3.4105,  4.4931],\n          [-1.3671, -3.2431,  0.6200,  ...,  0.9871, -2.5613,  3.1684],\n          [-1.7104, -3.1920,  0.2466,  ...,  1.4801, -1.9441,  2.4945],\n          ...,\n          [-1.0278, -3.2133,  1.0346,  ...,  0.9365, -2.4622,  2.7573],\n          [-1.5379, -3.5426,  0.6807,  ...,  0.8528, -2.8837,  2.9022],\n          [-1.5324, -2.7969,  0.7254,  ...,  0.9504, -2.3931,  3.4841]],\n\n         ...,\n\n         [[-0.4491,  1.8434, -2.4857,  ...,  2.6403, -3.3438, -3.1293],\n          [-0.4935, -0.0920, -2.2885,  ...,  1.6289, -3.7592, -3.2599],\n          [-0.7211,  0.0520, -1.6213,  ...,  2.0146, -3.9492, -1.2201],\n          ...,\n          [-0.2139, -0.0701, -2.5345,  ...,  1.5656, -3.8300, -2.1099],\n          [-0.4476, -0.3796, -2.1362,  ...,  1.4433, -4.4008, -2.4987],\n          [-0.1619,  0.5675, -2.4738,  ...,  1.8355, -3.4838, -3.1191]],\n\n         [[ 0.2490, -0.3519,  1.5793,  ...,  1.8238,  0.0377, -5.0118],\n          [ 0.6885,  0.5654,  2.5319,  ...,  3.7324, -0.7787, -4.3063],\n          [ 1.2043,  0.3416,  2.2646,  ...,  3.3371, -1.7167, -1.1075],\n          ...,\n          [ 0.7328,  0.6990,  2.7239,  ...,  3.4979, -1.1820, -3.4130],\n          [ 0.8691,  0.6294,  2.3086,  ...,  3.7879, -1.1024, -3.8020],\n          [ 0.5688,  0.7255,  2.0348,  ...,  3.4895, -0.7731, -3.9904]],\n\n         [[ 0.1328,  0.1375,  1.5381,  ..., -0.7067, -4.1060,  3.0197],\n          [ 0.2170,  2.2434,  2.1486,  ..., -1.1534, -3.2251,  2.1491],\n          [ 1.6068,  2.7724,  1.7707,  ..., -0.9226, -2.2411,  4.9218],\n          ...,\n          [-0.4061,  2.6510,  2.2722,  ..., -1.5623, -2.7045,  2.7597],\n          [-0.3080,  2.1930,  2.0164,  ..., -1.0267, -3.1341,  2.5430],\n          [-0.1596,  1.7531,  2.0175,  ..., -1.2363, -2.9472,  2.5287]]]],\n       grad_fn=<CloneBackward0>), tensor([[[[-3.4172,  1.0787,  0.5592,  ...,  0.2659,  1.5692,  2.7852],\n          [-2.7735,  0.5158,  1.3197,  ..., -0.0114,  2.7202,  1.7230],\n          [-2.3679,  0.7073,  2.0630,  ...,  0.3845,  2.6086,  0.3608],\n          ...,\n          [-2.4764,  0.4777,  1.0843,  ..., -0.1143,  2.6476,  1.4154],\n          [-2.8664,  0.4108,  1.3946,  ...,  0.1928,  2.9006,  1.5444],\n          [-2.7167,  0.5304,  0.9736,  ..., -0.0576,  2.3521,  2.0938]],\n\n         [[ 1.1481, -1.0877,  1.5298,  ...,  1.0577,  0.1133,  1.0938],\n          [ 0.4127, -0.0981,  2.7780,  ...,  1.2293,  0.8362,  1.2978],\n          [ 0.3100,  1.0273,  3.4573,  ...,  1.1948,  0.2214,  1.8313],\n          ...,\n          [ 0.5970,  0.8242,  2.8992,  ...,  1.3921,  0.8411,  1.1666],\n          [ 0.5940,  0.7589,  2.9244,  ...,  1.1975,  1.2741,  1.2461],\n          [ 0.7374, -0.2877,  2.6395,  ...,  1.1743,  0.7758,  0.9819]],\n\n         [[-0.1843,  2.0450,  0.5108,  ...,  1.3724, -0.5110,  1.1890],\n          [ 0.5307,  2.5777,  0.4665,  ...,  1.5634, -0.3317,  0.3862],\n          [ 2.1754,  2.7167,  0.3181,  ...,  0.5977,  0.0552, -0.3997],\n          ...,\n          [ 1.1440,  2.4958,  0.3837,  ...,  1.6530, -0.5029,  0.2539],\n          [ 0.6119,  2.4323,  0.5275,  ...,  1.5911, -0.3952,  0.6175],\n          [ 0.5320,  2.2969,  0.2673,  ...,  1.5311, -0.2322,  0.5931]],\n\n         ...,\n\n         [[ 0.1133,  2.8385,  2.4198,  ...,  0.2656,  0.2423,  1.0975],\n          [-0.2764,  3.7686,  1.8910,  ...,  0.1535,  0.6927,  1.0602],\n          [-0.8134,  3.2816,  1.8054,  ...,  0.0168,  0.7462,  0.4127],\n          ...,\n          [-0.3390,  3.6309,  1.7828,  ...,  0.0197,  0.1983,  0.5181],\n          [-0.3235,  3.9185,  1.8205,  ...,  0.1371,  0.2597,  0.5327],\n          [-0.2638,  3.4231,  1.9199,  ...,  0.0338,  0.2805,  0.9007]],\n\n         [[-2.0799, -2.3706,  1.8514,  ..., -2.2965, -1.2713,  2.0559],\n          [-2.2990, -2.4113,  1.2814,  ..., -2.8421, -0.6422,  1.8426],\n          [-2.9207, -1.8359,  1.8525,  ..., -3.6779,  0.7356,  2.1843],\n          ...,\n          [-2.6940, -2.2949,  1.7935,  ..., -2.9955, -0.3577,  1.9896],\n          [-2.3522, -2.2089,  1.1607,  ..., -3.1708, -0.8472,  1.9565],\n          [-2.2993, -2.3385,  1.5456,  ..., -2.6245, -0.6479,  2.1118]],\n\n         [[-0.3752, -0.4831,  0.3067,  ...,  0.9092, -1.9974, -0.3654],\n          [-0.3072, -0.1595,  0.6664,  ...,  1.1126, -1.8799,  0.2156],\n          [-0.5771,  0.1697,  1.4105,  ...,  1.1605, -1.8021, -0.5602],\n          ...,\n          [-0.1548, -0.1282,  0.8827,  ...,  1.5604, -1.6447,  0.3827],\n          [-0.3839, -0.4347,  0.9611,  ...,  1.3351, -1.7554,  0.5214],\n          [-0.2464, -0.3043,  0.6288,  ...,  1.1075, -1.7920,  0.0319]]]],\n       grad_fn=<CloneBackward0>)), (tensor([[[[-1.4812, -2.7121, -2.2793,  ...,  1.2447,  0.1213,  1.9581],\n          [-1.5070, -2.6792, -2.2476,  ...,  1.2684,  0.1243,  1.9542],\n          [-1.3653, -2.6290, -2.4032,  ...,  1.1475,  0.1226,  2.0119],\n          ...,\n          [-1.0227, -2.7539, -2.6731,  ...,  0.9457,  0.3372,  2.0224],\n          [-1.1773, -2.6399, -2.4633,  ...,  1.1968,  0.1427,  2.0475],\n          [-0.8076, -2.7889, -3.0796,  ...,  0.8844,  0.5519,  2.1250]],\n\n         [[ 5.3381, -1.2240, -4.2237,  ..., -3.7160, -8.0675,  4.3435],\n          [ 5.3063, -1.2506, -4.2268,  ..., -3.6893, -8.0516,  4.3110],\n          [ 5.5293, -1.1015, -4.3091,  ..., -3.9179, -8.1104,  4.5136],\n          ...,\n          [ 5.9606, -0.8047, -4.4135,  ..., -4.2999, -8.1832,  4.9343],\n          [ 5.6526, -1.0825, -4.4209,  ..., -4.0773, -8.2149,  4.6344],\n          [ 6.0346, -0.5914, -4.3751,  ..., -4.5396, -8.1712,  5.2021]],\n\n         [[-3.9943, -1.1882, -3.5199,  ..., -1.9952,  1.6264,  0.3722],\n          [-3.9934, -1.1889, -3.4817,  ..., -1.9779,  1.5757,  0.3748],\n          [-4.1211, -1.0869, -3.6994,  ..., -2.0226,  1.7360,  0.2582],\n          ...,\n          [-4.4362, -0.9868, -3.9783,  ..., -2.2472,  2.1794,  0.1978],\n          [-4.3762, -1.1264, -3.7485,  ..., -2.1902,  1.8662,  0.2033],\n          [-4.6270, -1.0336, -4.0667,  ..., -2.4257,  2.3517,  0.2676]],\n\n         ...,\n\n         [[ 2.6859,  1.6924,  6.2166,  ..., -1.7809, -2.5701,  1.1438],\n          [ 2.6578,  1.6559,  6.1704,  ..., -1.7789, -2.5784,  1.1527],\n          [ 2.8898,  1.9164,  6.4811,  ..., -1.8435, -2.5788,  1.2605],\n          ...,\n          [ 3.2867,  2.2362,  7.2692,  ..., -2.1018, -2.6273,  1.4853],\n          [ 3.0441,  2.0009,  6.8276,  ..., -1.8854, -2.6682,  1.3349],\n          [ 3.6883,  2.2805,  7.7973,  ..., -2.2021, -2.6317,  1.6895]],\n\n         [[ 2.9569, -3.1645, -2.6839,  ...,  6.3068, -3.9392,  2.5981],\n          [ 2.9525, -3.1133, -2.6843,  ...,  6.2809, -3.9066,  2.5776],\n          [ 2.9885, -3.3813, -2.8086,  ...,  6.5185, -4.1984,  2.7144],\n          ...,\n          [ 3.0215, -4.0433, -2.9910,  ...,  7.0263, -4.7528,  3.2379],\n          [ 3.1124, -3.6878, -2.9790,  ...,  6.7085, -4.3665,  2.8331],\n          [ 2.9744, -4.2946, -3.1683,  ...,  7.2477, -5.0943,  3.4645]],\n\n         [[ 1.2426, -6.5481, -1.2526,  ..., -1.8455,  1.2188, -1.1340],\n          [ 1.2339, -6.5172, -1.2544,  ..., -1.8185,  1.1771, -1.1274],\n          [ 1.1453, -6.7108, -1.0746,  ..., -1.9989,  1.3472, -1.1386],\n          ...,\n          [ 1.0693, -6.9822, -0.7114,  ..., -2.4184,  1.6608, -1.2872],\n          [ 1.0374, -6.7730, -0.8483,  ..., -2.2405,  1.3332, -1.1298],\n          [ 1.3102, -7.0175, -0.5851,  ..., -2.6362,  1.6731, -1.4080]]]],\n       grad_fn=<CloneBackward0>), tensor([[[[ 1.9346e-01, -1.9073e-01,  6.3339e-02,  ...,  4.0022e-01,\n           -1.1371e+00,  7.4556e-01],\n          [ 2.0565e-01, -1.9678e-01,  6.2033e-02,  ...,  3.9186e-01,\n           -1.1486e+00,  7.4643e-01],\n          [ 2.3435e-01, -2.1775e-01,  1.1049e-01,  ...,  4.5970e-01,\n           -1.1224e+00,  7.4290e-01],\n          ...,\n          [ 3.1699e-01, -2.9184e-01,  1.0993e-01,  ...,  4.5309e-01,\n           -1.0552e+00,  6.3500e-01],\n          [ 2.7953e-01, -2.0299e-01,  1.5793e-01,  ...,  4.0482e-01,\n           -1.0821e+00,  7.1018e-01],\n          [ 3.4741e-01, -3.4270e-01,  4.7453e-03,  ...,  4.7622e-01,\n           -9.6045e-01,  5.5271e-01]],\n\n         [[-1.7065e-01,  5.6499e-02, -7.3038e-01,  ...,  9.2577e-01,\n           -7.7974e-01, -3.3770e-01],\n          [-1.7490e-01,  6.3069e-02, -7.2755e-01,  ...,  9.2847e-01,\n           -7.7815e-01, -3.2417e-01],\n          [-1.4887e-01, -1.8219e-03, -7.4878e-01,  ...,  8.4971e-01,\n           -7.8659e-01, -3.1398e-01],\n          ...,\n          [-5.2933e-02, -5.2838e-02, -8.0378e-01,  ...,  7.3867e-01,\n           -8.3143e-01, -2.4347e-01],\n          [-1.3965e-01, -3.1710e-02, -7.6680e-01,  ...,  8.2758e-01,\n           -8.0159e-01, -3.0979e-01],\n          [ 2.3579e-02, -6.2295e-02, -8.7238e-01,  ...,  6.8730e-01,\n           -7.8026e-01, -2.1535e-01]],\n\n         [[-2.3691e-01,  1.1778e+00,  9.5154e-01,  ..., -1.6158e-01,\n            5.5052e-01,  1.2195e+00],\n          [-2.3741e-01,  1.1852e+00,  9.6412e-01,  ..., -1.4609e-01,\n            5.5843e-01,  1.2169e+00],\n          [-2.2683e-01,  1.1673e+00,  8.9498e-01,  ..., -1.8515e-01,\n            5.6032e-01,  1.1257e+00],\n          ...,\n          [-2.4467e-01,  1.0383e+00,  7.5737e-01,  ..., -2.4738e-01,\n            4.4364e-01,  1.0058e+00],\n          [-2.5716e-01,  1.1289e+00,  8.4204e-01,  ..., -2.1203e-01,\n            4.3388e-01,  1.0566e+00],\n          [-2.2547e-01,  9.5559e-01,  7.4893e-01,  ..., -2.8447e-01,\n            4.5385e-01,  9.1380e-01]],\n\n         ...,\n\n         [[-7.7565e-01, -1.7273e-02, -5.7894e-01,  ...,  4.9108e-01,\n            1.2656e-01,  6.8351e-03],\n          [-7.7521e-01, -1.1436e-02, -5.8405e-01,  ...,  4.9633e-01,\n            1.2576e-01,  2.7605e-03],\n          [-8.1428e-01, -1.0454e-01, -6.4307e-01,  ...,  4.8303e-01,\n            1.2140e-01,  4.1031e-04],\n          ...,\n          [-8.7318e-01, -2.4105e-01, -6.7097e-01,  ...,  4.4875e-01,\n            2.1210e-02, -1.4246e-01],\n          [-8.1042e-01, -1.9122e-01, -6.2221e-01,  ...,  5.1568e-01,\n            5.8291e-02, -6.3810e-02],\n          [-8.8820e-01, -3.2979e-01, -7.2922e-01,  ...,  4.1967e-01,\n            2.4013e-02, -1.7904e-01]],\n\n         [[-6.3613e-01, -1.6939e+00, -9.6713e-01,  ..., -2.8893e-01,\n           -2.4603e-01, -1.5846e-01],\n          [-6.2945e-01, -1.6892e+00, -9.6771e-01,  ..., -2.8982e-01,\n           -2.4766e-01, -1.5327e-01],\n          [-6.4463e-01, -1.7094e+00, -9.5267e-01,  ..., -2.6738e-01,\n           -2.0846e-01, -1.5965e-01],\n          ...,\n          [-6.3040e-01, -1.8233e+00, -9.2220e-01,  ..., -2.5879e-01,\n           -1.2649e-01, -2.0970e-01],\n          [-6.2864e-01, -1.7635e+00, -9.4713e-01,  ..., -2.5280e-01,\n           -2.0153e-01, -1.7997e-01],\n          [-6.4401e-01, -1.8943e+00, -9.2017e-01,  ..., -2.5337e-01,\n           -7.1234e-03, -2.5757e-01]],\n\n         [[-2.9629e-01,  3.7542e-02,  5.9607e-02,  ..., -2.6774e-01,\n            5.9329e-02,  4.4685e-02],\n          [-2.9519e-01,  3.2467e-02,  7.2149e-02,  ..., -2.6401e-01,\n            5.7244e-02,  3.6072e-02],\n          [-3.1591e-01,  6.7318e-02,  6.4226e-02,  ..., -3.1414e-01,\n            3.1191e-02,  5.7298e-02],\n          ...,\n          [-2.0890e-01,  1.0959e-01, -7.5977e-02,  ..., -2.4447e-01,\n           -2.6764e-02,  9.0491e-02],\n          [-2.2401e-01,  4.4882e-02,  2.0495e-02,  ..., -2.6209e-01,\n           -3.5888e-02,  4.4901e-03],\n          [-1.9673e-01,  1.5203e-01, -7.6486e-02,  ..., -2.1309e-01,\n           -1.0600e-02,  1.3360e-01]]]], grad_fn=<CloneBackward0>), tensor([[[[-1.6115, -3.5449, -1.4531,  ..., -0.1490, -2.2244, -3.5352],\n          [ 0.3609, -1.9360, -3.1096,  ..., -1.0106, -1.3328, -2.9397],\n          [-0.3827,  0.1991, -3.0526,  ..., -0.9760, -1.2201, -0.7944],\n          ...,\n          [ 0.0351, -1.3469, -3.7830,  ..., -1.5180, -0.5524, -2.2814],\n          [ 0.6979, -1.6910, -3.7931,  ..., -1.3010, -1.2707, -2.9789],\n          [-0.6310, -2.3544, -3.2337,  ..., -0.6255, -1.4946, -2.7263]],\n\n         [[-1.9658,  0.1043, -1.6613,  ...,  2.3125, -0.7840,  2.4834],\n          [-0.5391, -0.3951, -1.2884,  ..., -0.2032, -1.3682,  2.0892],\n          [-0.7824, -1.3097, -0.2602,  ...,  0.4012, -1.1627,  1.4587],\n          ...,\n          [-0.4828, -0.7561, -0.6168,  ..., -0.1965, -0.3842,  2.0715],\n          [-0.6246, -0.8240, -1.0156,  ...,  0.1824, -1.1679,  1.9315],\n          [-0.9443, -0.5083, -0.8827,  ...,  0.2913, -1.1356,  2.3513]],\n\n         [[ 1.2316,  7.5576,  1.5258,  ...,  2.5513,  1.1654,  1.3881],\n          [ 0.6389,  6.6319,  0.9496,  ...,  1.8054, -0.6852,  1.5263],\n          [ 0.1087,  4.9847, -0.4183,  ...,  2.4004, -0.3385,  2.5694],\n          ...,\n          [ 1.1891,  6.4147,  0.8753,  ...,  2.0236, -0.9732,  1.9931],\n          [ 0.9089,  6.5355,  1.0572,  ...,  1.7699, -0.5192,  1.7613],\n          [ 0.9148,  6.9733,  1.3535,  ...,  1.7974, -0.1721,  1.7733]],\n\n         ...,\n\n         [[-2.7872, -1.8147,  2.4361,  ..., -4.4276,  7.4077,  3.6831],\n          [-3.8881, -2.1572,  3.7513,  ..., -3.6363,  7.1234,  2.5774],\n          [-5.0402, -3.0204,  3.6665,  ..., -4.9966,  6.0066,  3.1345],\n          ...,\n          [-3.7226, -2.1458,  3.3820,  ..., -3.8503,  6.8430,  2.3256],\n          [-3.8066, -3.0285,  3.5120,  ..., -3.1266,  6.9108,  2.6551],\n          [-3.6594, -2.2301,  3.1076,  ..., -3.8743,  7.0058,  2.8457]],\n\n         [[ 0.9971,  0.2325,  1.2325,  ...,  6.0832,  1.6005, -3.7371],\n          [-0.1862,  2.0282,  0.4382,  ...,  6.3014, -1.5272, -2.9672],\n          [-1.0691,  4.5051, -0.4284,  ...,  5.4151, -2.0779, -2.9176],\n          ...,\n          [-1.0559,  2.6809, -0.0639,  ...,  5.9867, -2.1665, -3.0196],\n          [-0.5706,  2.3137,  0.1403,  ...,  6.0864, -1.6996, -3.1191],\n          [-0.3841,  1.4495,  0.1134,  ...,  6.0118, -1.1447, -3.4502]],\n\n         [[-3.4029,  0.6376, -0.0636,  ..., -4.6066,  3.7421, -2.5983],\n          [-2.9560,  2.3548, -0.4435,  ..., -4.0359,  3.9210, -1.8779],\n          [-3.6023,  4.2723, -1.2723,  ..., -4.1815,  3.5535, -3.7748],\n          ...,\n          [-2.7353,  2.5404, -0.5887,  ..., -3.9878,  3.7022, -2.1981],\n          [-3.4296,  3.2185, -0.5339,  ..., -3.6791,  3.2967, -2.0266],\n          [-3.1990,  2.1564, -0.0470,  ..., -3.8709,  3.8872, -2.0091]]]],\n       grad_fn=<CloneBackward0>), tensor([[[[ 0.0745, -2.7584, -1.3273,  ..., -0.5261,  4.0040,  0.5549],\n          [-0.0962, -2.7347, -1.4731,  ..., -0.6687,  3.9331, -0.2595],\n          [-0.3774, -2.4253, -2.7357,  ..., -1.4677,  3.1235, -0.6391],\n          ...,\n          [-0.2996, -2.8950, -2.0778,  ..., -0.7198,  3.7947, -0.4750],\n          [-0.3582, -2.6699, -1.7858,  ..., -1.2796,  3.7712, -0.4901],\n          [-0.0570, -2.7260, -1.7261,  ..., -0.7853,  3.8005, -0.4532]],\n\n         [[-0.8209,  0.8190,  2.4241,  ...,  1.2681,  1.0848, -0.4877],\n          [-0.6229,  1.0537,  2.9576,  ...,  1.5762,  0.5303, -0.1553],\n          [-0.5754,  0.5536,  3.1803,  ...,  2.0632,  0.6178, -0.0838],\n          ...,\n          [-0.6185,  0.7690,  3.2833,  ...,  1.8869,  0.4304,  0.0423],\n          [-0.6910,  0.9103,  3.0373,  ...,  1.5244,  0.5384, -0.2547],\n          [-0.8495,  1.0210,  2.8759,  ...,  1.6388,  0.5347, -0.2508]],\n\n         [[-1.5851,  0.7120,  0.0131,  ..., -2.8437, -2.2291,  1.6893],\n          [-2.0122, -0.4423, -0.5738,  ..., -2.9060, -2.5531,  1.3133],\n          [-2.4019,  0.4847, -1.0534,  ..., -2.9510, -2.0277,  0.3032],\n          ...,\n          [-2.6185, -0.4515, -0.4967,  ..., -2.7749, -1.9973,  0.7919],\n          [-2.2429, -0.0387, -1.1590,  ..., -3.3986, -2.0662,  1.8583],\n          [-2.1384,  0.0719, -0.3783,  ..., -3.1049, -2.3966,  1.5174]],\n\n         ...,\n\n         [[-2.6589,  0.8461,  2.2652,  ...,  0.7008, -2.5610,  3.9423],\n          [-3.1859,  1.8643,  3.2741,  ...,  0.7558, -2.0633,  4.1071],\n          [-2.2371,  2.6151,  2.6921,  ...,  0.9300, -1.1891,  2.8423],\n          ...,\n          [-3.0878,  2.0847,  3.2264,  ...,  1.0335, -2.1063,  3.5472],\n          [-3.4087,  1.8404,  3.1747,  ...,  0.5001, -1.3068,  3.8167],\n          [-3.3017,  1.5502,  2.9722,  ...,  0.8610, -2.0710,  3.7147]],\n\n         [[-2.5164, -1.6480, -1.8782,  ...,  0.1658, -4.4736, -2.5476],\n          [-2.3513, -1.5436, -1.8847,  ...,  0.1309, -5.0383, -2.2041],\n          [-2.4487, -0.9673, -1.7706,  ...,  0.9459, -4.3710, -1.4229],\n          ...,\n          [-2.5256, -1.5732, -2.1109,  ...,  0.7973, -4.8172, -1.4860],\n          [-2.1904, -1.9039, -1.8618,  ...,  0.3812, -4.7666, -1.7927],\n          [-2.3627, -1.7450, -1.9964,  ...,  0.6523, -4.8089, -2.1887]],\n\n         [[ 3.7206, -2.3456, -3.4419,  ..., -2.8878, -0.6846, -0.6617],\n          [ 3.8686, -2.0303, -2.7275,  ..., -2.9216, -0.8424, -0.8249],\n          [ 3.4866, -1.2774, -1.6394,  ..., -2.5144, -0.5752, -0.6173],\n          ...,\n          [ 4.3479, -1.7199, -2.4363,  ..., -2.9848, -0.7367, -1.1084],\n          [ 3.9688, -1.9872, -2.4876,  ..., -3.0106, -0.3939, -0.5837],\n          [ 4.4819, -2.0051, -2.6715,  ..., -2.9046, -0.7348, -0.9702]]]],\n       grad_fn=<CloneBackward0>)), (tensor([[[[-1.4667, -0.5441, -0.3662,  ..., -0.9884, -3.1186, -0.7739],\n          [-1.4628, -0.5298, -0.3909,  ..., -0.9797, -3.0989, -0.7924],\n          [-1.4351, -0.6261, -0.2056,  ..., -1.0314, -3.2885, -0.7868],\n          ...,\n          [-1.5037, -0.6244,  0.1094,  ..., -1.2221, -3.6310, -0.8902],\n          [-1.5364, -0.6467, -0.0620,  ..., -1.1410, -3.3752, -0.8870],\n          [-1.3242, -0.6661,  0.4388,  ..., -1.3230, -3.7079, -0.8164]],\n\n         [[-0.2148, -1.2839, -3.6123,  ..., -0.1958,  3.2742, -0.3911],\n          [-0.1983, -1.2841, -3.5940,  ..., -0.1632,  3.2700, -0.4216],\n          [-0.2248, -1.2929, -3.5892,  ..., -0.2679,  3.2048, -0.3775],\n          ...,\n          [-0.3888, -1.3737, -3.5805,  ..., -0.4568,  3.1265, -0.2708],\n          [-0.2517, -1.3932, -3.5644,  ..., -0.3216,  3.1467, -0.4390],\n          [-0.5075, -1.4560, -3.4383,  ..., -0.6197,  3.1471, -0.1071]],\n\n         [[ 2.0863,  1.0364, -3.6744,  ..., -1.4164,  1.1496,  0.4044],\n          [ 2.1055,  1.0693, -3.6833,  ..., -1.4062,  1.1440,  0.4046],\n          [ 2.0516,  0.9978, -3.5709,  ..., -1.5383,  1.1038,  0.3594],\n          ...,\n          [ 1.8947,  0.8699, -3.4407,  ..., -1.8880,  0.8959,  0.3292],\n          [ 1.9794,  0.9973, -3.5232,  ..., -1.6553,  0.9921,  0.3824],\n          [ 1.8390,  0.8918, -3.2728,  ..., -2.2157,  0.7971,  0.3445]],\n\n         ...,\n\n         [[-0.6540, -0.6644, -1.6449,  ..., -0.8539, -3.3097,  2.5165],\n          [-0.6790, -0.6361, -1.6246,  ..., -0.8556, -3.3152,  2.4957],\n          [-0.6954, -0.6339, -1.7549,  ..., -0.7466, -3.3019,  2.6241],\n          ...,\n          [-0.6407, -0.7587, -2.0290,  ..., -0.5423, -3.2500,  2.9311],\n          [-0.7155, -0.6281, -1.8204,  ..., -0.6815, -3.2761,  2.7639],\n          [-0.6984, -0.9675, -2.2273,  ..., -0.4039, -3.3310,  3.1441]],\n\n         [[ 0.1159, -2.6759, -0.3958,  ...,  2.2619,  0.0916, -1.4284],\n          [ 0.1123, -2.6812, -0.3864,  ...,  2.2474,  0.0787, -1.4161],\n          [ 0.3073, -2.7638, -0.3733,  ...,  2.3275,  0.0484, -1.5077],\n          ...,\n          [ 0.6002, -3.0293, -0.3306,  ...,  2.3891, -0.1464, -1.7203],\n          [ 0.3884, -2.9227, -0.3636,  ...,  2.2331,  0.0407, -1.6926],\n          [ 0.8694, -3.0503, -0.3013,  ...,  2.4423, -0.3641, -1.8553]],\n\n         [[-0.4528, -0.1736,  0.0773,  ..., -0.0661,  0.5712, -1.6037],\n          [-0.4671, -0.1514,  0.0644,  ..., -0.0754,  0.5918, -1.5900],\n          [-0.3978, -0.2859,  0.2052,  ...,  0.0261,  0.4122, -1.6731],\n          ...,\n          [-0.2833, -0.6342,  0.5296,  ...,  0.1004,  0.0777, -1.9321],\n          [-0.4233, -0.4818,  0.2782,  ...,  0.0715,  0.2813, -1.8412],\n          [-0.1598, -0.8695,  0.7335,  ...,  0.1621, -0.0967, -1.9798]]]],\n       grad_fn=<CloneBackward0>), tensor([[[[-0.9888, -1.0416, -1.0006,  ..., -0.0515,  0.5009, -0.6250],\n          [-0.9912, -1.0410, -0.9979,  ..., -0.0631,  0.5059, -0.6319],\n          [-0.9397, -1.0139, -0.9916,  ..., -0.0181,  0.4944, -0.5705],\n          ...,\n          [-0.8391, -1.0060, -0.8899,  ...,  0.1227,  0.4310, -0.4110],\n          [-0.8944, -1.0265, -0.9032,  ...,  0.0450,  0.4771, -0.5055],\n          [-0.7929, -0.9412, -0.8002,  ...,  0.2075,  0.4042, -0.2939]],\n\n         [[-0.1010,  0.5509, -0.1586,  ..., -0.0881,  0.9206,  0.4333],\n          [-0.1105,  0.5475, -0.1605,  ..., -0.0961,  0.9228,  0.4326],\n          [-0.0538,  0.5546, -0.1375,  ..., -0.0734,  0.9095,  0.4201],\n          ...,\n          [ 0.0308,  0.5276, -0.1126,  ...,  0.0066,  0.8646,  0.3941],\n          [-0.0588,  0.5454, -0.1688,  ..., -0.0746,  0.8881,  0.3783],\n          [ 0.0804,  0.5169, -0.0645,  ...,  0.0850,  0.8599,  0.3626]],\n\n         [[-0.2307,  0.2029,  0.7293,  ..., -0.5098,  0.5228, -0.3108],\n          [-0.2275,  0.1981,  0.7268,  ..., -0.5095,  0.5306, -0.3068],\n          [-0.2213,  0.1920,  0.7298,  ..., -0.5124,  0.4803, -0.3036],\n          ...,\n          [-0.1875,  0.2057,  0.7254,  ..., -0.5847,  0.3837, -0.3188],\n          [-0.1621,  0.2102,  0.7326,  ..., -0.5304,  0.4534, -0.3226],\n          [-0.1692,  0.1603,  0.6982,  ..., -0.5895,  0.3107, -0.3206]],\n\n         ...,\n\n         [[-0.0360, -0.2929,  0.1744,  ...,  0.5369,  0.4739,  0.0348],\n          [-0.0355, -0.2929,  0.1780,  ...,  0.5452,  0.4787,  0.0377],\n          [-0.0599, -0.2853,  0.1727,  ...,  0.5024,  0.4533,  0.0370],\n          ...,\n          [-0.0485, -0.3166,  0.1381,  ...,  0.4132,  0.3446,  0.0610],\n          [-0.0452, -0.3131,  0.1487,  ...,  0.4984,  0.4018,  0.0631],\n          [-0.0141, -0.2760,  0.1036,  ...,  0.3544,  0.3265,  0.0882]],\n\n         [[ 0.5549,  0.0837, -0.1720,  ..., -0.1541, -0.1031,  0.4503],\n          [ 0.5611,  0.0805, -0.1728,  ..., -0.1525, -0.1036,  0.4452],\n          [ 0.5281,  0.0860, -0.1479,  ..., -0.1853, -0.0938,  0.4863],\n          ...,\n          [ 0.4431,  0.1019, -0.1528,  ..., -0.2308, -0.0457,  0.4690],\n          [ 0.4926,  0.0866, -0.1794,  ..., -0.1923, -0.0685,  0.4582],\n          [ 0.3972,  0.0840, -0.1457,  ..., -0.2436, -0.0170,  0.4688]],\n\n         [[ 0.4700,  0.2570,  0.2926,  ..., -0.5128, -0.2847,  0.7790],\n          [ 0.4619,  0.2636,  0.2878,  ..., -0.5149, -0.2972,  0.7838],\n          [ 0.5206,  0.2638,  0.2466,  ..., -0.4718, -0.2594,  0.7618],\n          ...,\n          [ 0.5845,  0.2391,  0.2044,  ..., -0.4849, -0.1141,  0.6821],\n          [ 0.5214,  0.2381,  0.2337,  ..., -0.5071, -0.2470,  0.7379],\n          [ 0.6105,  0.2434,  0.2136,  ..., -0.4665, -0.0219,  0.6777]]]],\n       grad_fn=<CloneBackward0>), tensor([[[[-3.8448e-01,  3.4352e+00, -1.2427e+00,  ..., -2.3331e+00,\n            5.1640e+00, -5.4486e+00],\n          [-7.2872e-01,  3.5957e+00, -4.2670e+00,  ..., -1.4889e+00,\n            4.1699e+00, -5.0731e+00],\n          [-4.3636e-01,  2.7044e+00, -5.5141e+00,  ..., -1.1042e+00,\n            3.3097e+00, -5.2065e+00],\n          ...,\n          [-2.8896e-01,  3.5976e+00, -5.2030e+00,  ..., -1.0707e+00,\n            4.2670e+00, -4.9900e+00],\n          [-7.3552e-01,  3.7136e+00, -4.9905e+00,  ..., -1.1664e+00,\n            3.6460e+00, -4.8956e+00],\n          [-1.9159e-03,  3.6305e+00, -3.7863e+00,  ..., -1.4483e+00,\n            4.6666e+00, -4.9635e+00]],\n\n         [[-1.3292e+00, -5.3709e+00,  7.8350e-01,  ...,  3.2609e-01,\n           -2.4837e+00, -2.6321e+00],\n          [ 4.1322e-01, -5.4960e+00, -1.5066e+00,  ...,  7.7165e-01,\n           -3.6122e+00, -2.5722e+00],\n          [-2.5354e-02, -5.5214e+00, -2.3710e+00,  ...,  3.8599e-01,\n           -5.4250e+00, -2.7620e+00],\n          ...,\n          [ 3.7518e-01, -5.4453e+00, -1.8798e+00,  ...,  4.7970e-01,\n           -3.9249e+00, -2.2185e+00],\n          [ 6.2890e-02, -6.0785e+00, -1.9240e+00,  ...,  6.4333e-01,\n           -3.4739e+00, -2.3515e+00],\n          [ 3.4998e-01, -5.6333e+00, -8.9595e-01,  ...,  1.9876e-01,\n           -3.1707e+00, -1.9280e+00]],\n\n         [[ 2.1945e+00, -1.8339e+00,  1.2866e+00,  ...,  1.6758e+00,\n            1.7600e+00, -6.0620e+00],\n          [ 1.8281e+00, -2.8445e+00,  1.5710e+00,  ...,  1.6108e+00,\n            1.8509e+00, -5.4965e+00],\n          [ 2.4123e+00, -3.0193e+00,  2.0000e+00,  ...,  9.1674e-01,\n            2.5319e+00, -4.8117e+00],\n          ...,\n          [ 1.8696e+00, -2.6469e+00,  2.0882e+00,  ...,  1.3709e+00,\n            1.8717e+00, -5.3241e+00],\n          [ 1.8065e+00, -2.8435e+00,  1.5977e+00,  ...,  1.6243e+00,\n            1.7744e+00, -4.8714e+00],\n          [ 1.9398e+00, -2.3473e+00,  1.7154e+00,  ...,  1.8640e+00,\n            1.5628e+00, -5.5754e+00]],\n\n         ...,\n\n         [[-1.6257e+00,  2.6144e+00,  9.4049e-01,  ..., -7.5607e-01,\n            8.2380e+00,  6.8720e-01],\n          [-3.0290e+00,  3.4405e+00,  7.3795e-01,  ..., -9.7000e-01,\n            8.7800e+00, -3.1979e-01],\n          [-3.5444e+00,  3.5573e+00, -2.6256e-01,  ..., -2.2309e+00,\n            8.7654e+00, -1.4923e+00],\n          ...,\n          [-3.3287e+00,  2.8946e+00,  4.7874e-01,  ..., -5.2180e-01,\n            9.6076e+00, -2.7617e-01],\n          [-3.3774e+00,  3.8357e+00,  2.9921e-01,  ..., -1.3183e+00,\n            8.7531e+00, -5.0884e-01],\n          [-2.4687e+00,  2.8434e+00,  8.6180e-01,  ..., -8.6155e-01,\n            8.9370e+00,  4.5611e-01]],\n\n         [[-3.1061e+00, -2.3447e+00,  2.4874e+00,  ..., -3.5172e+00,\n            1.9294e+00,  3.7432e+00],\n          [-2.9232e+00, -3.7905e+00,  4.7958e+00,  ..., -3.9753e+00,\n            2.3242e+00,  2.8463e+00],\n          [-4.9896e+00, -2.1370e+00,  5.7075e+00,  ..., -4.5237e+00,\n            2.2541e-01,  4.0232e+00],\n          ...,\n          [-3.1892e+00, -3.4374e+00,  5.3840e+00,  ..., -4.3058e+00,\n            1.9689e+00,  3.4539e+00],\n          [-3.2944e+00, -3.6964e+00,  4.9284e+00,  ..., -3.5904e+00,\n            1.9743e+00,  2.6568e+00],\n          [-2.7801e+00, -3.3695e+00,  4.2636e+00,  ..., -4.2074e+00,\n            2.2513e+00,  3.1852e+00]],\n\n         [[ 3.6963e+00, -1.2497e+00,  1.3067e-01,  ..., -1.8488e+00,\n            6.0877e+00, -6.2392e+00],\n          [ 4.0977e+00, -1.5472e-01,  1.9597e+00,  ..., -1.6815e+00,\n            4.9387e+00, -3.9607e+00],\n          [ 2.3230e+00, -5.0374e-01,  4.5447e+00,  ..., -1.6144e-01,\n            2.7536e+00, -2.6896e+00],\n          ...,\n          [ 3.7481e+00, -3.2671e-01,  3.0299e+00,  ..., -8.5749e-01,\n            4.1676e+00, -3.2316e+00],\n          [ 4.1301e+00, -1.2624e-01,  2.3170e+00,  ..., -1.7321e+00,\n            5.0545e+00, -3.6419e+00],\n          [ 4.1491e+00, -3.0486e-01,  2.3372e+00,  ..., -1.4497e+00,\n            4.9032e+00, -4.6367e+00]]]], grad_fn=<CloneBackward0>), tensor([[[[ 1.4861, -0.2636,  0.9058,  ...,  1.4924, -0.9001,  1.7921],\n          [ 1.9527, -0.0581,  0.6048,  ...,  2.0689, -0.6989,  0.7655],\n          [ 1.2093, -1.5951,  0.9445,  ...,  2.1992, -2.1134,  0.8289],\n          ...,\n          [ 1.8328, -0.8372,  0.6543,  ...,  2.3749, -1.0366,  1.3694],\n          [ 2.1678, -0.5485,  0.7258,  ...,  2.1696, -1.1482,  0.7575],\n          [ 1.7857, -0.5396,  0.5297,  ...,  2.2039, -0.8527,  1.3675]],\n\n         [[-1.8925, -0.6272,  1.3088,  ..., -1.0754,  4.8387,  0.4032],\n          [-2.2872, -2.1821,  0.4953,  ..., -1.1631,  4.0240, -0.1740],\n          [-2.2240, -1.7082,  0.2950,  ..., -0.0563,  2.5375, -0.0157],\n          ...,\n          [-1.7424, -2.0492,  0.2747,  ..., -1.1345,  3.8244,  0.4449],\n          [-1.5436, -2.2985,  0.7753,  ..., -0.8763,  4.0842, -0.2897],\n          [-2.0682, -1.6799,  0.4728,  ..., -0.8341,  4.3833,  0.2350]],\n\n         [[ 2.5514, -2.0676,  2.3697,  ..., -3.3129,  4.4216, -2.4495],\n          [ 3.3055, -1.6854,  2.7545,  ..., -3.7794,  3.8927, -4.0874],\n          [ 3.0151, -2.5780,  2.2643,  ..., -3.1698,  3.3283, -1.4176],\n          ...,\n          [ 3.4374, -1.7447,  2.2427,  ..., -4.0343,  3.5666, -3.6739],\n          [ 3.1236, -1.5881,  2.5720,  ..., -3.8338,  3.9118, -3.6006],\n          [ 3.0804, -1.7444,  2.8363,  ..., -3.7099,  3.9974, -3.2787]],\n\n         ...,\n\n         [[-1.6705, -0.6002, -3.5061,  ..., -0.2269,  1.1707,  3.7516],\n          [-1.1120,  0.0799, -1.5060,  ...,  0.1906,  0.5280,  3.2965],\n          [-0.5409,  0.6015, -0.1903,  ...,  1.2090,  0.3505,  2.3485],\n          ...,\n          [-0.9916,  0.2694, -0.7434,  ...,  0.8825, -0.0289,  3.4676],\n          [-0.7674, -0.1204, -0.7180,  ...,  0.8092,  0.4381,  3.2651],\n          [-0.6967,  0.0983, -1.7015,  ...,  0.1835,  0.7681,  3.4264]],\n\n         [[ 0.2984,  1.0826,  1.8276,  ..., -2.3140, -1.6257, -1.5395],\n          [ 0.7983,  2.7205,  1.2403,  ..., -2.8160, -1.7874, -3.7248],\n          [ 1.0353,  1.1895,  0.0697,  ..., -3.0467, -1.4370, -3.7373],\n          ...,\n          [ 0.8121,  2.2122,  0.8655,  ..., -2.9950, -2.5827, -3.6309],\n          [ 0.8792,  2.5149,  0.7376,  ..., -3.0332, -1.6852, -3.4720],\n          [ 0.9266,  2.1798,  1.2130,  ..., -3.1165, -2.4062, -3.1693]],\n\n         [[ 2.3421,  1.6853, -0.0202,  ...,  0.4813, -0.2755, -3.9764],\n          [ 3.0720,  2.0012, -0.7651,  ..., -0.3548, -0.5009, -3.5038],\n          [ 2.6288,  1.9696, -0.5569,  ..., -0.6820,  0.1455, -2.4814],\n          ...,\n          [ 3.5908,  2.1170, -0.5887,  ..., -0.7801, -0.5945, -3.2344],\n          [ 3.4725,  2.1450, -0.8075,  ..., -0.7134, -0.5223, -3.1390],\n          [ 3.1921,  1.9664, -0.5445,  ..., -0.4279, -0.2777, -3.4642]]]],\n       grad_fn=<CloneBackward0>))), decoder_hidden_states=None, decoder_attentions=None, cross_attentions=None, encoder_last_hidden_state=tensor([[[-0.1927,  0.1325, -0.0935,  ...,  0.1336,  0.1667, -0.2281],\n         [-0.2560, -0.0443, -0.2410,  ...,  0.2294,  0.2373, -0.1805],\n         [-0.3796,  0.1099, -0.0796,  ...,  0.2590,  0.0720, -0.0265],\n         ...,\n         [-0.2294, -0.0381,  0.0099,  ...,  0.2866,  0.0706, -0.2316],\n         [-0.1166, -0.3544, -0.2962,  ...,  0.1777,  0.3711, -0.1312],\n         [-0.2424,  0.1021, -0.1326,  ...,  0.2657,  0.1282, -0.2727]]],\n       grad_fn=<NativeLayerNormBackward0>), encoder_hidden_states=None, encoder_attentions=None)"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = bart_large_tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "outputs = bart_large_model(**inputs)\n",
    "outputs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### donut-large"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "donut_large_config = VisionEncoderDecoderConfig.from_encoder_decoder_configs(\n",
    "    donut_base_model.encoder.config, bart_large_model.decoder.config\n",
    ")\n",
    "donut_large_model = VisionEncoderDecoderModel(\n",
    "    donut_large_config, donut_base_model.encoder, bart_large_model.decoder\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "(VisionEncoderDecoderModel(\n   (encoder): DonutSwinModel(\n     (embeddings): DonutSwinEmbeddings(\n       (patch_embeddings): DonutSwinPatchEmbeddings(\n         (projection): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))\n       )\n       (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n       (dropout): Dropout(p=0.0, inplace=False)\n     )\n     (encoder): DonutSwinEncoder(\n       (layers): ModuleList(\n         (0): DonutSwinStage(\n           (blocks): ModuleList(\n             (0-1): 2 x DonutSwinLayer(\n               (layernorm_before): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n               (attention): DonutSwinAttention(\n                 (self): DonutSwinSelfAttention(\n                   (query): Linear(in_features=128, out_features=128, bias=True)\n                   (key): Linear(in_features=128, out_features=128, bias=True)\n                   (value): Linear(in_features=128, out_features=128, bias=True)\n                   (dropout): Dropout(p=0.0, inplace=False)\n                 )\n                 (output): DonutSwinSelfOutput(\n                   (dense): Linear(in_features=128, out_features=128, bias=True)\n                   (dropout): Dropout(p=0.0, inplace=False)\n                 )\n               )\n               (drop_path): DonutSwinDropPath(p=0.1)\n               (layernorm_after): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n               (intermediate): DonutSwinIntermediate(\n                 (dense): Linear(in_features=128, out_features=512, bias=True)\n                 (intermediate_act_fn): GELUActivation()\n               )\n               (output): DonutSwinOutput(\n                 (dense): Linear(in_features=512, out_features=128, bias=True)\n                 (dropout): Dropout(p=0.0, inplace=False)\n               )\n             )\n           )\n           (downsample): DonutSwinPatchMerging(\n             (reduction): Linear(in_features=512, out_features=256, bias=False)\n             (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n           )\n         )\n         (1): DonutSwinStage(\n           (blocks): ModuleList(\n             (0-1): 2 x DonutSwinLayer(\n               (layernorm_before): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n               (attention): DonutSwinAttention(\n                 (self): DonutSwinSelfAttention(\n                   (query): Linear(in_features=256, out_features=256, bias=True)\n                   (key): Linear(in_features=256, out_features=256, bias=True)\n                   (value): Linear(in_features=256, out_features=256, bias=True)\n                   (dropout): Dropout(p=0.0, inplace=False)\n                 )\n                 (output): DonutSwinSelfOutput(\n                   (dense): Linear(in_features=256, out_features=256, bias=True)\n                   (dropout): Dropout(p=0.0, inplace=False)\n                 )\n               )\n               (drop_path): DonutSwinDropPath(p=0.1)\n               (layernorm_after): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n               (intermediate): DonutSwinIntermediate(\n                 (dense): Linear(in_features=256, out_features=1024, bias=True)\n                 (intermediate_act_fn): GELUActivation()\n               )\n               (output): DonutSwinOutput(\n                 (dense): Linear(in_features=1024, out_features=256, bias=True)\n                 (dropout): Dropout(p=0.0, inplace=False)\n               )\n             )\n           )\n           (downsample): DonutSwinPatchMerging(\n             (reduction): Linear(in_features=1024, out_features=512, bias=False)\n             (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n           )\n         )\n         (2): DonutSwinStage(\n           (blocks): ModuleList(\n             (0-13): 14 x DonutSwinLayer(\n               (layernorm_before): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n               (attention): DonutSwinAttention(\n                 (self): DonutSwinSelfAttention(\n                   (query): Linear(in_features=512, out_features=512, bias=True)\n                   (key): Linear(in_features=512, out_features=512, bias=True)\n                   (value): Linear(in_features=512, out_features=512, bias=True)\n                   (dropout): Dropout(p=0.0, inplace=False)\n                 )\n                 (output): DonutSwinSelfOutput(\n                   (dense): Linear(in_features=512, out_features=512, bias=True)\n                   (dropout): Dropout(p=0.0, inplace=False)\n                 )\n               )\n               (drop_path): DonutSwinDropPath(p=0.1)\n               (layernorm_after): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n               (intermediate): DonutSwinIntermediate(\n                 (dense): Linear(in_features=512, out_features=2048, bias=True)\n                 (intermediate_act_fn): GELUActivation()\n               )\n               (output): DonutSwinOutput(\n                 (dense): Linear(in_features=2048, out_features=512, bias=True)\n                 (dropout): Dropout(p=0.0, inplace=False)\n               )\n             )\n           )\n           (downsample): DonutSwinPatchMerging(\n             (reduction): Linear(in_features=2048, out_features=1024, bias=False)\n             (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n           )\n         )\n         (3): DonutSwinStage(\n           (blocks): ModuleList(\n             (0-1): 2 x DonutSwinLayer(\n               (layernorm_before): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n               (attention): DonutSwinAttention(\n                 (self): DonutSwinSelfAttention(\n                   (query): Linear(in_features=1024, out_features=1024, bias=True)\n                   (key): Linear(in_features=1024, out_features=1024, bias=True)\n                   (value): Linear(in_features=1024, out_features=1024, bias=True)\n                   (dropout): Dropout(p=0.0, inplace=False)\n                 )\n                 (output): DonutSwinSelfOutput(\n                   (dense): Linear(in_features=1024, out_features=1024, bias=True)\n                   (dropout): Dropout(p=0.0, inplace=False)\n                 )\n               )\n               (drop_path): DonutSwinDropPath(p=0.1)\n               (layernorm_after): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n               (intermediate): DonutSwinIntermediate(\n                 (dense): Linear(in_features=1024, out_features=4096, bias=True)\n                 (intermediate_act_fn): GELUActivation()\n               )\n               (output): DonutSwinOutput(\n                 (dense): Linear(in_features=4096, out_features=1024, bias=True)\n                 (dropout): Dropout(p=0.0, inplace=False)\n               )\n             )\n           )\n         )\n       )\n     )\n     (pooler): AdaptiveAvgPool1d(output_size=1)\n   )\n   (decoder): MBartDecoder(\n     (embed_tokens): Embedding(50265, 1024, padding_idx=1)\n     (embed_positions): MBartLearnedPositionalEmbedding(1026, 1024)\n     (layers): ModuleList(\n       (0-11): 12 x MBartDecoderLayer(\n         (self_attn): MBartAttention(\n           (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n           (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n           (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n           (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n         )\n         (activation_fn): GELUActivation()\n         (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n         (encoder_attn): MBartAttention(\n           (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n           (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n           (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n           (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n         )\n         (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n         (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n         (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n         (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n       )\n     )\n     (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n     (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n   )\n ),\n 328266872)"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "donut_large_model_params = cal_model_parameters(donut_large_model)\n",
    "donut_large_model, donut_large_model_params"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "SAVE_MP = \"J:/model/mllm-model/donut-large\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%SAVE\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": "(1, 0)"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor = DonutProcessor.from_pretrained(MP1)\n",
    "donut_large_model.config.pad_token_id = processor.tokenizer.pad_token_id\n",
    "donut_large_model.config.decoder_start_token_id = processor.tokenizer.convert_tokens_to_ids([\"<s>\"])[0]\n",
    "\n",
    "donut_large_model.config.pad_token_id, donut_large_model.config.decoder_start_token_id"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "expand_vocab = [\"</n>\"]\n",
    "newly_added_num = processor.tokenizer.add_tokens(expand_vocab)\n",
    "\n",
    "# if newly_added_num > 0:\n",
    "donut_large_model.decoder.resize_token_embeddings(len(processor.tokenizer))\n",
    "\n",
    "max_length = 2560\n",
    "if max_length > donut_large_config.decoder.max_position_embeddings:\n",
    "    donut_large_config.decoder.max_position_embeddings = max_length\n",
    "    ignore_mismatched_sizes = True"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "donut_large_model.save_pretrained(SAVE_MP)\n",
    "processor.save_pretrained(SAVE_MP)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Config of the decoder: <class 'transformers.models.mbart.modeling_mbart.MBartForCausalLM'> is overwritten by shared decoder config: MBartConfig {\n",
      "  \"_name_or_path\": \"J:/model/facebook_bart-large\",\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_cross_attention\": true,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.1,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_decoder\": true,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 2560,\n",
      "  \"model_type\": \"mbart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 128,\n",
      "      \"min_length\": 12,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_cnn\": {\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_xsum\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 62,\n",
      "      \"min_length\": 11,\n",
      "      \"num_beams\": 6\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.34.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 58891\n",
      "}\n",
      "\n",
      "Some weights of the model checkpoint at J:/model/mllm-model/donut-large were not used when initializing VisionEncoderDecoderModel: ['decoder.layers.10.encoder_attn_layer_norm.bias', 'decoder.layers.2.self_attn_layer_norm.bias', 'decoder.layers.7.encoder_attn.k_proj.weight', 'decoder.layers.4.self_attn.v_proj.weight', 'decoder.layers.5.self_attn.v_proj.bias', 'decoder.layers.2.self_attn.out_proj.bias', 'decoder.layers.1.self_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.layers.0.fc2.bias', 'decoder.layers.11.fc1.weight', 'decoder.layers.1.final_layer_norm.weight', 'decoder.layers.3.self_attn_layer_norm.bias', 'decoder.layers.10.self_attn.out_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.3.self_attn.v_proj.bias', 'decoder.layers.0.self_attn.k_proj.weight', 'decoder.layers.3.self_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.layers.6.self_attn.k_proj.bias', 'decoder.layers.5.encoder_attn.out_proj.bias', 'decoder.layers.8.encoder_attn.q_proj.weight', 'decoder.embed_positions.weight', 'decoder.layers.0.fc2.weight', 'decoder.layers.4.encoder_attn.k_proj.bias', 'decoder.layers.11.encoder_attn.k_proj.weight', 'decoder.layers.11.encoder_attn_layer_norm.bias', 'decoder.layers.9.self_attn_layer_norm.bias', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.4.fc1.bias', 'decoder.layers.4.self_attn_layer_norm.weight', 'decoder.layers.6.self_attn.k_proj.weight', 'decoder.layers.7.self_attn.out_proj.weight', 'decoder.layers.10.encoder_attn.q_proj.bias', 'decoder.layers.8.encoder_attn.k_proj.bias', 'decoder.layers.11.fc2.bias', 'decoder.layers.5.encoder_attn.v_proj.bias', 'decoder.layers.0.encoder_attn.k_proj.bias', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.2.self_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.layers.3.self_attn.out_proj.bias', 'decoder.layers.7.fc1.bias', 'decoder.layers.10.self_attn.v_proj.bias', 'decoder.layers.0.self_attn.out_proj.bias', 'decoder.layers.5.self_attn_layer_norm.weight', 'decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.layers.9.self_attn.k_proj.bias', 'decoder.layers.7.self_attn_layer_norm.bias', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.layers.9.fc1.bias', 'decoder.layers.10.self_attn.out_proj.weight', 'decoder.layers.4.self_attn.k_proj.weight', 'decoder.layers.5.encoder_attn.k_proj.bias', 'decoder.layer_norm.weight', 'decoder.layers.5.final_layer_norm.bias', 'decoder.layers.11.encoder_attn.out_proj.bias', 'decoder.layers.3.self_attn.k_proj.bias', 'decoder.layers.7.fc1.weight', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.6.encoder_attn.q_proj.weight', 'decoder.layers.8.self_attn.k_proj.weight', 'decoder.layers.3.fc1.weight', 'decoder.layers.0.self_attn.q_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.layers.4.fc1.weight', 'decoder.layers.8.final_layer_norm.bias', 'decoder.layers.6.encoder_attn.out_proj.bias', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.8.encoder_attn.k_proj.weight', 'decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.7.self_attn.k_proj.bias', 'decoder.layers.9.encoder_attn.v_proj.bias', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.11.self_attn_layer_norm.weight', 'decoder.layers.9.self_attn.k_proj.weight', 'decoder.layers.9.self_attn.v_proj.bias', 'decoder.layers.7.encoder_attn.out_proj.bias', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.2.fc2.bias', 'decoder.layers.7.final_layer_norm.weight', 'decoder.layers.2.self_attn.v_proj.weight', 'decoder.layers.7.fc2.weight', 'decoder.layers.10.fc2.bias', 'decoder.layers.10.self_attn.k_proj.weight', 'decoder.layers.7.self_attn_layer_norm.weight', 'decoder.layers.8.self_attn.q_proj.bias', 'decoder.layers.4.fc2.bias', 'decoder.layers.10.fc1.weight', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.7.encoder_attn.k_proj.bias', 'decoder.layers.4.self_attn.q_proj.weight', 'decoder.layers.4.self_attn.v_proj.bias', 'decoder.layers.6.self_attn.out_proj.bias', 'decoder.layers.11.self_attn.q_proj.bias', 'decoder.layers.6.fc2.bias', 'decoder.layers.6.encoder_attn.v_proj.weight', 'decoder.layers.4.fc2.weight', 'decoder.layers.6.final_layer_norm.bias', 'decoder.layers.1.self_attn.out_proj.bias', 'decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.layers.6.encoder_attn.k_proj.weight', 'decoder.layers.10.encoder_attn.q_proj.weight', 'decoder.layers.8.encoder_attn.v_proj.bias', 'decoder.layers.5.self_attn.k_proj.bias', 'decoder.layers.6.encoder_attn_layer_norm.bias', 'decoder.layers.6.self_attn_layer_norm.bias', 'decoder.layers.6.self_attn_layer_norm.weight', 'decoder.layers.7.self_attn.q_proj.weight', 'decoder.layers.7.self_attn.out_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.bias', 'decoder.layers.8.fc1.weight', 'decoder.layers.10.encoder_attn_layer_norm.weight', 'decoder.layers.8.self_attn.out_proj.bias', 'decoder.layernorm_embedding.weight', 'decoder.layers.1.self_attn.v_proj.bias', 'decoder.layers.6.self_attn.q_proj.bias', 'decoder.layers.3.self_attn_layer_norm.weight', 'decoder.layers.7.self_attn.k_proj.weight', 'decoder.layers.8.fc2.weight', 'decoder.layers.7.encoder_attn.v_proj.weight', 'decoder.layers.5.fc1.bias', 'decoder.layers.5.self_attn.q_proj.weight', 'decoder.layers.9.encoder_attn.q_proj.weight', 'decoder.layers.6.encoder_attn.v_proj.bias', 'decoder.layers.7.final_layer_norm.bias', 'decoder.layers.0.self_attn_layer_norm.bias', 'decoder.embed_tokens.weight', 'decoder.layers.5.fc1.weight', 'decoder.layers.0.self_attn.v_proj.weight', 'decoder.layers.2.final_layer_norm.weight', 'decoder.layers.8.final_layer_norm.weight', 'decoder.layers.8.self_attn.v_proj.bias', 'decoder.layers.9.encoder_attn.k_proj.weight', 'decoder.layers.0.self_attn.v_proj.bias', 'decoder.layers.8.self_attn.k_proj.bias', 'decoder.layers.1.self_attn.k_proj.bias', 'decoder.layers.10.encoder_attn.k_proj.bias', 'decoder.layers.8.encoder_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.layers.11.self_attn.k_proj.weight', 'decoder.layers.6.final_layer_norm.weight', 'decoder.layers.5.self_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.9.encoder_attn.out_proj.bias', 'decoder.layers.2.self_attn.out_proj.weight', 'decoder.layers.6.fc1.bias', 'decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.layers.10.self_attn_layer_norm.bias', 'decoder.layers.2.fc2.weight', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.layernorm_embedding.bias', 'decoder.layers.11.self_attn.out_proj.bias', 'decoder.layers.5.encoder_attn.q_proj.bias', 'decoder.layers.9.encoder_attn_layer_norm.bias', 'decoder.layers.1.fc2.weight', 'decoder.layers.10.self_attn.v_proj.weight', 'decoder.layers.6.encoder_attn.k_proj.bias', 'decoder.layers.9.fc1.weight', 'decoder.layers.10.encoder_attn.k_proj.weight', 'decoder.layers.3.final_layer_norm.bias', 'decoder.layers.9.self_attn.q_proj.bias', 'decoder.layers.11.encoder_attn.v_proj.weight', 'decoder.layers.6.fc1.weight', 'decoder.layers.3.fc1.bias', 'decoder.layers.6.encoder_attn.q_proj.bias', 'decoder.layers.4.encoder_attn.out_proj.bias', 'decoder.layers.1.self_attn.out_proj.weight', 'decoder.layers.4.final_layer_norm.weight', 'decoder.layers.4.encoder_attn.q_proj.bias', 'decoder.layers.6.self_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.8.self_attn_layer_norm.bias', 'decoder.layers.8.self_attn.v_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.layers.11.self_attn.k_proj.bias', 'decoder.layers.2.final_layer_norm.bias', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.7.encoder_attn.q_proj.bias', 'decoder.layers.10.fc2.weight', 'decoder.layers.7.self_attn.v_proj.weight', 'decoder.layers.10.encoder_attn.out_proj.bias', 'decoder.layers.2.self_attn.q_proj.weight', 'decoder.layers.10.encoder_attn.v_proj.weight', 'decoder.layers.11.fc1.bias', 'decoder.layers.5.self_attn.v_proj.weight', 'decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.layers.10.fc1.bias', 'decoder.layers.4.self_attn.k_proj.bias', 'decoder.layers.11.fc2.weight', 'decoder.layers.6.self_attn.out_proj.weight', 'decoder.layers.2.self_attn.k_proj.bias', 'decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.layers.5.final_layer_norm.weight', 'decoder.layers.7.encoder_attn_layer_norm.weight', 'decoder.layers.5.fc2.weight', 'decoder.layers.2.self_attn.v_proj.bias', 'decoder.layers.11.encoder_attn.v_proj.bias', 'decoder.layers.2.fc1.weight', 'decoder.layers.9.self_attn.v_proj.weight', 'decoder.layers.3.fc2.bias', 'decoder.layers.11.encoder_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.layers.10.encoder_attn.out_proj.weight', 'decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.layers.10.encoder_attn.v_proj.bias', 'decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.layers.2.fc1.bias', 'decoder.layers.7.encoder_attn.out_proj.weight', 'decoder.layers.9.self_attn.q_proj.weight', 'decoder.layers.11.self_attn.v_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.10.self_attn_layer_norm.weight', 'decoder.layers.3.self_attn.v_proj.weight', 'decoder.layers.8.fc1.bias', 'decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.layers.5.self_attn.q_proj.bias', 'decoder.layers.6.self_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.11.encoder_attn_layer_norm.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.layers.3.self_attn.out_proj.weight', 'decoder.layers.8.self_attn_layer_norm.weight', 'decoder.layers.3.self_attn.q_proj.weight', 'decoder.layers.10.self_attn.q_proj.bias', 'decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.layers.8.self_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.6.encoder_attn.out_proj.weight', 'decoder.layers.1.fc2.bias', 'decoder.layers.4.self_attn.q_proj.bias', 'decoder.layers.10.self_attn.q_proj.weight', 'decoder.layers.11.self_attn.out_proj.weight', 'decoder.layers.1.fc1.bias', 'decoder.layers.11.encoder_attn.q_proj.bias', 'decoder.layers.5.fc2.bias', 'decoder.layers.11.self_attn.v_proj.weight', 'decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.layers.11.final_layer_norm.bias', 'decoder.layers.3.self_attn.q_proj.bias', 'decoder.layers.9.fc2.weight', 'decoder.layers.0.final_layer_norm.bias', 'decoder.layers.7.encoder_attn_layer_norm.bias', 'decoder.layers.7.self_attn.q_proj.bias', 'decoder.layers.9.self_attn.out_proj.bias', 'decoder.layers.9.encoder_attn.k_proj.bias', 'decoder.layers.6.encoder_attn_layer_norm.weight', 'decoder.layers.2.self_attn.q_proj.bias', 'decoder.layers.7.self_attn.v_proj.bias', 'decoder.layers.3.final_layer_norm.weight', 'decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.layers.0.final_layer_norm.weight', 'decoder.layer_norm.bias', 'decoder.layers.5.self_attn.k_proj.weight', 'decoder.layers.8.self_attn.q_proj.weight', 'decoder.layers.0.self_attn.out_proj.weight', 'decoder.layers.1.self_attn.k_proj.weight', 'decoder.layers.9.self_attn_layer_norm.weight', 'decoder.layers.1.self_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'decoder.layers.8.encoder_attn.out_proj.bias', 'decoder.layers.9.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.k_proj.bias', 'decoder.layers.3.encoder_attn.k_proj.bias', 'decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.layers.0.self_attn.k_proj.bias', 'decoder.layers.10.final_layer_norm.bias', 'decoder.layers.10.final_layer_norm.weight', 'decoder.layers.2.encoder_attn.k_proj.bias', 'decoder.layers.7.fc2.bias', 'decoder.layers.11.encoder_attn.k_proj.bias', 'decoder.layers.9.final_layer_norm.weight', 'decoder.layers.6.fc2.weight', 'decoder.layers.0.self_attn.q_proj.weight', 'decoder.layers.1.final_layer_norm.bias', 'decoder.layers.10.self_attn.k_proj.bias', 'decoder.layers.0.fc1.bias', 'decoder.layers.9.self_attn.out_proj.weight', 'decoder.layers.6.self_attn.v_proj.bias', 'decoder.layers.1.self_attn_layer_norm.bias', 'decoder.layers.8.fc2.bias', 'decoder.layers.4.self_attn.out_proj.bias', 'decoder.layers.4.final_layer_norm.bias', 'decoder.layers.11.self_attn.q_proj.weight', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.11.encoder_attn.out_proj.weight', 'decoder.layers.8.encoder_attn_layer_norm.bias', 'decoder.layers.4.self_attn_layer_norm.bias', 'decoder.layers.9.encoder_attn.out_proj.weight', 'decoder.layers.5.self_attn.out_proj.bias', 'decoder.layers.1.fc1.weight', 'decoder.layers.9.encoder_attn.q_proj.bias', 'decoder.layers.5.self_attn_layer_norm.bias', 'decoder.layers.11.self_attn_layer_norm.bias', 'decoder.layers.2.self_attn_layer_norm.weight', 'decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.8.encoder_attn.v_proj.weight', 'decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.layers.3.fc2.weight', 'decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.layers.9.encoder_attn_layer_norm.weight', 'decoder.layers.0.fc1.weight', 'decoder.layers.11.final_layer_norm.weight', 'decoder.layers.7.encoder_attn.v_proj.bias', 'decoder.layers.0.self_attn_layer_norm.weight', 'decoder.layers.8.encoder_attn.q_proj.bias', 'decoder.layers.4.self_attn.out_proj.weight', 'decoder.layers.1.self_attn.q_proj.bias', 'decoder.layers.1.self_attn_layer_norm.weight', 'decoder.layers.7.encoder_attn.q_proj.weight', 'decoder.layers.9.fc2.bias', 'decoder.layers.8.encoder_attn_layer_norm.weight', 'decoder.layers.9.final_layer_norm.bias']\n",
      "- This IS expected if you are initializing VisionEncoderDecoderModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing VisionEncoderDecoderModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at J:/model/mllm-model/donut-large and are newly initialized: ['decoder.model.decoder.layers.7.self_attn_layer_norm.bias', 'decoder.model.decoder.layers.9.encoder_attn.q_proj.bias', 'decoder.model.decoder.layers.0.self_attn.out_proj.bias', 'decoder.model.decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.9.self_attn.k_proj.weight', 'decoder.model.decoder.layers.1.self_attn.out_proj.bias', 'decoder.model.decoder.layers.3.encoder_attn.out_proj.bias', 'decoder.model.decoder.layers.6.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.model.decoder.layers.4.self_attn.v_proj.weight', 'decoder.model.decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.model.decoder.layers.9.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.7.fc1.weight', 'decoder.model.decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.4.final_layer_norm.weight', 'decoder.model.decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.2.fc1.bias', 'decoder.model.decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.11.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.0.self_attn.q_proj.weight', 'decoder.model.decoder.layers.9.self_attn.out_proj.weight', 'decoder.model.decoder.layers.11.self_attn.out_proj.bias', 'decoder.model.decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.10.encoder_attn.out_proj.bias', 'decoder.model.decoder.layers.7.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.5.self_attn.k_proj.weight', 'decoder.model.decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.model.decoder.layers.8.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.6.fc1.bias', 'decoder.model.decoder.layers.6.self_attn.q_proj.bias', 'decoder.model.decoder.layers.10.self_attn.out_proj.weight', 'decoder.model.decoder.layers.7.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.8.self_attn.k_proj.bias', 'decoder.model.decoder.layers.6.self_attn.k_proj.weight', 'decoder.model.decoder.layers.3.self_attn.k_proj.bias', 'decoder.model.decoder.layers.2.self_attn.q_proj.bias', 'decoder.model.decoder.layers.3.fc1.weight', 'decoder.model.decoder.layers.1.final_layer_norm.weight', 'decoder.model.decoder.layers.4.self_attn_layer_norm.weight', 'decoder.model.decoder.layers.11.self_attn.v_proj.weight', 'decoder.model.decoder.layers.9.self_attn.q_proj.weight', 'decoder.model.decoder.layers.9.self_attn_layer_norm.weight', 'decoder.model.decoder.layers.5.self_attn_layer_norm.bias', 'decoder.model.decoder.layers.4.self_attn.k_proj.bias', 'decoder.model.decoder.layers.10.self_attn.k_proj.bias', 'decoder.model.decoder.layer_norm.bias', 'decoder.model.decoder.layers.3.encoder_attn.k_proj.bias', 'decoder.model.decoder.layers.4.self_attn_layer_norm.bias', 'decoder.model.decoder.layers.7.self_attn_layer_norm.weight', 'decoder.model.decoder.layers.9.encoder_attn.v_proj.bias', 'decoder.model.decoder.layers.8.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.8.self_attn.out_proj.weight', 'decoder.model.decoder.layers.5.self_attn_layer_norm.weight', 'decoder.model.decoder.layers.5.fc2.bias', 'decoder.model.decoder.layers.11.encoder_attn.v_proj.bias', 'decoder.model.decoder.layers.1.self_attn.k_proj.weight', 'decoder.model.decoder.layers.1.self_attn_layer_norm.bias', 'decoder.model.decoder.layers.7.final_layer_norm.weight', 'decoder.model.decoder.layers.5.fc1.weight', 'decoder.model.decoder.layers.11.self_attn.v_proj.bias', 'decoder.model.decoder.layers.10.self_attn_layer_norm.weight', 'decoder.model.decoder.layers.3.self_attn.k_proj.weight', 'decoder.model.decoder.layers.10.self_attn.k_proj.weight', 'decoder.model.decoder.layers.6.encoder_attn.out_proj.bias', 'decoder.model.decoder.layers.6.self_attn.k_proj.bias', 'decoder.model.decoder.layers.0.self_attn.k_proj.weight', 'decoder.model.decoder.layers.3.fc2.weight', 'decoder.model.decoder.layers.3.self_attn_layer_norm.weight', 'decoder.model.decoder.layers.1.self_attn_layer_norm.weight', 'decoder.model.decoder.layers.9.fc2.weight', 'decoder.model.decoder.layers.9.self_attn.v_proj.weight', 'decoder.model.decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.7.self_attn.out_proj.bias', 'decoder.model.decoder.layers.10.encoder_attn_layer_norm.bias', 'decoder.model.decoder.embed_tokens.weight', 'decoder.model.decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layer_norm.weight', 'decoder.model.decoder.layers.1.fc2.weight', 'decoder.model.decoder.layers.3.self_attn.v_proj.weight', 'decoder.model.decoder.layers.4.self_attn.out_proj.bias', 'decoder.model.decoder.layers.1.final_layer_norm.bias', 'decoder.model.decoder.layers.5.encoder_attn.v_proj.bias', 'decoder.model.decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.8.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.5.self_attn.k_proj.bias', 'decoder.model.decoder.layers.3.self_attn.out_proj.bias', 'decoder.model.decoder.layers.11.encoder_attn.k_proj.bias', 'decoder.model.decoder.layers.3.final_layer_norm.bias', 'decoder.model.decoder.layers.2.fc2.bias', 'decoder.model.decoder.layers.2.self_attn.k_proj.bias', 'decoder.model.decoder.layers.7.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.2.self_attn.v_proj.weight', 'decoder.model.decoder.layers.6.final_layer_norm.bias', 'decoder.model.decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.9.encoder_attn.out_proj.bias', 'decoder.model.decoder.layers.3.self_attn.out_proj.weight', 'decoder.model.decoder.layers.4.self_attn.q_proj.weight', 'decoder.model.decoder.layers.10.encoder_attn.q_proj.bias', 'decoder.model.decoder.layers.7.final_layer_norm.bias', 'decoder.model.decoder.layers.9.self_attn.v_proj.bias', 'decoder.model.decoder.layers.11.encoder_attn.v_proj.weight', 'decoder.model.decoder.layernorm_embedding.weight', 'decoder.model.decoder.layers.6.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.8.fc2.weight', 'decoder.model.decoder.layers.6.fc2.weight', 'decoder.model.decoder.layers.9.self_attn_layer_norm.bias', 'decoder.model.decoder.layers.9.self_attn.k_proj.bias', 'decoder.model.decoder.layers.4.fc2.weight', 'decoder.model.decoder.layers.4.fc2.bias', 'decoder.model.decoder.layers.8.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.4.encoder_attn.k_proj.bias', 'decoder.model.decoder.layers.7.self_attn.q_proj.weight', 'decoder.model.decoder.layers.9.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.1.self_attn.q_proj.bias', 'decoder.model.decoder.layers.1.encoder_attn.k_proj.bias', 'decoder.model.decoder.layers.2.self_attn.k_proj.weight', 'decoder.model.decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.10.fc2.bias', 'decoder.model.decoder.layers.8.self_attn.out_proj.bias', 'decoder.model.decoder.layers.1.self_attn.out_proj.weight', 'decoder.model.decoder.layers.10.self_attn.out_proj.bias', 'decoder.model.decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.model.decoder.layers.0.self_attn_layer_norm.weight', 'decoder.model.decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.model.decoder.layers.9.encoder_attn_layer_norm.weight', 'decoder.model.decoder.embed_positions.weight', 'decoder.model.decoder.layers.6.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.6.fc1.weight', 'decoder.model.decoder.layers.5.final_layer_norm.bias', 'decoder.model.decoder.layers.5.encoder_attn.k_proj.bias', 'decoder.model.decoder.layers.8.self_attn.v_proj.weight', 'decoder.model.decoder.layers.6.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.model.decoder.layers.8.self_attn.v_proj.bias', 'decoder.model.decoder.layers.2.self_attn.out_proj.bias', 'decoder.model.decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.10.encoder_attn.v_proj.bias', 'decoder.model.decoder.layers.11.final_layer_norm.bias', 'decoder.model.decoder.layers.6.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.7.fc2.weight', 'decoder.model.decoder.layers.7.encoder_attn.v_proj.bias', 'decoder.model.decoder.layers.5.self_attn.q_proj.bias', 'decoder.model.decoder.layers.1.self_attn.v_proj.bias', 'decoder.model.decoder.layers.9.fc1.weight', 'decoder.model.decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.3.self_attn_layer_norm.bias', 'decoder.model.decoder.layers.6.self_attn_layer_norm.weight', 'decoder.model.decoder.layers.11.self_attn.k_proj.weight', 'decoder.model.decoder.layers.7.self_attn.v_proj.weight', 'decoder.model.decoder.layers.0.self_attn.q_proj.bias', 'decoder.model.decoder.layers.2.self_attn_layer_norm.bias', 'decoder.model.decoder.layers.6.encoder_attn.q_proj.bias', 'decoder.model.decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.3.self_attn.q_proj.weight', 'decoder.model.decoder.layers.2.fc2.weight', 'decoder.model.decoder.layers.4.self_attn.v_proj.bias', 'decoder.model.decoder.layers.6.encoder_attn.k_proj.bias', 'decoder.model.decoder.layers.0.self_attn.v_proj.weight', 'decoder.model.decoder.layers.8.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.2.self_attn_layer_norm.weight', 'decoder.model.decoder.layers.10.fc1.weight', 'decoder.model.decoder.layers.8.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.5.encoder_attn.out_proj.bias', 'decoder.model.decoder.layers.10.fc2.weight', 'decoder.model.decoder.layers.1.encoder_attn.q_proj.bias', 'decoder.model.decoder.layers.10.self_attn.v_proj.bias', 'decoder.model.decoder.layers.4.fc1.weight', 'decoder.model.decoder.layers.10.final_layer_norm.bias', 'decoder.model.decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.1.fc1.bias', 'decoder.model.decoder.layers.11.encoder_attn.out_proj.bias', 'decoder.model.decoder.layers.5.self_attn.q_proj.weight', 'decoder.model.decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.1.fc1.weight', 'decoder.model.decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.model.decoder.layers.4.self_attn.q_proj.bias', 'decoder.model.decoder.layers.1.self_attn.k_proj.bias', 'decoder.model.decoder.layers.7.fc2.bias', 'decoder.model.decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.model.decoder.layers.11.self_attn.q_proj.bias', 'decoder.model.decoder.layers.5.final_layer_norm.weight', 'decoder.model.decoder.layers.3.fc2.bias', 'decoder.model.decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.0.self_attn.out_proj.weight', 'decoder.model.decoder.layers.7.encoder_attn.q_proj.bias', 'decoder.model.decoder.layers.8.self_attn_layer_norm.bias', 'decoder.model.decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.11.self_attn_layer_norm.bias', 'decoder.lm_head.weight', 'decoder.model.decoder.layers.7.self_attn.k_proj.weight', 'decoder.model.decoder.layers.8.encoder_attn.v_proj.bias', 'decoder.model.decoder.layers.3.fc1.bias', 'decoder.model.decoder.layers.0.fc2.bias', 'decoder.model.decoder.layers.0.fc2.weight', 'decoder.model.decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.2.final_layer_norm.weight', 'decoder.model.decoder.layers.9.final_layer_norm.weight', 'decoder.model.decoder.layers.6.self_attn.out_proj.weight', 'decoder.model.decoder.layers.0.final_layer_norm.weight', 'decoder.model.decoder.layers.6.encoder_attn.v_proj.bias', 'decoder.model.decoder.layers.7.fc1.bias', 'decoder.model.decoder.layers.7.encoder_attn.out_proj.bias', 'decoder.model.decoder.layers.2.final_layer_norm.bias', 'decoder.model.decoder.layers.5.fc1.bias', 'decoder.model.decoder.layers.10.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.7.self_attn.k_proj.bias', 'decoder.model.decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.8.fc1.weight', 'decoder.model.decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.5.self_attn.v_proj.bias', 'decoder.model.decoder.layers.0.encoder_attn.k_proj.bias', 'decoder.model.decoder.layers.10.encoder_attn.k_proj.bias', 'decoder.model.decoder.layers.4.fc1.bias', 'decoder.model.decoder.layers.0.fc1.weight', 'decoder.model.decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.9.fc2.bias', 'decoder.model.decoder.layers.6.self_attn_layer_norm.bias', 'decoder.model.decoder.layers.1.self_attn.q_proj.weight', 'decoder.model.decoder.layers.1.fc2.bias', 'decoder.model.decoder.layers.5.encoder_attn.q_proj.bias', 'decoder.model.decoder.layers.1.self_attn.v_proj.weight', 'decoder.model.decoder.layers.5.self_attn.v_proj.weight', 'decoder.model.decoder.layers.0.fc1.bias', 'decoder.model.decoder.layers.0.encoder_attn.v_proj.bias', 'decoder.model.decoder.layers.7.self_attn.v_proj.bias', 'decoder.model.decoder.layers.3.self_attn.q_proj.bias', 'decoder.model.decoder.layers.4.encoder_attn.q_proj.bias', 'decoder.model.decoder.layers.6.self_attn.out_proj.bias', 'decoder.model.decoder.layers.11.self_attn.k_proj.bias', 'decoder.model.decoder.layers.8.self_attn.q_proj.weight', 'decoder.model.decoder.layers.2.encoder_attn.k_proj.bias', 'decoder.model.decoder.layers.6.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.8.encoder_attn.k_proj.bias', 'decoder.model.decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.0.self_attn.v_proj.bias', 'decoder.model.decoder.layers.8.encoder_attn.q_proj.bias', 'decoder.model.decoder.layers.7.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.4.encoder_attn.out_proj.bias', 'decoder.model.decoder.layers.11.self_attn.out_proj.weight', 'decoder.model.decoder.layers.2.fc1.weight', 'decoder.model.decoder.layers.11.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.10.self_attn.v_proj.weight', 'decoder.model.decoder.layers.11.fc2.bias', 'decoder.model.decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.9.final_layer_norm.bias', 'decoder.model.decoder.layers.5.fc2.weight', 'decoder.model.decoder.layers.10.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.11.self_attn_layer_norm.weight', 'decoder.model.decoder.layers.10.self_attn_layer_norm.bias', 'decoder.model.decoder.layers.11.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.7.self_attn.q_proj.bias', 'decoder.model.decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.3.final_layer_norm.weight', 'decoder.model.decoder.layers.11.fc1.bias', 'decoder.model.decoder.layernorm_embedding.bias', 'decoder.model.decoder.layers.6.self_attn.v_proj.bias', 'decoder.model.decoder.layers.9.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.7.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.10.final_layer_norm.weight', 'decoder.model.decoder.layers.10.fc1.bias', 'decoder.model.decoder.layers.4.final_layer_norm.bias', 'decoder.model.decoder.layers.7.self_attn.out_proj.weight', 'decoder.model.decoder.layers.11.final_layer_norm.weight', 'decoder.model.decoder.layers.10.self_attn.q_proj.bias', 'decoder.model.decoder.layers.8.fc1.bias', 'decoder.model.decoder.layers.6.self_attn.v_proj.weight', 'decoder.model.decoder.layers.4.self_attn.out_proj.weight', 'decoder.model.decoder.layers.7.encoder_attn.k_proj.bias', 'decoder.model.decoder.layers.2.encoder_attn.v_proj.bias', 'decoder.model.decoder.layers.11.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.8.encoder_attn.out_proj.bias', 'decoder.model.decoder.layers.0.final_layer_norm.bias', 'decoder.model.decoder.layers.8.self_attn.q_proj.bias', 'decoder.model.decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.2.self_attn.q_proj.weight', 'decoder.model.decoder.layers.10.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.8.final_layer_norm.weight', 'decoder.model.decoder.layers.9.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.11.fc1.weight', 'decoder.model.decoder.layers.8.self_attn_layer_norm.weight', 'decoder.model.decoder.layers.9.self_attn.q_proj.bias', 'decoder.model.decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.10.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.11.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.6.final_layer_norm.weight', 'decoder.model.decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.5.self_attn.out_proj.bias', 'decoder.model.decoder.layers.2.self_attn.out_proj.weight', 'decoder.model.decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.11.encoder_attn.q_proj.bias', 'decoder.model.decoder.layers.11.self_attn.q_proj.weight', 'decoder.model.decoder.layers.11.fc2.weight', 'decoder.model.decoder.layers.9.fc1.bias', 'decoder.model.decoder.layers.10.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.7.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.model.decoder.layers.8.final_layer_norm.bias', 'decoder.model.decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.3.self_attn.v_proj.bias', 'decoder.model.decoder.layers.5.self_attn.out_proj.weight', 'decoder.model.decoder.layers.6.fc2.bias', 'decoder.model.decoder.layers.9.encoder_attn.k_proj.bias', 'decoder.model.decoder.layers.6.self_attn.q_proj.weight', 'decoder.model.decoder.layers.10.self_attn.q_proj.weight', 'decoder.model.decoder.layers.8.fc2.bias', 'decoder.model.decoder.layers.0.self_attn.k_proj.bias', 'decoder.model.decoder.layers.0.self_attn_layer_norm.bias', 'decoder.model.decoder.layers.8.self_attn.k_proj.weight', 'decoder.model.decoder.layers.2.self_attn.v_proj.bias', 'decoder.model.decoder.layers.4.self_attn.k_proj.weight', 'decoder.model.decoder.layers.9.self_attn.out_proj.bias', 'decoder.model.decoder.layers.9.encoder_attn.k_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "donut_large_model_load = VisionEncoderDecoderModel.from_pretrained(SAVE_MP)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "0"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newly_added_num"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "MBartDecoder(\n  (embed_tokens): Embedding(58891, 1024, padding_idx=1)\n  (embed_positions): MBartLearnedPositionalEmbedding(2562, 1024)\n  (layers): ModuleList(\n    (0-3): 4 x MBartDecoderLayer(\n      (self_attn): MBartAttention(\n        (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n      )\n      (activation_fn): GELUActivation()\n      (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n      (encoder_attn): MBartAttention(\n        (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n      )\n      (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n      (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n      (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n      (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n    )\n  )\n  (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n  (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n)"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "donut_base_model.decoder.model.decoder"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%don\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "(MBartForCausalLM(\n   (model): MBartDecoderWrapper(\n     (decoder): MBartDecoder(\n       (embed_tokens): Embedding(58891, 1024, padding_idx=1)\n       (embed_positions): MBartLearnedPositionalEmbedding(2562, 1024)\n       (layers): ModuleList(\n         (0-11): 12 x MBartDecoderLayer(\n           (self_attn): MBartAttention(\n             (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n             (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n             (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n             (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n           )\n           (activation_fn): GELUActivation()\n           (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n           (encoder_attn): MBartAttention(\n             (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n             (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n             (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n             (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n           )\n           (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n           (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n           (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n           (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n         )\n       )\n       (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n       (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n     )\n   )\n   (lm_head): Linear(in_features=1024, out_features=58891, bias=False)\n ),\n MBartForCausalLM(\n   (model): MBartDecoderWrapper(\n     (decoder): MBartDecoder(\n       (embed_tokens): Embedding(58891, 1024, padding_idx=1)\n       (embed_positions): MBartLearnedPositionalEmbedding(2562, 1024)\n       (layers): ModuleList(\n         (0-3): 4 x MBartDecoderLayer(\n           (self_attn): MBartAttention(\n             (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n             (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n             (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n             (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n           )\n           (activation_fn): GELUActivation()\n           (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n           (encoder_attn): MBartAttention(\n             (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n             (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n             (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n             (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n           )\n           (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n           (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n           (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n           (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n         )\n       )\n       (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n       (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n     )\n   )\n   (lm_head): Linear(in_features=1024, out_features=58891, bias=False)\n ))"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "donut_large_model_load.decoder, donut_base_model.decoder"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "(60304384, 2623488, 60304384)"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "donut_large_decoder_tokens_embedding_params = cal_model_parameters(donut_large_model_load.decoder.model.decoder.embed_tokens)\n",
    "\n",
    "donut_large_decoder_positions_embedding_params = cal_model_parameters(donut_large_model_load.decoder.model.decoder.embed_positions)\n",
    "\n",
    "donut_large_decoder_lm_head_params = cal_model_parameters(donut_large_model_load.decoder.lm_head)\n",
    "\n",
    "donut_large_decoder_tokens_embedding_params, donut_large_decoder_positions_embedding_params, donut_large_decoder_lm_head_params"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "(0.2951765336698225, 0.18370536031427503)"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "donut_base_token_embedding_params_rate = donut_large_decoder_tokens_embedding_params / donut_base_params\n",
    "donut_large_token_embedding_params_rate = donut_large_decoder_tokens_embedding_params / donut_large_model_params\n",
    "donut_base_token_embedding_params_rate, donut_large_token_embedding_params_rate"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- donut模型使用MBart作为解码器, token数量58891, 使得token_embedding和lm_head层的参数量非常大, donut-base模型参数量占比接近60%, donut-large模型参数量占比约36%\n",
    "- 下一步需要减少token数,并重构token_embedding和lm_head层的参数矩阵, 确保现有模型效果不影响"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 区分tokenizer里面的语言"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "import langid"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "('en', -33.54452037811279)"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = langid.classify(\"what is your name\")\n",
    "res"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "{'wear': 54272,\n 'ことに': 41572,\n 'مجلس': 4644,\n '▁From': 35067,\n 'cula': 15682,\n '▁cere': 40064,\n 'pis': 42969,\n 'EC': 15265,\n '▁•': 51233,\n '▁враг': 57238,\n 'cidas': 48839,\n '▁клінічн': 21057,\n '근': 27265,\n 'gner': 43346,\n '有不少': 3017,\n '▁one': 14754,\n '扩张': 19540,\n '▁Outlook': 48498,\n '▁Selv': 22546,\n '创造': 36471,\n '▁recommend': 48440,\n 'netz': 7987,\n 'ultima': 51582,\n '▁команду': 8070,\n '▁nagyjából': 23409,\n 'tol': 9134,\n 'jia': 46791,\n '▁Пир': 6190,\n 'jä': 49278,\n '464': 14277,\n '争议': 8041,\n '▁gedink': 24394,\n '救援': 1717,\n '▁꾸준히': 25991,\n '006': 13942,\n '溥': 31083,\n '▁ڀيرو': 21374,\n '▁kolejnych': 9968,\n '橄': 33792,\n 'ッド': 53487,\n '貉': 58636,\n '亮': 16094,\n 'ASA': 35860,\n '쾰': 31672,\n '하는데': 55471,\n '▁styre': 47917,\n '▁평균': 50231,\n 'יקס': 4158,\n '鹌': 33230,\n '▁Blackberry': 26492,\n '艳': 57668,\n 'ႄ': 30496,\n 'طالع': 24940,\n 'books': 52586,\n '系统': 53508,\n 'riji': 54595,\n 'ंद्र': 12458,\n '▁(22': 43729,\n 'wasser': 51427,\n '번째': 9886,\n '▁Sinne': 49895,\n 'ounce': 53541,\n 'im': 41736,\n '細緻': 26109,\n '▁libroj': 13387,\n '銣': 32626,\n '组织的': 7951,\n 'סטודנט': 13755,\n '텀': 27913,\n '😤': 31664,\n 'めて': 39144,\n '▁gives': 51064,\n '诰': 58622,\n '菇': 15078,\n '靂': 33599,\n 'PK': 42114,\n '▁ga': 55253,\n '▁anlattı': 16660,\n '殃': 33557,\n 'ಖಾ': 3423,\n 'LEA': 2687,\n 'rei': 56401,\n 'دیا': 7611,\n '歎': 29620,\n '▁Patria': 5068,\n '▁önüne': 9760,\n '▁million': 21841,\n 'poli': 25868,\n '▁тратить': 23777,\n '▁판매': 40672,\n '▁Oslo': 50429,\n '▁mamma': 25460,\n '盈利': 2066,\n '▁ගිණුම': 4619,\n '▁распоред': 13377,\n '▁registr': 24650,\n '沙灘': 27625,\n '躯': 29338,\n 'pama': 55338,\n 'bing': 28044,\n '▁حقوقی': 2970,\n '竖': 29173,\n '▁Llo': 803,\n '汇': 42107,\n '▁predict': 55472,\n '拆': 45289,\n '▁കാറ': 11080,\n '适合': 40394,\n '▁odziv': 12602,\n '▁escultura': 20651,\n 'nev': 25215,\n '엎': 30414,\n '一大': 42548,\n 'ndolo': 55935,\n 'lom': 47767,\n '▁Elizabeth': 50005,\n '석': 10942,\n '▁стоматолог': 21446,\n '娥': 58124,\n '▁Sala': 8103,\n '工作的': 37962,\n '曼联': 18904,\n 'zwi': 41644,\n '▁Australia': 35101,\n '▁Legisla': 5623,\n '願': 26714,\n 'nez': 20997,\n '▁http': 14530,\n '嗖': 31114,\n 'iter': 15882,\n '▁ослободи': 9625,\n 'impia': 13879,\n '▁Yan': 34215,\n '▁यासाठी': 6662,\n '5.00': 52944,\n '▁poni': 49015,\n '▁терет': 24364,\n '蠱': 30986,\n '胥': 31750,\n '▁apps': 42981,\n 'ရိုက်': 3182,\n '▁eco': 42775,\n '▁študentov': 9770,\n 'Super': 50419,\n '加入': 12167,\n '\\uf061': 30987,\n '▁dres': 7593,\n '▁shir': 43823,\n '▁Fateh': 51334,\n '諷': 28965,\n '▏': 58798,\n 'だったので': 54517,\n '明確': 53534,\n '▁जागे': 25175,\n '▁Fas': 39798,\n '他': 53561,\n '螂': 58163,\n '754': 19941,\n '騁': 31693,\n '▁Pendaftaran': 22152,\n '烏': 53907,\n '▁Days': 56812,\n 'ිති': 23055,\n '▁Perse': 51503,\n 'minis': 47737,\n 'θηκα': 19968,\n '荆': 29337,\n 'Link': 47509,\n '鬧': 12901,\n '▁tau': 45216,\n '曝': 57607,\n 'ssi': 6398,\n '▁líbí': 16213,\n 'drücken': 18073,\n '▁Miele': 13968,\n 'usto': 47337,\n '▁Woensdag': 27584,\n 'schlecht': 28138,\n '▁завршен': 9847,\n '▁Świe': 2184,\n '윗': 25739,\n 'の': 12587,\n '對': 22096,\n 'İsmayılov': 22918,\n 'ေမြ': 27028,\n '미': 45022,\n '▁contabil': 2279,\n 'سوق': 11188,\n 'גזר': 4801,\n 'Ну': 12221,\n '詣': 30348,\n 'させて': 36260,\n 'nok': 39131,\n '▁odio': 14352,\n '牴': 32293,\n '缤': 30429,\n '▁उड': 2631,\n '▁jel': 39883,\n '▁bijoux': 28356,\n 'O': 48318,\n '▁ಪ್ರಥಮ': 21213,\n '褪': 29191,\n '4/': 51366,\n 'Fun': 56800,\n '堕': 29198,\n 'ങ്ങൾക്ക്': 9021,\n 'よう': 26548,\n 'tour': 35387,\n 'rec': 36461,\n '컸': 33725,\n '试验': 53318,\n 'ことが': 11831,\n '▁уряду': 15670,\n 'መግባባት': 25809,\n '▁Aceh': 57149,\n '▁xərclə': 10535,\n 'ήμα': 6075,\n 'á': 49458,\n '▁arrive': 44368,\n '▁argent': 52945,\n '▁Medikamente': 25119,\n '▁manager': 36242,\n '▁stock': 36522,\n '▁Spar': 38635,\n 'sjö': 1610,\n '술': 27561,\n '▁Cze': 53518,\n '⇆': 32193,\n '競': 51606,\n '이자': 2058,\n 'とは': 2813,\n '▁Batı': 11680,\n '곱': 24300,\n '▁Alfred': 50635,\n 'октябр': 8209,\n '▁აბა': 158,\n 'פנק': 21564,\n '上面': 38535,\n '▁қарастырыл': 19866,\n 'hidra': 20350,\n 'ți': 19756,\n '▁удовлетвори': 24418,\n '榴': 28902,\n '▁bur': 48682,\n '▁Oli': 35500,\n '耀': 49576,\n 'жиж': 1718,\n '▁val': 37722,\n 'hlav': 10173,\n '▁heli': 43197,\n '▁Regi': 36563,\n '骅': 58313,\n '▁electrònica': 12364,\n '▁become': 28693,\n '的意见': 9914,\n 'ադաշտ': 27980,\n 'skan': 26903,\n 'andin': 37275,\n '▁avocado': 25204,\n '▁dnevi': 14479,\n '功夫': 5441,\n 'bayang': 9628,\n '狈': 30645,\n '▁revanche': 21521,\n '▁කටු': 23900,\n '▁Cer': 54241,\n 'مراهق': 25939,\n 'hane': 37498,\n 'НОСТ': 1658,\n '▁جزا': 14916,\n '▁verejnosť': 23857,\n '▁Dział': 19398,\n '▁দশ': 19689,\n '▁обвинител': 18807,\n '▁Макрон': 22696,\n 'סיכום': 19558,\n '685': 16419,\n 'Po': 49804,\n '▁customer': 40729,\n 'ֳ': 31171,\n 'ирајте': 9492,\n '▁대표': 35930,\n '컵': 53873,\n 'sher': 40510,\n '▁mica': 36688,\n '▁Iron': 49381,\n '尼克': 20105,\n 'ically': 49650,\n '訂房': 53672,\n 'תקשר': 18844,\n '▁ტექნოლოგი': 22741,\n '▁निर्वाह': 27063,\n 'のない': 52221,\n '潘': 56651,\n 'としては': 44414,\n '▁kab': 43051,\n '▁Innova': 50996,\n 'gjafa': 14099,\n '▁minera': 54194,\n 'DIRI': 17864,\n '斬': 28887,\n 'ver': 52553,\n '杨欢': 7339,\n '▁quorum': 55360,\n 'oeuvre': 600,\n '▁πρόταση': 4570,\n '▁відкрив': 8791,\n 'stor': 23514,\n '飮': 32386,\n '倩': 26055,\n '▁Dato': 38455,\n '自動車': 9449,\n 'pli': 34236,\n '蒻': 33206,\n 'avis': 52081,\n '提高': 53777,\n '▁Zlín': 11683,\n 'vou': 29187,\n 'oku': 38867,\n '綦': 58355,\n '▁cuir': 2499,\n 'ท้าทาย': 27854,\n '▁Eva': 35081,\n '▁Kung': 41325,\n 'TRI': 46903,\n 'nang': 25261,\n '▁агентство': 12764,\n 'prost': 42332,\n '▁multo': 7524,\n '聽': 12136,\n '▁Selangor': 37614,\n 'pojení': 14109,\n '▁rettigheter': 16666,\n '鲔': 32620,\n '▁civil': 56933,\n '▁سوئال': 25887,\n '上海市': 1660,\n '▁تمدید': 25069,\n '쿠폰': 19726,\n '並且': 40397,\n '▁колега': 6405,\n '▁의원': 53930,\n 'seg': 34000,\n 'cima': 9009,\n '醃': 28938,\n '顷': 29632,\n '▁ayaw': 10556,\n '▁panasz': 15276,\n '▁Nema': 50405,\n '▁Computer': 39584,\n 'varuste': 8259,\n 'ime': 38101,\n 'nie': 49225,\n '▁Bern': 49628,\n 'ตั๋ว': 11960,\n '晒': 4960,\n '▁Muna': 6067,\n 'பக': 6866,\n '▁लिग': 193,\n '祟': 30178,\n '▁підвищен': 15656,\n '▁Spotify': 4620,\n '▁belgilangan': 9377,\n '▁legate': 48766,\n '▁കുട': 1764,\n '▁болашақ': 14708,\n '▁herceg': 18689,\n '▁съобщи': 5620,\n '▁envie': 39009,\n '漠': 13038,\n 'limit': 55816,\n 'yd': 2770,\n '▁Pievienot': 5881,\n 'ījies': 19035,\n 'ΠΑΙ': 24725,\n '▁nur': 15125,\n '▁резултатите': 2227,\n '▁scor': 35344,\n 'nir': 55669,\n '🇦': 31017,\n '▁کلاه': 20931,\n '咗': 26925,\n '▁os': 38129,\n '缓缓': 28376,\n '▁gaur': 38511,\n '▁Schweden': 26551,\n 'のみ': 42552,\n '▁Aus': 44536,\n '▁Brasil': 52829,\n '▁sincer': 40836,\n '▁vrednost': 2243,\n '▁Tempo': 40532,\n '▁assign': 56341,\n 'sv': 48555,\n '▁applications': 54003,\n '▁나': 52161,\n '–': 2664,\n '▁classico': 16271,\n '▁Wtedy': 21373,\n '▁elast': 45160,\n 'teurs': 14165,\n 'haridus': 20229,\n '▁Jerry': 9339,\n '看上去': 19923,\n '▁இங்கு': 1375,\n '▁здаецца': 20271,\n 'েল': 56935,\n '▁psychische': 23988,\n 'more': 17526,\n '▁merusak': 22160,\n '▁Baza': 10770,\n '▁користувача': 14543,\n '▁Reiki': 9787,\n '▁baizik': 15098,\n 'uch': 6685,\n '择': 57651,\n '▁montaj': 13930,\n '▁सतत': 16539,\n '▁popula': 34879,\n '檀': 29554,\n 'uð': 7771,\n 'lyasiya': 16176,\n 'JO': 34452,\n '▁Karácsony': 22075,\n '为了': 5486,\n 'වෙන්න': 4116,\n '▁목': 40798,\n '출장소이스': 25521,\n '廠商': 50977,\n 'Gu': 26707,\n 'pili': 38687,\n '▁drift': 38909,\n '▁Kuwa': 48124,\n '▁آخري': 12333,\n '▁mainos': 13899,\n '▁prices': 3074,\n '▁Time': 20320,\n '▁rele': 42034,\n 'īd': 43619,\n 'BOL': 57064,\n '▁문제': 37787,\n '▁2006.': 36111,\n '▁ohjaa': 14600,\n 'uck': 50875,\n '▁SD': 25287,\n 'řízení': 13257,\n 'മന്ത്രി': 7631,\n '耶穌': 9552,\n '▁značaja': 15537,\n 'Han': 48079,\n '▁Mwenye': 26607,\n '▁board': 41457,\n '扮演': 54731,\n '渊': 26048,\n '至今': 37810,\n '不知道': 8871,\n '▁rendere': 52019,\n '▁posta': 56836,\n 'តូច': 3570,\n '▁image': 35636,\n 'cen': 9390,\n '▁заряд': 3245,\n '종': 5766,\n 'ىلدى': 12824,\n '▁suiker': 14729,\n '鋭': 30050,\n '▁pertanian': 10330,\n 'bū': 50561,\n 'ILA': 39232,\n '휼': 32059,\n 'TIVO': 14348,\n 'หัก': 15027,\n '▁риз': 6416,\n '▁lind': 48789,\n '▁ընտրել': 22907,\n 'erat': 7826,\n '國務院': 28760,\n '▁효과를': 23847,\n '▁вака': 2798,\n '▁бичгийн': 10135,\n '▁mac': 20463,\n 'ljni': 7872,\n '▁gennemføre': 16658,\n '認證': 52607,\n '▁objektov': 17966,\n '茧': 30720,\n '▁Than': 50098,\n '▁مصروف': 5648,\n '▁vuelo': 16349,\n 'gas': 39309,\n '有利': 54910,\n '▁Arena': 39546,\n 'paper': 50548,\n '斗': 41404,\n '☽': 31854,\n '▁Storbritannia': 25452,\n 'plu': 47056,\n '▁Nacional': 54908,\n '寅': 29149,\n 'faqat': 12424,\n '▁anno': 53997,\n '动物': 49749,\n '▁consultant': 7281,\n 'آپ': 10159,\n '▁Pêş': 17880,\n '榕': 29509,\n 'くなって': 52541,\n '跛': 31297,\n 'Lab': 56838,\n '▁AC': 7316,\n '▁болуш': 7864,\n '▁qaadan': 5030,\n '▁ramane': 8627,\n '▁raz': 21214,\n '▁काळा': 19334,\n '▁mann': 6960,\n '▁ομάδας': 1311,\n '高級': 49419,\n '▁ekonomika': 3498,\n '熟': 35832,\n 'पत्': 939,\n '▁Staats': 53100,\n 'tuba': 54090,\n 'MÅL': 27139,\n '頂ける': 23262,\n 'eaza': 51474,\n 'sthetic': 28086,\n '▁Scan': 47898,\n '▁çerçeve': 25523,\n '얍': 32498,\n 'mental': 43872,\n '▁całą': 2219,\n 'ahi': 30110,\n '▁capital': 854,\n '▁Մարտի': 14948,\n '未来': 6410,\n 'tym': 38919,\n '▁chối': 13278,\n 'CL': 38645,\n '舒適': 44082,\n '▁колдо': 3088,\n '▁σοκ': 25833,\n '▁Promo': 27739,\n 'asa': 41162,\n '多種': 53924,\n '식을': 1412,\n '基本上': 49110,\n '怯': 29250,\n '▁link': 36293,\n '捕捉': 21123,\n 'tinu': 46106,\n 'rann': 50489,\n 'kh': 1784,\n '▁തുറന്ന': 4749,\n '▁Bir': 35784,\n '▁ուսումնական': 17114,\n 'FM': 45988,\n '們': 5426,\n '我的': 48303,\n '一是': 51904,\n '▁ljudskih': 23545,\n '▁av': 19025,\n '錢': 19505,\n 'üveg': 23807,\n '▁വരിക': 13698,\n '寰': 30142,\n '▁Phen': 53294,\n '可能會': 53367,\n 'ၣ': 33667,\n '印': 20419,\n '▁ځواک': 22297,\n '焚': 23932,\n '▁indice': 8855,\n '▁une': 51364,\n 'どのくらい': 27518,\n '笛': 26930,\n 'ោក': 15340,\n '▁landinu': 17289,\n '▁borsa': 14668,\n 'များ၊': 17501,\n '▁аумағы': 19641,\n '悍': 25982,\n '▁(5': 3870,\n 'afegeix': 23710,\n 'ment': 48429,\n 'ందంటే': 27032,\n '▁Kristen': 51131,\n 'හන': 9984,\n '坛': 13898,\n '▁recent': 16816,\n 'toon': 34697,\n '債': 54310,\n '▁Dj': 39359,\n '岌': 33606,\n '▁lup': 46703,\n '一声': 55889,\n 'lícula': 10640,\n '枋': 30906,\n '▁성장': 44550,\n '▁Wag': 7795,\n '植物': 38604,\n '濯': 58676,\n '▁देशभर': 3892,\n 'eszköz': 13584,\n '▁indkøb': 20637,\n '7.6': 18282,\n 'trait': 54342,\n '应用': 22298,\n '궈': 30939,\n '钓': 29856,\n '▁산': 33998,\n '▁acum': 55397,\n '辊': 58397,\n '▁Crew': 18265,\n '▁1939': 37559,\n '鵝': 21293,\n 'white': 11610,\n '爱': 50881,\n '▁لمح': 5149,\n '▁రాసిన': 15356,\n 'isesta': 39750,\n '▁fair': 40980,\n '▁causa': 50082,\n 'GD': 54539,\n 'kulttuuri': 9717,\n '85': 7330,\n 'ysgrif': 9700,\n '▁influência': 20008,\n '飞': 17132,\n '▁ej': 54051,\n '别': 6963,\n '▁ඉවරයි': 26010,\n '▁Fle': 26997,\n '职业生涯': 27213,\n 'തുകൊണ്ട്': 8712,\n '▁Hind': 48612,\n '▁जगत': 106,\n '彐': 58606,\n '驚': 28269,\n '〉': 50975,\n '와의': 7111,\n '▁копира': 15975,\n '▁kri': 54030,\n '▁Simple': 46372,\n '▁glaz': 49232,\n 'pose': 51704,\n '無論': 52162,\n '➉': 58770,\n '从': 21415,\n '嘤': 31948,\n '▁영업': 9257,\n '▁woman': 41866,\n '▁cole': 44524,\n '▁particular': 16820,\n '▁hul': 54147,\n '怆': 32593,\n '不得不': 45209,\n '烛': 58110,\n 'UO': 55692,\n 'ुप': 5499,\n 'аѓање': 19208,\n 'ที่ระลึก': 26989,\n '▁Venezia': 3940,\n 'iniuose': 4272,\n '▁кујна': 26135,\n '▁ince': 36248,\n '臘': 29636,\n '▁Los': 38567,\n 'ος': 53804,\n 'vakantie': 19356,\n 'גרסה': 14243,\n '事业单位': 16796,\n '▁게임': 34649,\n '▁Come': 1552,\n '▁традиционални': 25710,\n '▁आयो': 6789,\n '▁have': 51139,\n 'çıların': 11726,\n '▁사용': 2563,\n '这一': 4632,\n '▁ਤਬਦੀਲ': 27331,\n 'uden': 35149,\n 'inio': 56587,\n '▁kobiety': 57100,\n 'ではない': 34788,\n '▁Oct': 37217,\n '▁highlight': 6826,\n '狗': 34756,\n '▁lama': 55259,\n '▁commercial': 46223,\n '▁вест': 2984,\n '▁diharapkan': 463,\n '▁epi': 33957,\n '▁Młodzi': 21600,\n 'linjen': 1506,\n '▁inser': 35051,\n '▁Sper': 40913,\n 'ବତୀ': 22361,\n '▁حلقہ': 23493,\n '▁α': 46070,\n '▁결제': 49701,\n 'ione': 22951,\n 'gga': 36276,\n '▁peaks': 38522,\n 'uz': 37073,\n '▁frustra': 45324,\n '▁tug': 37332,\n 'eng': 8129,\n '▁Perhatikan': 27349,\n 'SAN': 37877,\n '事態': 22609,\n '▁Take': 41867,\n 'ਾਤ': 635,\n '▁arti': 17490,\n '餅': 43613,\n 'atos': 46315,\n 'direct': 52344,\n 'obec': 6343,\n 'angebot': 9220,\n '▁VO': 44598,\n '▁Volta': 10901,\n '▁espazos': 11386,\n '▁Kru': 35096,\n '썩': 29415,\n '▁dobrym': 6447,\n '▁ero': 27092,\n '▁bel': 55514,\n 'chie': 44689,\n '▁îţi': 7543,\n '▁Real': 43367,\n '你的': 48958,\n '▁continua': 56093,\n '▁Hj': 49759,\n '咄': 33624,\n '湘': 57550,\n '艸': 30357,\n '網路': 16681,\n '蛆': 58554,\n '物': 37067,\n '網頁': 46604,\n 'ɑ': 29434,\n '炯': 30141,\n 'DOS': 53661,\n 'peri': 16507,\n 'Ỹ': 33761,\n '絕': 39736,\n '珂': 29494,\n '▁Glo': 41694,\n '▁идеология': 24747,\n '▁Florida': 43897,\n '鴇': 32896,\n '▁örugg': 14048,\n '▁kesken': 1572,\n '▁Split': 41799,\n 'pres': 56268,\n 'หัด': 25927,\n '▁beschermen': 25537,\n '脯': 30392,\n '▁Parti': 55304,\n '▁rozhodl': 24140,\n '▁Mihemed': 18240,\n 'raf': 3636,\n '徹底': 771,\n '▁Βι': 2752,\n 'isté': 56955,\n '遛': 30556,\n '딨': 33042,\n '▁sultan': 13180,\n '▁присутни': 15429,\n 'ion': 18811,\n 'VIN': 45107,\n '▁여러분': 54233,\n 'vete': 39106,\n '面前': 41550,\n 'iral': 53925,\n 'automat': 43735,\n '▁mendatangkan': 22958,\n '▁sjúkra': 28101,\n 'となり': 2553,\n '▁sover': 56457,\n 'കളായി': 24665,\n '僥': 31827,\n 'Ἰ': 33634,\n 'ய்யா': 19178,\n '▁7000': 54141,\n '▁cai': 39465,\n 'wari': 42913,\n '翘': 29324,\n '▁ibi': 49231,\n 'のお': 35613,\n '▁приступа': 15072,\n '▁الرابع': 8096,\n '▁§': 44118,\n '▁assure': 55680,\n '▁саласындағы': 4623,\n '▁Цр': 4352,\n '113': 4216,\n 'ques': 49706,\n '嫚': 58393,\n '钰': 58298,\n '.10.': 480,\n '▁There': 53880,\n '▁korri': 2785,\n '▁matematika': 2446,\n '▁группе': 12688,\n '▁Toki': 44152,\n '▁mbunge': 25561,\n '▁markaas': 12337,\n '蝸': 33595,\n '❖': 30051,\n '▁башкы': 5272,\n '▁musíme': 9033,\n '想着': 11863,\n '閑': 29370,\n 'なくなって': 56030,\n '虐待': 25580,\n '验': 57567,\n 'gos': 54351,\n '▁засоби': 2254,\n '雋': 30792,\n 'વણ': 16932,\n '▁pigment': 43303,\n '-4': 3989,\n 'மைப்பு': 17506,\n 'と': 46494,\n '比': 34273,\n '▁conosce': 3521,\n '▁smart': 19766,\n '語言': 45538,\n '琵': 29719,\n '这样的': 12061,\n 'ຯ': 29696,\n '▁Đi': 3631,\n 'ვდი': 3796,\n '向け': 48431,\n 'ifa': 36716,\n '▁ubi': 18165,\n 'pto': 39880,\n 'ᅳ': 19994,\n '習': 41051,\n 'ジェ': 55684,\n 'sper': 23135,\n '▁ändring': 16672,\n 'geschichte': 2661,\n '▁Ł': 47452,\n '和谐': 770,\n 'tella': 38235,\n '▁먹': 42255,\n '▁مختار': 16969,\n '▁منظمة': 5138,\n 'Start': 6671,\n '▁ଡାକ': 19321,\n 'បុរាណ': 26112,\n 'യറ്റ': 7870,\n '蘆': 22449,\n 'sys': 56201,\n '▁meðan': 1193,\n '멩': 32048,\n '▁자': 53562,\n '▁óg': 3740,\n 'दूत': 24734,\n '跨': 37083,\n '桐': 21040,\n '▁ئاستى': 19808,\n '執行': 39183,\n 'liyinə': 8644,\n '绥': 30954,\n '▁bout': 47292,\n '▁clair': 53124,\n '▁kha': 21401,\n '各種': 17766,\n 'tr': 41099,\n '餘': 38383,\n 'صندوق': 18043,\n '▁MAIS': 16574,\n 'ws': 21559,\n '▁тонко': 23074,\n '惧': 57830,\n '弩': 30514,\n '▁gilda': 24565,\n '第一个': 51406,\n '▁Alter': 39286,\n '▁folklor': 51562,\n '▁Performance': 55973,\n '▁ಮತ': 2916,\n 'py': 47704,\n '意見': 36386,\n '▁redu': 35419,\n '31)': 56431,\n '動画': 40915,\n '▁res': 37098,\n '▁ಸ್ಪ': 374,\n '▁хүлээж': 12024,\n 'ລະອຽດ': 15557,\n '白色': 46849,\n '▁proposal': 12400,\n '▁mod': 35075,\n '进入': 14814,\n '쳐': 45222,\n '▁Интересно': 8592,\n 'eder': 34956,\n '隨': 35824,\n 'Ï': 29331,\n '▁event': 21578,\n '桡': 58466,\n 'ພຣະອົງ': 28067,\n 'ნდო': 1171,\n '▁busque': 18342,\n '▁direttore': 18607,\n '鞆': 32618,\n '▁quadra': 48811,\n '▁separar': 21497,\n '▁Ky': 53993,\n 'tajwa': 17246,\n '않': 30170,\n 'yun': 22913,\n '▁kama': 40505,\n '▁kt': 45079,\n '▁inte': 54657,\n '▁straf': 53622,\n '▁남': 11042,\n '▁여부': 15453,\n '麿': 32449,\n '▁სხეულ': 3811,\n '!\"': 39164,\n 'dingen': 6696,\n 'ුනා': 1256,\n '▁mitjà': 16520,\n '▁때문이다': 52043,\n '▁Male': 37806,\n '贈': 43599,\n 'Porn': 45229,\n '▁fertil': 55706,\n '诠': 57787,\n '霊': 14347,\n '▁αδελφ': 15558,\n '▁pi': 24622,\n '▁기능': 43684,\n 'indi': 21887,\n '▁filter': 41738,\n '安心': 37781,\n '쩡': 30770,\n '▁Shop': 28696,\n '▁talle': 38034,\n 'oto': 13856,\n '被称为': 15616,\n 'PP': 17156,\n 'tame': 41973,\n '조차': 22325,\n '餌': 29437,\n '까요': 47610,\n '▁기록': 42800,\n 'デメリット': 24078,\n '▁conflit': 3462,\n '▁환': 53016,\n '▁Ross': 44076,\n '机器人': 616,\n '321': 969,\n 'RS': 12790,\n 'Gənc': 10601,\n 'ສະດວກ': 27334,\n '▁Roh': 37386,\n '테크': 11519,\n '▁tube': 40718,\n '▁Dus': 43170,\n '依据': 53093,\n 'rub': 42147,\n 'Europa': 42965,\n '▁naturligt': 12889,\n '▁المرضى': 25404,\n '▁propos': 42311,\n '小学': 50767,\n '書籍': 3861,\n 'تعدادی': 18730,\n 'cky': 33915,\n '鎵': 32295,\n '▁животу': 8093,\n 'aime': 24767,\n '▁Jorge': 49511,\n '方が': 36793,\n '▁Energia': 46006,\n '濁': 29315,\n '▁Dom': 55837,\n 'Code': 3620,\n 'ਹੋ': 16194,\n '갉': 32965,\n 'Wahai': 22210,\n '需求': 1669,\n '▁Khasiat': 27402,\n '▁Lü': 46526,\n 'لفت': 5832,\n 'を紹介します': 24407,\n '美女': 48178,\n '扭': 51060,\n '▁Νομ': 71,\n '负责': 34377,\n '▁aurrean': 1089,\n '▁поступа': 4818,\n 'utha': 57343,\n 'flutning': 1080,\n ...}"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.tokenizer.vocab"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 58891/58891 [01:35<00:00, 614.14it/s]\n"
     ]
    }
   ],
   "source": [
    "# 统计vocab中语言词表分布\n",
    "from tqdm import tqdm\n",
    "lang_dict = dict()\n",
    "for k,v in tqdm(processor.tokenizer.vocab.items()):\n",
    "    lang = langid.classify(k)\n",
    "    if lang[0] not in lang_dict:\n",
    "        lang_dict[lang[0]] = 0\n",
    "    lang_dict[lang[0]] += 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "{'en': 16663,\n 'zh': 12734,\n 'lv': 5748,\n 'ja': 3087,\n 'ko': 2538,\n 'de': 1183,\n 'ru': 975,\n 'hu': 745,\n 'es': 713,\n 'ar': 644,\n 'pl': 607,\n 'th': 530,\n 'fr': 521,\n 'el': 509,\n 'he': 454,\n 'fi': 432,\n 'fa': 410,\n 'tr': 366,\n 'ml': 363,\n 'bg': 350,\n 'cs': 339,\n 'kk': 334,\n 'it': 315,\n 'sv': 308,\n 'uk': 288,\n 'be': 287,\n 'te': 286,\n 'mr': 281,\n 'si': 279,\n 'sr': 272,\n 'ka': 264,\n 'et': 260,\n 'nl': 257,\n 'mn': 253,\n 'ta': 249,\n 'sl': 248,\n 'vi': 235,\n 'da': 233,\n 'kn': 226,\n 'lt': 223,\n 'hi': 212,\n 'sk': 210,\n 'am': 187,\n 'bn': 187,\n 'gu': 184,\n 'ro': 178,\n 'lo': 174,\n 'az': 158,\n 'pa': 148,\n 'pt': 142,\n 'ne': 141,\n 'hy': 136,\n 'km': 133,\n 'or': 133,\n 'eu': 118,\n 'mt': 101,\n 'rw': 94,\n 'id': 91,\n 'ur': 89,\n 'is': 89,\n 'no': 79,\n 'sq': 79,\n 'ga': 78,\n 'ug': 73,\n 'hr': 62,\n 'mk': 59,\n 'gl': 59,\n 'ku': 53,\n 'vo': 44,\n 'oc': 41,\n 'ms': 36,\n 'bs': 31,\n 'af': 29,\n 'ps': 28,\n 'cy': 24,\n 'mg': 22,\n 'ht': 20,\n 'tl': 16,\n 'eo': 15,\n 'nn': 14,\n 'xh': 13,\n 'ca': 13,\n 'qu': 12,\n 'nb': 12,\n 'dz': 11,\n 'zu': 10,\n 'br': 9,\n 'as': 6,\n 'wa': 5,\n 'la': 5,\n 'jv': 4,\n 'sw': 4,\n 'lb': 3,\n 'fo': 3,\n 'se': 2,\n 'an': 2,\n 'ky': 1}"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lang_dict = dict(sorted(lang_dict.items(), key=lambda x: x[1], reverse=True))\n",
    "lang_dict"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "0.49917644461802313"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(lang_dict['en'] + lang_dict['zh'])/len(processor.tokenizer.vocab)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "分析一下embed_tokens 和 lm_head"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "Embedding(58891, 1024, padding_idx=1)"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_tokens_layer = donut_base_model.decoder.model.decoder.embed_tokens\n",
    "embed_tokens_layer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "data": {
      "text/plain": "(<generator object Module.parameters at 0x0000015E0E179E40>, generator)"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_tokens_layer.parameters(), type(embed_tokens_layer.parameters())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "data": {
      "text/plain": "\u001B[31m╭─\u001B[0m\u001B[31m──────────────────────────────\u001B[0m\u001B[31m \u001B[0m\u001B[1;31mTraceback \u001B[0m\u001B[1;2;31m(most recent call last)\u001B[0m\u001B[31m \u001B[0m\u001B[31m───────────────────────────────\u001B[0m\u001B[31m─╮\u001B[0m\n\u001B[31m│\u001B[0m in \u001B[92m<module>\u001B[0m:\u001B[94m1\u001B[0m                                                                                    \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m                                                                                                  \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m \u001B[31m❱ \u001B[0m1 embed_tokens_layer[\u001B[94m0\u001B[0m]                                                                        \u001B[31m│\u001B[0m\n\u001B[31m│\u001B[0m   \u001B[2m2 \u001B[0m                                                                                             \u001B[31m│\u001B[0m\n\u001B[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001B[0m\n\u001B[1;91mTypeError: \u001B[0m\u001B[32m'Embedding'\u001B[0m object is not subscriptable\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1 embed_tokens_layer[<span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>]                                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2 </span>                                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">TypeError: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Embedding'</span> object is not subscriptable\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
